% 1 Introduction
% The purpose of this laboratory session is to learn how to use an SVM library. 
% The chosen one is LIBSVM.
% 
% The library can be used through Matlab and it should be included in the Matlab 
% path. To make sure it works, just type				

svmtrain
%% 
% or

svmpredict
%% 
% Matlab must promt the help for both functions.
% 2 SVMTRAIN inputs and outputs
% 2.1 Training matrix and training labels
% Data vectors are defined as row vectors. Then, the input matrix must be $N 
% \times  D$, where $N$ is the number of training vectors (or patterns or instances) 
% and $D$ is the dimension of the space.
% 
% The labels corresponding to the training vectors are defined as a column vector 
% of scalars.
% 
% A simplistic example illustrates the data structure. We can generate a set 
% of data by generating *4 Gaussians around 4 different centroids*. Two are labelled 
% with +1 and two with −1, + and o respectively in the figure generated by the 
% following code:

clear all; clc;
c = [1,1;2,1.5;2,1;3,1.5]; % Centers of centroids
N = 50; % Number of samples
X = [];
sigma = 0.2; % Std Dev
for i = 1:4
    X = [X;sigma*randn(N,2)+repmat(c(i,:),N,1)];
end
Y = [ones(1,2*N) -ones(1,2*N)];
plot(X(1:end/2,1),X(1:end/2,2),'+')
grid on
grid minor
hold all
plot(X(end/2+1:end,1),X(end/2+1:end,2),'o')
title('\sigma = 0.2');
hold off
%% 
% We will use this code later.
% 2.2 SVM options
% The SVM function needs to know what is the kind of SVM that we want to run. 
% To this end, the user must construct a string with the options. The main ones 
% are:
% 
% -s svm type : set type of SVM (default 0)
% 
% -t kernel type : set type of kernel function (default 2)
% 
% -c cost : set the parameter C of C-SVC, epsilon-SVR, and nu-SVR (default 1)
% 
% An example can be run with the previous data and the Matlab line

model = svmtrain(Y',X, '-h 0 -s 0 -t 0 -c 100');
%% 
% This sentence means that we want to train an SVM using the previous data and 
% that the SVM must be a linear (-t 0) classifier (-s 0), and with C = 100.
% 2.3 SVM model
% This structured variable is the SVM training output and contains all the parameters 
% of the trained machine. For the example above, the output must be similar to 
% the following (please note that the variable names may change from version to 
% version):

model
%% 
% The important ones for us are:
% 
% rho: the SVM bias (usually called $b$).
% 
% sv_coef: The dual coefficients (usually called $\alpha_i$).
% 
% SVs: The patterns corresponding to the support vectors.
% 
% idx: the indexes of the support vectors.
% 
% The model is introduced in the test function (svmpredict). In that func- tion, 
% if the test labels are known, it outputs the predicted labels plus an estimate 
% of the probability of error. If the labels are not known, the corre- sponding 
% variable must be a column of arbitrary numbers (we can just use zeros).
% 3 Experiments
% 3.1 Construction of a classifier with the model parameters
% Using the parameters of the training, construct a machine that classifies 
% the training data. Compare your results with the ones of 'svmpredict’. In order 
% to find the equation of the classifier, you must compute the primal parameter 
% *w* from the dual ones $\alpha_i$ and the data.

svmpredict(Y',X,model);

b = -model.rho
alpha = model.sv_coef;
idx = model.sv_indices;
w = model.SVs' * alpha
% 3.2 Graphical representation of an SVM
% Use the example of subsection 2.1 to plot the separating line, the two margin 
% lines and the support vectors resulting of the SVM training. Do it for a smaller 
% value and a higher value of parameter 'sigma' of the data generating code. Comment 
% the results.

if model.Label(1) == -1
    w = -w;
    b = -b;
end

hold on
% Circle support vectors
sv = full(model.SVs);
plot(sv(:,1),sv(:,2),'ko', 'MarkerSize', 10);

% Classifier equation
y_hat = sign(w'*X' + b);
plot_x = linspace(min(X(:,1)), max(X(:,1)), 30);
plot_y = (-1/w(2))*(w(1)*plot_x + b);

% Margin equations
d_plus = plot_y + 1/norm(w);
d_minus = plot_y - 1/norm(w);

% Plot margins and hyperplane
plot(plot_x, d_plus, 'r--');
plot(plot_x, d_minus, 'b--');
plot(plot_x, plot_y, 'k-')
% 3.3 Estimating the structural risk
% The objective of this exercise is to draw a plot of the actual risk ($R(\alpha)$) 
% and the empirical risk $R_{emp}(\alpha)$. Consider a space of 10 dimensions 
% and a set of data that can be represented and classified in a subspace of only 
% 3 dimensions. Thus, a linear classifier of $h = 4$ should be sufficient to properly 
% solve the classification problem.
% 
% The code at the end of this file produces a set of data.
% 
% It first generates eight points describing a cube of dimension $\sigma$. Four 
% of the points are at one side of the plane described by the equation $\Pi = 
% \rm w^\top  x +$$b$ where $\rm w_i = 1/\sqrt{10}$ and $b = 0$, at a distance 
% $\sigma /2$, and they are labeled with −1. The other four are at the other side 
% of the plane and they are labelled as +1.
% 
% Each pattern is generated by choosing at random one of the eight points and 
% then adding a 10 dimensional Gaussian noise of standard deviation 0.2$\sigma$. 
% It is straightforward to prove that hyperplane $\Pi$ is the optimal separating 
% hyperplane in a Bayesian sense, that is, it will produce the minimum possible 
% actual risk.
% 
% Work out the following:
%% 
% * Using the function described above, compute and represent estimates of the 
% empirical risk and the actual risk with the following steps:
%% 
% # Set a given value of C (between $10^{-1.5}$ and $10$ should be enough).
% # Generate a set of 100 data, which will have a standard deviation $\sigma 
% = 1$.
% # Train an SVM and compute its empirical error.
% # Generate another set of data.
% # Test the SVM and compute the test error.
%% 
% Use the previous procedure to draw a graph (in logarithmic scale) that represents:
% 
% - The average training and test errors,
% 
% - the difference between the average test and training errors for 100 values 
% of C ranging from $10^{-1.5}$ and $10$ (better results are obtained when C is 
% logarithmically spaced.

clear all; clc;
N = 10:1:500;
c = logspace(-1.5,1,100);   % C between 10^{-1.5} and 10, 100 data
sigma = 1.0;                % standard deviation = 1
param ='-h -s 0 -t 0';

for k=1:500
    for l = 1:length(N)

        % Split into training and testing data
        [x_train,y_train] = data(N(l),sigma);
        [x_test,y_test] = data(N(l),sigma);

        % Train the SVM
        model = svmtrain(y_train,x_train,param);
        w = model.SVs' * model.sv_coef;
        b = -model.rho;

        if model.Label(1) == -1
            w = -w;
            b = -b;
        end

        % Estimated labels
        y_hat = sign(x_train*w + b);
        
        % Calculate training error or empirical risk
        emp_risk(l,k) = 0.5 / N(l) * sum(abs(y_hat - y_train));

        % Predict the labels from the sample test data with SVMpredict
        [pred_label, accuracy,prob_est] = svmpredict(y_test,x_test,model);

        % Calculate testing error or actual risk
        act_risk(l,k) = 0.5 / N(l) * sum(abs(pred_label - y_test));
    end
end

% Average of training and testing errors
for p = 1 : length(N)
    emp_risk_m(p) = mean(emp_risk(p,:));
    act_risk_m(p) = mean(act_risk(p,:));
end

% Plotting the actual risk, empirical risk, and structural risk
str_risk = act_risk_m - emp_risk_m;

figure;
semilogx(N,emp_risk_m, N,str_risk, N,act_risk_m,'LineWidth',1.5);
grid minor
xlabel('# of Data Points');
ylabel('Risks');
title('Empirical Risk, Structural Risk, and Actual Risk');
legend('Empirical Risk','Structural Risk','Actual Risk', Location='best');
%% 
% * Comment and give an explanation of the results with respect to the concepts 
% of actual, structural and empirical risks. In particular, give an explanation 
% of what is an estimation of the structural risk and why the minimum actual risk 
% does not coincide with the intersection of the other two errors.
% * Draw a graph showing the actual and the empirical risks as a function of 
% the number of data from $N=10$ to $N=500$ for the data of the previous experiment 
% with $C = 1$. Explain the graph. Does the learning procedure seem consistent?

function [X,Y]=data(N,sigma)
w=ones(1,10)/sqrt(10);
w1=w.*[1 1 1 1 1 -1 -1 -1 -1 -1];
w2=w.*[-1 -1 0 1 1 -1 -1 0 1 1];
w2=w2/norm(w2);
x(1,:)=zeros(1,10);
x(2,:)=x(1,:)+sigma*w1;
x(3,:)=x(1,:)+sigma*w2;
x(4,:)=x(3,:)+sigma*w1;
X1=x+sigma*repmat(w,4,1)/2;
X2=x-sigma*repmat(w,4,1)/2;
X1=repmat(X1,2*N,1);
X2=repmat(X2,2*N,1);
X=[X1;X2];
Y=[ones(4*2*N,1);-ones(4*2*N,1)];
Z=randperm(8*2*N);
Z=Z(1:N);
X=X(Z,:)+0.2*sigma*randn(size(X(Z,:)));
Y=Y(Z);
end