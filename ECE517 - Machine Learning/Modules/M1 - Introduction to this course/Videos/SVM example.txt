 Once the support vector machine package is installed, we can do a small example to understand how it works. This is an example for classification. We will do some others for regression, if you want, and then you can do yourselves. In this example, we generate two Gaussians. This is a function, this line, to generate a two dimension in Gaussian with a covariant function which is a diagonal matrix with variance as 2 and the mean is 1, 2. We have another Gaussian here with mean equal to -1, -2. For the first set of data, we assign positive labels. For the second set of data, we assign negative labels. We can do the representation. And here we have this data. As you can see, this data is nonlinearly separable. For the training, we first set the options. In this case, -s 0 indicates that we have a support vector machine for classification. -t 0, it means that this support vector machine is linear. And -c 100 means that parameter c is 100. -h 0 is to keep the machine quiet so it doesn't output too many messages. Get to the training. And as a result of training, we have a model. Model is a struct with variable. You can see it here. And for example, it has a variable, which is rho. Rho is equal to -b, right? So if we want to extract the bias, we take rho and change the sign. Sv_indices, they are the positions of the support vectors in the training dataset. And sv_coef is the set of support vector coefficients. These are the values of alpha multiplied times the label. We can type model.sv_coef. And here we have several values that are equal to 100. Some of them they're -100. If we remember that these are alphas times the labels. We see that we have three that are non-saturated. They are less than 100. That means that we have three vectors on the margin. And the rest are inside the margin, or outside the margin, but misclassified. We can do a prediction here. And here what we use is the same training dataset. And the prediction is y2. So we can type y2 = y in order to see the agreement. We have a high agreement. We see that there are some, Data points that are misclassified because this data is not linearly separable. I want to talk a little bit about the data structure. y here is a column vector. I use x as column vectors. But this function wants row vectors, so we have to transpose it, okay? In the prediction, we can use the desired labels, the two labels if we want the support vector machine to predict the error. But if we don't have the labels because this is true test data, then we just put 0s. We put as many 0s as labels that we have. It's irrelevant in order to obtain y2. Then we can do our representation. Here we have it. Remember that we have these coefficients for the support vector machine. We have three coefficients that are not saturated. They are here, here, and here. This is not exactly 100, it's 99.63, but it corresponds to a support vector, which is on the margin. Let's see it. And indeed, we find one support vector, one negative support vector, On the margin here. And we have two support vectors, two positive support vectors on the margin, this one and this one. And the rest they are support vectors that are either inside the margin or outside the margin, but misclassified.