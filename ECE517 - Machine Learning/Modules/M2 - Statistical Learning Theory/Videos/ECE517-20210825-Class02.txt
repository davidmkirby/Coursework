 And also so yeah, there is a lot more from the past one so I can talk about all the time. Let me wait for one more minute. Just basically have to answer it. All right. Hi, Professor. Yes. Okay. Sorry. Oops. I think I did everything right because the last session to start my login. Oh, I need to do it. Yeah. Okay. So so but it's not long the projector. Okay. Perfect. Thank you so much. What's up, guys? If you forget to ensure panelists to share, spend time on that because you're so good mind. I think everything is set up. First of all, last Session, this case on line. So you have to follow me. Just go here. You go to sessions and you will find something, some stuff you don't have to mine this recording just yet. We'll go through them. But this one, it says ST, this is not a special topics class. Right away. It's a regular one. So here is the last class. Play again and say it, and then I will keep posting classes here. Now. So for the, I forgot to mention that we have some additional materials here. Like so you click here. You play in our view and our view of their supplement that Matthias supplementary means that you can take a look to them in order to review concepts and they are explain with HIV with a lot of detail. This is R. In this case, what we have is better, right? So you can just click and view arrives. And it's tutorial super vector machines, which is the order of this first block of, of materials. But if you don't have to read it and not yet arrived, but it's something that I advise you to do, right? This materials, they are not they are dance, right. It's hard to read them at the beginning. So don't worry if you don't really get the ideas first. And once we go deeper in this concept, you will say that you, you will end up by fully understanding almost everything except maybe some theorems that I want to explain, right with them. You can give them. This is not an original paper. It's a Dojo, but it's very well known. And it was published right away after it was published in 1998, right? It's a little after it's super vector machines, very intuitive. And so there are more supplementary materials in the overviews and multipolar. There are two books and they are online, right? All my materials, they are they are online and they are for free, right? Legally for frame. And one is a book on statistical learning theory is for you to reveal, to find concepts that you want to too big. And also this book on Gaussian processes. There's the original book on Gaussian process by aldehyde. Remember the answer? Anyway? It's the original book. It's a very small one and which you will find everything. I have to say that is pretend. It's not self-contained. So it's better to first review the concepts in glass and then you take a lot about Anthony. Yes, you will get everything. Right. So this is a supplement, anything relating to the dates of deadlines and everything. It's outdated because basically what we do is to copy everything from way up to another and end date. We have to put to update all the deadlines and everything. So 100 thing that I have to two checks, right? You don't have to worry about homework. But basically, what I finish up about module, I give you another traditional way to, to, to do. And I usually tell people, is advisable to start taking a look through the camera. I don't know if you have any further questions. So let's keep going with this with jewelry. So well, last session, what they did is to introduce you to some really silly example. And this example was basically intended to introduce some notation and some basic concepts. Alright, and then also to introduce you to the three main, welcome to the three main concepts that we're going to see in machine learning. And general concepts are general framework that we're seeing in machine learning here and its structures. And algorithms, right? Criteria as plural, criterion, a singular. It's silly to say that too, but I, a serious mistake in many papers. They use criteria itself, break periods, right? It's Greek. So such as greedy algorithms, we're going to develop everything this way. Having to struggle of learning machines. All the machines that we're going to see in this class, they have this structure in which I have an input vector x. And this even better is always a column vector. With the, the features I'd be numbers, the scalars. And sometimes, but not always, we will add this one here that will help to integrate a bias inside the structure. And then we have another vector, w. And this will contain the parameters plus a bias. Sometimes we'll put this inside, sometimes outside for our consideration. Sometimes it's better to put it inside and sometimes it's better to take it outside. And there's two, X and W are vectors that live in a space of the org, if you want, here, in this case, d plus one dimensions, right? This number, David and number of dimensions of space, as we will see, is of paramount importance. In a moment there in this class. This will turn into an infant. And this is why this thing is so important. But now, just saying that they as a number that might be very high. So you get rid of any intent to understand the space from a geometrical point of view. Not dramatic of a visual point of view. It's not easy. We can go through two-dimensions, three-dimensions, but we're going to have Ovianne when the number of dimensionless very high and things, things become weird. As an exercise. Do this. Take a sphere, ecosphere in a space of the dimensions. Compute its volume, right? Then put inside a square, another square, sorry, a hypercube are the dimensions. And compute the volume of this sphere. N squared. Compute the volume of the square. And then compute the limit. When x tends to infinity. The volume of a square. Over here. What is the solution of us? Any guesses? It's another sights on infinity. 00, 00, 00, 00. So when you have many dimensions, a space here. This volume here, take, takes all the space. All right, this is counterintuitive. And this is why learning machines that are based on take a class during this phase using hyper cubes, they don't perform pretty well because in a very high, imagine, boxes don't fit anything. Right side there. Universes with many dimensions. People don't use boxes, spheres because they cannot fit anything. So this is some example of how things are counter-intuitive and the number of dimensions increase. And this is why I don't advise you to try to understand things intuitively prime. So sometimes we have many dimensions. Now, back to the structure. My machine, as my machine is dot-product between the pattern x, the data, the pattern. I know the data is plural to be similar is datum, but nobody uses it. So we valid data, or we call it pattern, right? X and W is a set of parameters, the vector of parameters other and this is the vector of parameters. So W transpose times x. If we put the virus outside, we will say w transpose x plus b. Now, transpose means that this vector here is our row vector and another column vector. So we can write this like that. W1 to W2 will be the serine and then B. And here x one to x the right. And this is, and to this sum, w i represents a scalar, xi1 plus b, right? So this is a scalar. It's a dot product or inner product between two vectors. And so this is a core element that we are going to use for this class, a dot product. And we will see many ways of doing dot products. So this is the structure. We are not going to use this notation or this notation we're going to use. W transpose x four. Since this is symmetric, we are dealing with real numbers, or we can say x transpose w. And we will add a little bit. We will use another operation which is a little bit more complex, but formal is taken is exactly equal to this one. We will be able to multiply two matrices in order to get not just one output, but a set of outputs. So we'll generalize this a little bit. But at this point I just wanted one fully understand that dot product, the dot dr is very important. That's the only tool that we use here as a starter. And graphically, we have a set of elements. X is elements, they are piled together after multiplying them times the coefficient. So this is the graphical structure. Now, they said there are two ways of representing this machine, this operation. One is this primer, this is the primal representation. And this space per day, it's up high now steps. And then we will use what we call a dual subspace. So together with this notation, we will be using an additional palpation, which is fully. But in order to prove that it's Fourier goodbye. Sometimes we want really care about the proofs of theorems, but we really need to understand their meaning. So one of the theorems that we will say is related to relationship between primary TO spaces. And this is fundamental because it solves almost all problems that we might encounter when we're working spaces. Namely this, well then to infinity, I mean this. I think the dimensions here coming all the way to list five. So I need to go to, I DO space and problem-solve. Okay? So, but structure, this is the stock. Now, I have a problem. And this problem was to glass in mind two glasses of data in a given space. This is a space of two-dimensions. The data, it has two-dimensions. But since we are a buyer's, we will end up with an additional dimension. But in this case, we can always represent the data, sorry, the output like that, w transpose x plus b. Now access to language. And all. I can do this, I can just change a little bit denotation. And now B is inside of us. So the data, the data has a label. Every sample, every pattern has a label. In this case, we have a set data. Sometimes it's called calligraphic D, which is a collection of patterns. Let's call them x i, vector XI. And every, every sample is associated to a label. So X i belongs to R, d, or d plus one, and y belongs to minus one plus one. Right? So here, in this style of us, will space. Sql API said that the Xn that we use live, do they need last one space? So that means in this case the b is one. The dimension, the dimension is still okay because remember that was behind you and there was a big day. Right? So two dimensions considering this equation, statement, protected have satisfaction. You are lucky. Yes, So this is two-dimensions. So these two vectors, x, that contains the two numbers, it lives in two dimensions. But then here we add the bias and here a one. So it's three. Excuse me. Yes. Could you explain what the plus and minus one represent? Yes. Hello. 1 second. Okay. Is that understood, fully understood. I mean, because it's part b plus 1, so our spear beats B is what is the date is equal to two days, the number of number of features inside of bothered or we have to select that. This is the three-dimensional yes, no, plus or minus one. So here we had a problem in which we wanted to distinguish between horses and cows. And what we do in order to do that is measure their height and a wife. So horses are skinnier and taller. So that will be a horse, because a horse is tall and skinny. Right? And this will be a cow and a Galvez solve. And it's wider, right? So we're assuming that we only have cars and horses. We can distinguish between both. The cow is sorry, is integral, is install. Then just using the height, we won't be able to distinguish both animals. And same with the wine body would put both together. We have a space in which both blasters appears to be separated. And so in order to distinguish between cows and horses, what we know is to put a label here, which is plus one and minus one for the cows. So if this is their height and weight corresponding to our horse, we add a plus one here. And if it's a car with the domain, that will be part of it. And so I have this set of data for this data and AI, this data, I, I observed it. So I take the measurements, but I also, using other means, I determine which is which. This is a training set. The training set, the setting which I know they observation. And I know that inside this box. Yes. Sorry. Yes. Two-dimensional. Why do we need to add a Buyers? Correct. So if I do not have ionise magazine that I have a different problem. And the clusters, I felt like that. If I don't add a bias, my hyperplane in this case, my line will necessarily contain the origin. So in well-being, this and I want to generalize this a little bit more. Well, I use this line. I had a bias and I can place it here. And so doing that is equivalent to add a new dimension. And as I mentioned, maintains constant one. And this new hybrid plane. It goes through the origin but it's not going to be, right? Yes. Well, when you have an outlier, like you've been saying, that it's not exactly a constant. That is a good question. And it needs long answer. So in order to make things easier, let's keep this post them. And I will, you will see the answer. So, alright, bye. Lets in less. And any case introduced the concept of outliers. An outlier is a sample which is not where it's supposed to be. Literally. So for example, we have a sample corresponding to a cup or here, right? Cows are not supposed to eat any of those Whenever, I mean, this, this one, for example, minus one, this is plus one. So the sample minute plus one, sorry, minus one. And this is minus one. So the sample is an outlier. Why? Because if we compute one way or another or estimate the likelihood of my samples, all the samples likely to be inside this cluster. B samples, they will have a high 90. The samples, they have really, really low probability of appearing. And this is why they are called outliers. Outliers may pose problems in our learning machines. And we will see these problems. We will see some examples of ways to deal with this, right? In classic statistics. They introduced what they called robust statistics that are robust against outliers. We will introduce modern methods based on machine learning that are able to take into account this things and in an automatic way reduce the effects that they produce, the distortion of the groups. Any other questions? Alright, so yes, as I was kind of the US for data. So the x will be the training at the y is our expectation. This is a observation which we put at the input or the machine, right? And graphic or this big interview. If you picture a day, Raphael started with the input. And this is the desired output, right? I guess. And then suddenly there was going to do here is to introduce a little bit of rotation. So that as the input, if they observation and this is latent for the test data, right? So if I get a new animal and only have these two measurements that leg out to the IMO. When these y will be blatant, I have to estimate it. At any rate. When I put here an x, sorry, here, or here, an x I input, the output is never going to be plus or minus one. Right? So this notation is not consistent. The output will be either positive or negative, but only the data that are. Let me just go back to the previous diagram. We have this. This is the hyperplane that I choose arbitrarily. Only give me this is the hybrid plan, hi equal to w transpose x equal to 0. Then there are two more hyperplanes for wage. Or a fine hyperplanes that fit this novation. There's properties, right? W transpose x equal to one is a blame. And w transpose x equal to minus 1 is also a plane. And this data is never going to be in a plane. They will be gloves to this. Right? So what happens is that when I, when I take a sample and I put it in my machine, the output is either positive or negative, but never one or minus one. So, so with a set of data with this other data file name, which is a set of samples labeled samples from I between one and n. Sample I have n data. And the output, y output is going to be equal to w transpose x plus b. Wasn't ever. All right, so the error is the difference between the, the sign up, but I will put b inside or outside arbitrarily. Alright. I don't mind if I don't put B is because it's inside of it. So, but in any case, for every sample that I have, I will have an error. Now, we got this chapter and particularities about an instructor. I can be talking for a month. Just about This is Dr. Ryan just the basics to understand everything. Now, here's something else which is the error that leads us to the right. Do I want a hyperplane? That works? That tells me what is the animal through design of the I1 positive four, negative four cups. So since I have an error, what is the gradient that they may use in order to optimize this hyperplane. While reduce the error one way or another, reduce the error. So use the minimum possible error. The error, it has to be minimum across all the data. So how to average this batter? Using some convex function up here. If, if I use a hyperplane which is center around the data, nobody works well or not. If I average the error, the error my Vizio, be the average of error. The error might be like, I want a convex function. Wherever they are. In this convex function, it has a meaning. I have to find this mean. Right? So let's now we are approaching to some criteria. I have my starter, I have an error. I want to minimize a function of this error. And this function must be convex. Happens is that this is not always possible. But in this Clegg and display in this class, it is possible. Because all the structures that we have there like that. So let's use, well, we can prove that, that there is convexity, hey, my criteria. So let's use as a first, easy to understand and easy to apply criteria. The minimum mean squared error. Right? So we have the expectation of the error squared, which is a convex function. The other reservoir is minimum. We have an optimal machine. So this is not something that we can compute because the expectation needs the probability, the probability distribution of that atom. So we cannot compute it. But we make use of the weak law of large numbers. That says that if we have a collection of random variables that are independent and identically distributed, then the sample average of this variable tends to the expectation. Right? So this long about my camera says that he's wearing pants. The probability p-squared differential base point. As to, let's see, 1 over n times the sum of the square. Hence to be expectation. Let's put it like that. If n tends to infinity, we don't have infinity infinitely many samples. But this is all we got around this, that is a double months of theory. We don't need that. Let's kill this and trust that stress that it will work properly. So this is my gradient. And now I changed the error. With the error squared plus for me, I use e i. Let's change the error by an expression as a, as a function of y transpose x plus b over a. And then once we have a solution for this criterion, then what do we have? What is a solution for this criteria that they can apply in order to find w, in order to find the optimal hyperplane. How do I call? Yes, this is Mike radiative. And now I operate up to wherever. And I found a way to optimize w, to find the value of w that minimizes this expectation, right? I found a way to minimize this. What do I have? Got the minimum? Yes, I have anyone ever buy their way to optimize this is what you'd be saving my signal squared. How do I call it an algorithm? Another day? And it was a difficult question. All right? I have an algorithm. There are several ways to solve x, right? And there are several ways to solve that. To solve this, we will see them. So they said, Let's go to I don't know when do I need to? Probably this is the right answer. Alright, so let's, let's go back and see some concepts of machine learning. So that will be an introduction to structures, criteria, our instance, some notation to stomach. Now, let's read your 0.5%. So let's be more general, right? Let's go back to some of the key elements in mature. First of all, depending on the nature of the function that we have, this is my function is one related to the presence or not of levels. We're doing supervised or unsupervised learning. This is an example of supervised learning because I have a bunch of data which is labeled. And then I want the machine to classify between two classes on new sample not previously seen by the machine during strength training data. And this new sum over the test data. That will be supervised because we teach the machine how to classify using the labels. In many cases. In most cases, we have data which is not labeled. Right. There is a paper by Geoffrey Hinton. They won, they won. The leader of the group That's introduce the deep learning about unlabeled samples, about unsupervised machine learning. Because all or almost all the data that we can download from the Internet, it's it's unable. We have pictures. Pictures are YY or no. All right. So if I have 100, It's okay. Let's assume that I have faces. I can detect faces, download them from danger, and they have easily 1 million faces. I did this experiment and I publish a paper on that. So these phases, they are happy and sad, whatever they might have seven different emotions. What do I do? Label them manually. No way. Right? So we have to do other things like gaba to the cows and horses. What if somebody tells me, well, we need to distinguish between both animals. Here's the data and I say, Hey, how about the labels? And they say, I don't know. But the data is, the data. Data is like that. I can always set up an algorithm that discovers the structure of the day. And then I will say I have two clusters. One is the gauss, the other one is they process, which is weeks. I don't know. But I know that I have two glasses of particles and they're distributed into masters. After that, if I want to do a classifier, I will need some samples. A few samples that tell me this is plus or plus one to minus one. And then I will can solve the problem using a different methodology. Unsupervised learning is learning where I done, huh? Labels, I only have observations. And the only thing that they can do is to infer the structure of the data, the geometry of the data, the probability properties of this data. This might be, it might be the case the business to achieve now shops by now. And then I can fit two versions here and here. When I get a new sample, I will tell you. This has a probability of 99% of being a cow and a probability of 1% of weighing up force. That will be even better, right? Because if my probabilities are 64 day now instead of I don't trust it, right? So unsupervised machine learning exists as very important. We're not going to see a lot of it here. But, but it's all about discovering the structure of the observation in two-dimensions is lazy. In three-dimensions, knowing we have to apply algebra, we have a probability, we have to apply other techniques that are able to discover this structure for us. So the learning, learning from a formal point of view is the task of optimizing the parameters of my function. And optimizing. What does this mean? Optimizing? Well, we have to choose a criterion and then we have to minimize or maximize whatever my parameters with respect to this. The criterion might contain a minimum or a maximum. It doesn't matter. If it contains a minimum. We call a cost function. If you condense a maximum, usually we call it a fitness function. There are more keywords here. Learning aggregation or optimization criterion is this. We choose one that is mathematically tractable, right? One might say, Well, you know what the optimal criterion is, the one that minimizes the probability of error, right? This is one we want. We have an animal and when I want to classify with the minimum possible error probability, what happens is that this criterion is not mathematically adaptable. So we need to find a frog system. The minimum squared error than others. And so on. From the beginning was the CMS, the algorithm complexity. Complexity. They have. Occam's Razor says that multiplicity should not be unnecessarily. Alright, this is something that is a Latin. I just don't remember the last word of advice. I always say multiplicity should not be added unnecessarily. How do we read this? We have to use they. We have to find explanations of the data. And they, among all the explanations of the data, all the phenomena that work. The one which is simpler is probably the best math, right? Pdms, coupons stuck the departments to hypothesis really wise, argue that, that didn't leave any reasonable information. Humans or aliens. Aliens are really wise as you know, right? They can do everything, almost everything. What is the simplest answer, right? The architects, somebody say aliens one. So aida, It's probably the engineers, right? Is probably the right answer. So this thing which was introduced by by the friar of Ockham in 16th century regulate. This becomes a mathematical truth. It becomes a theorem with Vladimir Vapnik, which was one of the fathers of the statistical learning theory together with Chairman anchors, Chairman, I guess. And, and, and by midday or both. Students are Kolmogorov. And so they introduced theorems that deal with a generalization capability of my shifts. But there's a generalization capability of machine welding. The flexibility that a machine has to explain a set of data. So if I have two sets of data distributed until Belgians like that or something like that. Why would they still Gaussians are symmetric, isotropic circles and identical except for the, for the mean. We can prove we will that the optimal separating hyperplane is a Stripe Main. That is the one that minimizes the error. But that is another way to explain the data, which is this lying down. Or this classifies the training data with 0 error. But it did then it will not work that well during the test. All right. This machine yes. Range it. There's my share and it has a high flexibility. This has a high probability. In general, if I put infinite points in this whiteboard. And then either this thread, I can classify this infinite points with 0 error. So my machine will have infinite, infinite capacity. This is not good. I had to use the simplest machine that explains correctly the data. Right? So there's theorems about that. We will only see once the fundamental ones and we will not even profit. I just want you to understand. So complexity is what we have to minimize. We have to deal with complexity, we have to fight against it. But in principle way in wisdom. And also regarding complexity, it's worth to talk about linear or non-linear. This is linear. This is non-linear. This is lazy to assault. This is not that easy, but that is our way to do that. And we will need two elements that I mentioned. First, the kernels or the transformation into an infinite-dimensional. And a view of representation is two elements, but let's not care about that. Yeah, this is linear. This is non-linear, period. We need nonlinear approaches. Why? Because usually the data is not that simple. We have several clusters that we have known. All right, and so we will need surfaces that are more flexible, that are able to adapt to a variety of different situations. So we will become non-linear in the second block of this class. I'm always kidding all the properties of the things that we learned in the first one. So here we have, again this example. So here we have y part. That's an estimation. If I put the heart, that means that this is a pure outward and it's never one or minus one in something. And so when I criterion to optimize these parameters w, one of the simplest one is to compute the error. This error, which is the label minus a squared. There are many other criteria. For example, use the L1, which is another 103 pair. If an expression similar to this one, that they are communist, maximum margin is another criteria. We will talk about that maximum likelihood. It's an art material. We will see this to basically other criteria. They are, they are suitable. We're not going to talk about them. But what I want you to do is if you find papers or machines that work with other criteria, you understand what they have there. So they'll come present, okay, yeah, I ended this should not be multiplied without necessity. Here. Fluoride as an exponent, seen any necessity. Make Has he got it? That is what the Occam's razor say. So an English, keep it simply smooth. All right? So in this case, particular case, smoother functions work better than abrupt once. So smoothness as an equivalent to simplicity or low complexity. In mathematical terms of what else? So this has an examples of machines that are non-linear. And in this case, we have a machine which is non-linear. It's very simple. This is a problem which is called the x, or rather we have four clusters of data, is still beyond one does is to belong to another class. So here I worked out solution which is relatively simple icon, they control the complexity. What happens here is that this machine has some errors during their training. There are some training errors. You're going to see them and some blues are here, and some here. Some here are soluble, these are bad here. There's a blue tooth. So there's some errors. But this, by visual inspection, seems to be a correct solution. A good, nothing but a woodland. The solution. It has less errors during the training, right? Still have some errors. I can make it more alive, more complex. So you have 0 training, but this is not the reason, right? Some minimizing the training error, which is what we pretend to do here, is not my work. By not always. In some circumstances. This is Warranty not to work. This is what we call overfitting. Overfitting is a consequence of complexity. Under fitting is a consequence of simplicity. If I put a linear classifier here, yes, number is two symbols. Here, I put a non-linear one. But they keep it as simple as possible. If I don't control the complexity, then here we have overfitting. That means night during the training, the machine is going to work wonderfully. 0 ever. In any case, you give me any problem that you want. I'm able to find a solution with 0 errors. I just need to keep adding about anybody. I can add to infinite complexity framework to our wonderfully for the training data, but not for the task. So during training, you will have some errors during a test or a validation if you want. When you put more samples for which you know days there. Plus education and four, which are the labels. In order to compute the test error. You will see that here. The training error is love that SRR is high. There's difference between the training and the test is called overfitting. This always happens. If I use the same trick here. Thank you, the training error, and then you get more samples not seen by the training. And it will be there. There will be higher, but a little bit better. So here we have a lot of overfitting. Here. We don't have a lot of that confine the complexity is controlling the overfitting. And this is exactly what we want. One machines with. Overfitting is the minimum. And this is what Barney and towering is introduced in order to control it. It's valuable. And when I was there, my PhD, I work in machine learning for communications. And by the end of the PhD, my advisor said, take a look to this super vector machines. They work really, really well. And that was the beginning of everything. I have to go to the theater and I thought this is what it was. Everything was no, my productive. And so and I thought Why have to do a lot of thanks to that. And so almost identical into my papers right before I came here in 2013. All of them, they are about super vector machine sends this partnership. Because I thought this is really cool and it's very useful for that sex. Anyway. So that is another example, which is the double spiral problem. Here we have because pile with noise. And then these two simple solution that works well. You're outside, but it doesn't work well here, right. This is a reasonable solution that it doesn't work that well can the outside, but it works nicely inside. And this is a two complex solutions, solution that doesn't work anywhere. This solution as the minimum of the three examples, the minimum number of training errors. But this is not reasonable for testing Earth. Yeah, exactly. There's a maximum overfitting. Here we have a reasonable solution. Here we have a solution that looks cool, but it's not reasonable inside. So this is complex. It's a difficult problem because and by the way, we'll be able to reproduce these examples easily, right? Actually, you will do that in an exercise. So this is boundless, complex but a difficult problem. Why? Because in the outsides, parallels in complexity is lower than science. So here, a better solution would need some prior knowledge about the structure of the data in order to type functions than that, that introduced much more complexity inside than outside. And this is not an easy problem. And that leads, that will lead to choose on the functions that they use to construct this. And what functions, how am I talking about? Well, don't products, right? This is about product, but there are others. And so buzzing from this to that, it's a matter of changing the dot-product, nothing else. If I choose the right dot product, then I will have as possible opposite five. How do I choose the best dot-product? Right now? Nobody knows. So don't ask me. Well, yes, you can ask me of course, everything. And in this case is more an art than a science. We have to try trial and error. We will sail. And that's it. So main concepts to remember this class or this supervised and unsupervised. What is the criteria and what is an algorithm? And what is complexity and what are its consequences? What are the consequences of complexity? How do we translate complexity, translating to overfit? And this is the concept that we're advised to develop in order to find a great helium and then an algorithm, right? Dose. Right, So then in this case I'll try to update all their webpage, everything. I finish here because I type high tenets, it's going to be a long day today. So thank you very much and see you next week. Just relax. And I was sorry. So yes? Yes, yes. Yes. Yeah. Yes. That's Yeah, that's true.