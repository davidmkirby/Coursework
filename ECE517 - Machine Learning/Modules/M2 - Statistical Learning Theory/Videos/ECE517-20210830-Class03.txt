 It's pretty long? Yes. Thank you. Good morning, everyone. So so far what we need is our brief introduction to some general concepts. But today, we're going to start with what really is this algorithm that I told you? That I told you that was the easiest machine learning algorithm again ever implement. The easiest wide basis version of it was implemented in 19 sixties, maybe. Some of this by Twitter own path. They didn't call it machine learning whatsoever that was accountable to create a new one at a time. By the computers. They were very simple. They could not perform complex operations. It was visible for them to remember matrices. Even difficult to store a large quantity of data. So they were looking for an algorithm that could be used in communications and signal processing that was very easy to implement. And that was based on these criteria and minimum mean square error. And this is what I'm going to explain here as an example of how we pass from unstructured to an average, right? And how this gathering those comes from a great deal. But this gradient can lead to different our time. So the minimum mean square, mean square error criterion leads to what we call the delta of all algorithm. That is the easiest thing. There are many ways to, to, to say in many ways the study it. If you bought two of the classic book, the filled by him. I don't remember the name of the guy when I was. So that is, this is a really, really faint about analyzes this hour and from many points of view. Hey, buddy, nails of this balk at one stupid, they were from cover to cover. You realize that we know how things work only if we make an assumption over the error. And the assumption is that the error must be Gaussian. District pumps must come from a Gaussian distribution, must be independent and identically distributed. Otherwise, wherever you can find as derivations that boat, they are mere approximations. With this. I can say that there's still want to do, maybe we don't have the right math to fully understand what we have when we aren't doing, right? Sometimes basically we don't know. But still we do wonderful things, right? We have cars that drive by themselves, robots that can do particle, right? And many other things. So this is the most simplistic thing. Let me mute this thing. And so this is very simple. We have a set of data. Let's see. Everyone can see the ethics. So assume a set of data which is represented in Poland, diapers, exon. And this center baby has dimension d. And here, four here. Now we put the bias inside, outside, but you know that this can be in size pretty easy. So w now is a vector containing the parameters. So they still w and x laser. So this two, they live in the same space. Hi. And then we can perform a dot product between both. And so these are the, this is the structure of this data here. These represent person wrong by since that is the transpose operator here, that means that this vector w is a column. This is another cup, right? I will always represent everything in columns. As we stated before, we get rid of the buyers by adding an extra, an extra dimension with a constant equal to one is arbitrary. This is arbitrary now. But in the future we will see that this thing might not be, right. So do not take my words as if they were written on stone. They are written over starting with chalk so you can erase them easily. And then w d plus one is what we call formula would probably be a bias this device. Now this is my operations did not round. This, we have seen that before. Now. We assume a set of data base calligraphic be usually symbolizes a set of capital N data. So we have to be next. And for the input data and for the decide output. And then with this, we established a great deal which is to minimize the mean square error of the estimation. So we compute the distance or the difference between y, n and the output of my estimator, my function, which is this dark brown. And we have to minimize this error, this expected bearer with respect to w. And here we include the virus in some nail notation. A new piece of validation expanded our bags, right when I, when you see a capital number, capital letter in both, daddy is a matrix. And this matrix is wrong. It's an array of column vectors, right? Why is lowercase letter in ball? So it must be a vector. In this case, y is a vector that contains all samples, wine, and all the instances of Wagner and so on. Stupid question, sir. That's anyways, we'll do without SEWODA. This I wanted to do is to draw two Reich and a white, or a doctor of that. X is an array of column vectors. And so, um, I guess column of this vector. So what are the dimensions of banks? While it has, it is composed by column vectors of dimension D plus one in this case. So it has d plus one colors. And it has an robes, right? Sorry, all the way around. It has D plus one rows. D plus 1 robes. Tried. Every column is a vector, the debugger dimension, sorry, d plus one rows and n columns, right? And Y is a column vector with n rows. And it's important to visualize this all the time. If you don't have a good idea, just writes, that is d times n. This is n times point. Hi, but we will do that a million times this broccoli then this one's in order to check that everything is correct. It's very important and it's very simple right? Now. So here is a little bit of boring algebra. This is my approximation to the expectation of the error. Remember that the expectation of the error cannot be computed because we don't know what is the distribution of the error, which is in turn. So what we have to do is to approximate the expectation of the error by a sample mean. We add together all the errors and we divide times n. Yes. So I don't even know where to go here. Once. What he, what does he mean in this equation here? That is the expectation operator, right? This is the expectation operator. So this is the integral of the error squared times the probability of the error. By this integral cannot be solved because we don't know the probability distribution of the Earth. Does this answer the question? You, you guys can hit the Spacebar and talk whenever you want right? Now the answer to my question. So thanks. Thank you. So alright. So this expectation is approximated by sample mean. Now, we have to develop this equation here. And here is what we have. This square plus this quantity squared. This is a scalar, right? Because it's a dot product. So basically, this square is this quantity multiplied by itself. But we'll write it like that, which is very convenient. So w transpose x and x times w is exactly the same. The dot-product is and drink, and it's a scalar y. So we ready, it's convenient to write it like that. And now minus twice this times this, which is here. Now. This quantity is Sarah. So we can develop this expression here by first adding together the y's. They do not depend on w, So this is irrelevant for us, but let's keep it at this point. Now. What is the sum of all these values here? It's the sum of these values is equal to this quantity words, I multiply matrix x times matrix X transpose y. And here I have vector Amsterdam, and here I have better xn transpose. If I want to add all these things together, they summation of this quantity. This is equal to w transpose times X, X transpose times darling. Okay? So I will insist on not doing more times, but just write the expressions and you will see how this summation is equivalent to this. Proud of work here, we have now a vector, a matrix, a matrix and a vector. What are they? They mentioned some of this product. Remember that this matrix is the nightstand. This mandate must be n times the than the product is d times d, right? The inner dimension disappears. It's particular case of a tensor product. And so this matrix is d times d. Now, what is that I mentioned, while d part d plus 1 actually are rightly plus what is the dimension of this quantity? It's also the plus one. And this is, it has dimension d plus 1, 2. So this is one times the d times d, b times 1. The outer dimensions are one here, one here, and the inner dimensions here, they much b plus 1, t plus 1. So this is nothing but a scalar. Hey, this is an exercise that I, I recommend you to do all the time to see whether the dimensions are right or not. Many years ago, I was teaching this something similar in Spain. And some students, they came to me with their Wikipedia page for the least squares mean, minimum, mean square algorithm. And they could not understand how things work. And I take a lot, and it was because the dimensions of the matrix, they were up. So I correctly the page this page doesn't exist anymore it with something else. But anyways. So it's very important. All right, when we work with dancers, things are actually a little bit simpler. Model, a little bit more and more simplified. It's more abstract so we don't have to factor my spot where we put or the dimensions or how, or whether there's our columns or rows or whatever. But anyway, so this is the way it should be. And here we have exactly the same. This matrix is d times, and this vector is n times 1. So when we do this product, what we have here is aim is another vector of dimensions, d times 1. Write the n, n one. The outer dimensions. I want That's prevail the inner dimensions too much. So we have d plus one. Alright, not the tip was fine. So this is our vector of d plus 1 times 1, and this is an error of one times d plus one. So the product is scattered. And say that this is equivalent to all these vectors, all this scalar to gap. So then we can pass to the next expression here. In classics statistics. Well, I don't know if classic rock. And also in signal processing. This matrix, this matrix here, which is equal to this product divided times n, divided by n is called, our bees are okay that now this n, it's affects only this product here. And what they did is to put this here. So x times x transpose over m is what they call back. This is an approximation to a correlation matrix, right? So this is, we call that a correlation matrix. And the same happens with this. X times Y. By times n is a sample, is a sample approximation to a cross-correlation vector where respiration is between the input and the output and the desired output. So we have this vector, a correlation, an autocorrelation matrix. And this is a cross-correlation vector. Right? And so this equation, we can solve it easily. Recall that, well, here I explained, I explained how we pass from this criterion. They square error, what we want to minimize this expression here, and we're both are equivalent. Now when we have to do is to minimize the squared error with respect to w. And this is a simple minimization. There is no, there are no constraints whatsoever. The only thing that is left is to compute the gradient of this expression with respect to w equal to 0. Right? This is, this is a convex function. It has a minimum. It's always positive. It's always positive and it has a single minimum. And so is this expression. This one is equivalent to saying, so this expression and it has a single minimum with respect to w. So in order to find this minimum, what we have to do is to compute the gradient of this function with respect to w. And find the point where the gradient is 0. So the gradient will be the tangent. The tangent plane will be, for that, that would be the minimum. So let's compute these derivatives. So if we compute, we'll do decimal tax, right? If we compute the derivative of this expression with respect to w, What do we have? First? This quantity doesn't depend on w, so it goes away. Right? Then when we compute the derivative or the gradient of this expression with this, with respect to w, The solution is 2 times r, right? In this case, the gradient is similar to, this is a quadratic expression, right? It's a, W was just a scalar. We will have w squared R. Okay? So when we compute the gradient, what we have is two W. When we compute the derivative with respect to this expression here, with respect to w, What we have is two. And then we, so we discard this, we've got this already, this two toes. They go away because we want to, to equal this expression to 0. And this is what we know. The gradient of w off of this criteria minimizes the squared error. Mean square error with respect to w is proportional to r w minus p. Now we equal it to 0, y equal to 0. And what we have is minus one. This is my optimal, right? So we call this the 0. Then p goes to the other side of the holiday. And then by pre multiplying by the inverse of R in both sides, then we arrive to this expression. So this is my solution, is a symbol as what is the problem? None now, because we can do that with a very small machine, right? But imagine that you have a computer which is as powerful as, for example, an Arduino. And Arduino, right? It's as small microcontroller with a, remember logical unit inside and with case of memory, some pace, one K for decades, whatever. So there's two problems here. First, your speed of the processor is, I don't know, one megahertz. Clock is one algorithm or less. Your memory. It doesn't hold a lot of data. So if you want to compute our unit as mites, as many samples as possible because R is an approximation. Why? And what same with b? Here you have n data. If n is a lot, then you need a lot of memory. And then you have to compute this matrix landmark that's going to be b plus 1 times d plus 1. Not much. But still, the computation time for a matrix inversion and arbitrary metric summation is proportional to the number of dimensions to the third power. Right? So if you want to know this very quick and you have an Arduino. Good luck. All right? And this is what happened at that time. But there is another problem. We do this. Let's go back here. We did this, make an assumption over the error. What is the assumption that makes this expression? This expression here, similar to this one, an equal 1 when n tends to infinity. When does this one assumption makes that two expressions equal 0 and n tends to infinity. Remember that I mentioned a weak law of large numbers says that when n tends to infinity, the sample mean, which is this, tends to the expectation. This is not always true. Why is this true? When the data is to set up a fourth independent and identically distributed samples, the error stage, they should be independent and identically distributed. And one can say, well, it's not difficult to prove that these errors are independent from each other. It's not difficult. Not that I'm going to prove it now. By identically distributed, it might be that the data is not identically distributed. For example, imagine that what you have is a set of antennas, an array of tasks. And where you want to do is to estimate some signal that comes from transmitters, right? And you want to classify or to do some estimation of from similar gap. And the target is moving. So they, properties of the signal when the target is in one position, they will live pretty different from a different precisions. First, if my target approaches four, goes away from me, taking into account that the antennas, they have a given evaporator and there are some of the tanks. They receiver will produce noise. Is noise, well, the constant will be independent and identically distributed, right? But then my target will produce a signal that whose power will increase or decrease depending where beta agonists. So the signal-to-noise ratio is going to be different and two different times. And I'm instance, now, the signal is not identically distributed anymore. So this is not true, but then they're them. The signal reflects in the ground and obstacles. And so I will have direct signal, signals coming from other from other places that where they disagree with the signals reflected, right? Assuming that my, my target is isotropic, it tends to that mixing on that we work. So I will get reflections. Reflections heavily depend on the position and it had, they heavily depend on whether the environment is also moving. So this thing in communications, for example, radar, in applications where they scenario is not fixed, this approximation is really, really bad. So this, what they did here is pretty cool because we have a solution that we obtained in a blink of an eye with a computer, this solution might be wrong. So we need something which is robust to these changes in the environment. So this expression learns about the environment in order to get a signal which is close to the desired one. So it learns, it learns from the environment. By, since the environment changes the thing, this has to forget. And that was the big discovery of this two guys, we're on path. I think we always have the form. And then the other I came and they had a coffee or something else. And they started two months. And they discover something that was super relevant for communication for many years. So with an approximation to this algorithm, what we want to do is first, to make it a lot easier. So by only need to see the present signals in order to do this computation, I don't have to, then I don't have to store a lot of data. I then can forget about the past data in an optimal way. And then I don't want to remember this matrix. This is kind of annoying because I don't have the power to what is the core? What is the key of that? Well, this is the gradient or this is proportion of the degraded, right proportions. Because we took away the two factor here. These are two that we just took power in its irrelevant. So we have the gradient. And so when we have a given value for W, Where are we? Remember that this, that this L, this cost function is the squared error. It's the average of the squared error. It's a convex function. And we are somewhere out of the minimum. And this is the gradient. So if I am in a position out of the minimum and I have the gradient, well, let's go in a direction which is opposite to the gradient. And so we will descend. And then let's do it again. And we will descend even more until we reach a position where the gradient is exactly 0 or very small. And then we will know that we enrich a solution or we are very close to the resolution, which is probably good enough. So we change this algorithm, which is a block algorithm because we get all the data, all the data. We solve an equation, this one, and we have a solution. We go from this to a solution which is a gradient descent. So it's a recursive algorithm. We compute the gradient, we move a little bit degraded again, we're going to get, and so, right. So this is what we call a gradient descent algorithm. But still, I said that we wanted to get rid of this matrix because we cannot store the data. All right, so far, if we use this gradient, the same thing, we won't get the retails. Now. We don't have to remember this matrix. We don't have this, we have this, we got this and then, and then fall again. I have to do the process many times until I get the minimum. But they don't have to invert matrix, which is But, so we pass from n to the power two n square because I just need to compute a product like that. Now n but deep squat. So, but I don't want to compete on what do I do? This is the idea, right? So this is my terror function. And I know that it only has one minimum. This is a square error. And I have a given value of w is arbitrary. I can start with some arbitrary number, a random number, so that I will be here. Then I compute the gradient, the gradient that we have in the previous line. And so what I do with this is the gradient. What they do is to take my feet my previous solution, w, and I subtract a small quantity multiplied times the gradient, right? And then I will be here. And I do it again. When the gradient is 0, then this quantity will be 0. And the thing, What more anymore? Right? This quantity must be small. Why? Because if I multiply the gradient by a very high quantity, I will move in the direction of the minimum, but they will go a B on it. So I can, and I might end up here. And then I will do a woman in zigzag, which is probably lower than choosing a smaller number, which will lead me to the minimum unless steps by, it might be even worse. If my, if my meal by parameter is very small, then I will end up in a position where the greatness even smaller, higher. So that six eye movement, it will be towards the opposite direction. So that algorithm might be unstable. While my previous one in black, it's not, I'm stable. It will always lead to a solution. Will prove that this algorithm might be unstable depending on the value of. It might be slow if mu is very small, it might be slow if mu is 20. Dead, end it by being unstable if milk is relatively bits. And this is the drama of these others. There are a couple of theorems that say that there is no free lunch. They are called an offline steroids. So if you want something that has to pay something. So if I want an algorithm that doesn't need to invert a matrix, I have to introduce a parameter, this parameter I have two triggered, I have to validate, right? This is what I pay here. Now. Yes, meal is small enough. It can be proven that provided that the data are at the data inside R is identically distributed and independent identically distributed, that will be probably estimate that. And then if mu is not too high, I will always go to the right solution with an error between my present solution and the optimal one that tends to 0, mean and variance. So I will write to a point in which the solution doesn't move anymore, right? By them have to balance. Now, I want something else, which is to get rid of art. I want to get rid of r. I have to pay something else. And where they have to pay is that if I remove r and put an approximation than a mean, my solution will converge to the optimum solution, but not invades. So the solution that they obtain, it will always go moving around there, right? Solution. This is the only about the other thing that I have to pay because there's no free lunch theorem. So this is the optimization. So I don't have the luxury of storing R and B or even computed. What do I do? Well, are you an approximation? This is our rank one approximation. And this rank one approximation is, well, we don't have five, but we have the pressing sample they wanted. We are observing now. So I compute an approximation to i that it has, it has only one vector inside. So this matrix, it has one. And with B, I did the same instead of using B as the sum of all these vectors are these problems. I compute the US, there's approximation the present value of the observation. And recall that this is training, so this barriers, why am I No, No. So if I do this, the only thing that I have to do is to put this expression here, this expression here, and then operate. So I have my approximation to the correlation, my approximation to the cross-correlation. Now I take this x and this x and out, take it out. This expression and this a are equivalent and this is a scalar. This is a scalar. And what do we have here? This is the output of my estimator, and this is the side, the desired signal. So this is the error. And then the only thing that I have to do is to compute the error multiplied times the signal will divide. Thanks. And that's it. This is the delta rho, this is the list mean squares, B, L, M, S algorithm. So and we'll run through the examples. You will see how this works. It works nicely as long as mu is small enough and not too small. Yes. That is available question my adviser asked me this question while I was sleeping in his class. Because I came back from my hometown. I did it every morning and during the night in a bus. And then I have his class. I always fell asleep his class and he asked me this question. And I then I suddenly woke up and I came up with an answer like that. So here's the answer. This, this meal here as a constant, right? But then you might do. And this is, this is called learning parameters as a learning parameter. So you can come up with algorithms that do management of their learning parameter. So by observing the error, by observing this error here, where you can do is to decide if you want a larger parameter to go faster or a smaller parameter to get to a solution which is more accurate. Right? And so if we put a constant, what happens is that, say that we are very close to the solution. We will have an error which is small but arbitrary. And it will be venoms also fun day noise of my receiver or the noise of my observation. Right? So this quantity will never be 0. Right? And I want the solution that comes to quantity here that tends to 0. But this is not possible yet. There is an error because there is an ohms and is multiplied by this quantity here that has a norm. And depending on the norm, this stem will be lower, higher, or shorter, right? So when I get close to a solution, I won't be able to be very small. So I get as low approach to my solution. The very accurate than what they have to do is to observe in the era of x and then decrease mu. And depending on this quantities and they most, the, the easiest algorithm for that is the normalized mean squares, which simply chooses a constant value for a meal and divides it by the norm of x. Right? And what you can come up with other algorithms that take into account, and algorithms that take into account the speed where you did the speed that you have towards the solution. So you add a momentum here, right? This algorithm is a gradient descent algorithm. But then you have the item type algorithms that take into account all this text. So yes, this is something that we can do and there's a variety of algorithms. So if you look, if you browse on the Ethernet, there is an unpublished algorithm, a published paper, which is called Atom. I don't remember what it is that up then something right top Adam, if you type machine learning, you will say they are favoring in archive, right? I think it was never published. So that it's very, very common Muslims. So this is the, is the easiest algorithm for a learning machine. And it has drawbacks because you can get an advantage without paying anything. So here, what you have is that meal has to be chosen properly. If the error is identically distributed independent, identically distributed Gaussian, then the maximum value of mu always the Albany converges, is equal to the inverse of the largest eigenvalue of the matrix R. And this is proven in that book that I, in this classical adaptive feel things by my error is non-Gaussian ion. So we have to apply heuristics. We have to apply something which is a lot less science. And about modern kitchen, right? For everyone, it's not a science but an art. You'll figure out that there's not a lot of science, not a lot of principles behind the solutions that we have. Basically because my error is not Gaussian, then the 10, that is the list mean squares. So let's, of course, here we have same instructor, the instructor one. When we chain shaped throughout the class, we will express it in a different way away, but the structure remains the same. Distance there a question, yes. So does the algorithm better than the last slide? For this is only valid or B, the function, square function. But what if we have a function varies so much? Well, there's always has a single mean. It's always like that. Alright, this is another thing that we will be proving. We will prove that this has a single minimum. Actually, it's very easy to prove that because this is quadratic function is the sum of squared errors are linear functions, right? Google said I will see clearly how this algorithm, under this structure, it has a single minimum. But if you change the stator, you put a leaf and wine, for example, you put a multilayer perceptron in which you have, many of these are fine projections, w transpose x plus b. And you pass them through nonlinear functions. And you combine many of them than the single minimum function. This is not true, but still we use least squares to optimize its functions. Then it's true that we have a landscape. My error. Has several minimum, right? So what I have to do is seal that. Now we have several minimi run here, but they have to do is to initialize the algorithm in different positions in space and run it. And then I will read several minimum. I get the best one. I get. They among all the minima that I got, I keep the one that produces the list. And that's it. There's nothing else that we can ensure that we don't know. We don't know. We're not, because we don't know what is the probability distribution of the error. In that case. It may have minima that we know that we never see whenever rich. There are ways to initialize the algorithm that are wiser than just doing it randomly. For example, simulated annealing right? By, it still uses that. They're reaching the global minima is never a warranty is in general. If we have time in a supercomputer, we can make sure that we reach minimum which original text, right? So all this algorithms, for example, what is the name of this generative adversarial networks? That's big guns, that they do. Things like producing fake faces that are hard to distinguish from, right? Once that machine, two years to work is at the beginning the machine and work too well. And so they had to go tweaking parameters, number of layers, nodes, right? And then I tried trainings many, many times a year because there's no way to find the global minimum. And also we need pyramid. We need to choose parameters property here we have one by in guns, we have many, many free parameters that we have to try and see they work or not, right? Once the machine works, it will work forever, right? But this is what we have today. By I saw it for the first time. I didn't I never pay attention to to the results of the machine. I saw it six months ago in New York. Nice name for that example, I was really nice by this machine does many other things which are more practical than just faking phases. That people use them to have fake identities in Twitter, right? So this is another thing which is very simplistic. It has a single minimum. In general, this is not true. But in our class, we will see algorithms that have a single minimum, so the solution exists and it is unique. And this is a good thing. Carlo methods. More questions. I don't know if I answered your question. I'm a professor, so I park and thought they know that at home make Midwestern part anything. So about so I know that the question they the answer of this question, they they they fish on. The short answer is, I don't know. And many times is like that. I don't know. Alright. But they're shortcuts. There are approximate solutions for thanks. So there's something for that. Thank you. Yeah, we learned one sample at a time. So if the statistics suddenly change, then the algorithm will adapt to the new situation. And this is why this is an adaptive algorithm. This is an algorithm that if needed, it forgets about the class to learn the new situation, which is something that we cannot do. If we apply block algorithms. In this class. We will apply algorithms most of times. Alright bye. There are adaptive approximation for almost everything. So DALYs, one gradient based in plastic. So for the rest of the class of this, of this class, we will assume that the data is distributed given at this. Is this division according to a given distribution. So this distribution is the same for all the samples, which is not always to write. We have to always bear in mind this is, this is not always true. By, in order to simplify things, we will assume that there is still, that is the distribution. This distribution exists. And it's the same for all the samples. Now. Scattered values or labels access for each Theta. So if we have an observation x, then we have a label. What a value for each one of these samples. It might be the case that we know them or not, but they exist, right? For example, we might we might just get the temperature and the pressure and assess the probability of rain. That is a probability of rain that goes between 01. We know it, or we don't, but it exists. And also sense, there is this division f. And there is a set of labels that depend on f, but they depend on x. Or there is a probability distribution, which is that the probability of x given y, right? There is a joint distribution f of x and y. That is a conditional distribution, f of y given x idiocy. All right, so this till they do exist and they are fixed. They don't change over time. They don't change across samples. And this is very important for us to assume it. If this doesn't happen, then we might have a problem. And we will need to add knowledge to my algorithm in order to solve the properly on, we might need to use adaptive algorithms or we might use, I need to use many other things. All right, So thanks. When this is not true, things get harder, right? But let's start from a simple things. And the simplest thing is this. It also, it might be that this doesn't exist. I got caught in a discussion by running a bottleneck when he was in my university in Spain. He says, Well, people assume that this thing exists. Why? If it doesn't, then why would, why do we do? And this guy was, is as a genius, right? Is a total genius. Probably the best mathematician. So, and so he came with a solution for the case in which this is something that we cannot assume. So and we will say, But now this is another way to express things. We have x, the observation, a distribution, join the strict division between x and the labels are scattered values are the values that we want to infer from X. Then we have a machine, and this machine, it has a set of parameters. So far I call the parameters w, and here I bought them Alpha. Well, when I put Roman characters, we will assume that we are in a primal space, in a space in which the data live. And we went when we put Greek, then we will, we know that we are in a defense, which is equivalent is what we call dual subspace. It's always a subspace, not make another space, right? But so now, the other thing that I want you to know now to, to get to know now is that we might have w alpha. They are equivalent. And my function, my estimation, is always a function that depends on x and a set of parameters. And we have to optimize these parameters in order to make this thing work. In order to make the machine infer values of y given x. Right side put the value of x and I get a value for y. And I have to choose an optimality criteria. And under this optimality, what I do, The only thing that I need to do is to find adequate values of f or w. So this is a learning machine. And deep learning process consists of putting adequate barriers here. What's adequate barrier as well? Beyers given criteria, how fluent at it. Now. So. We saw the center in a particular criteria, in a particular algorithm. To generalize this, we just say that we have an estimation function for which we input our value of an observation. We have a set of parameters. And what we obtain is that the side outward plus, plus an error, right? And this is what we call the estimation errors. This might be noise or it might be pure bearer, because my value of alpha, even if it's optimal, it doesn't get there right, the exact value of one. So I've risk minimization criterion or resonant decision criteria. And those criteria intended to minimize a convex function of the Earth. We have, so we have seen one minimize the squared error, which is a convex function. We want it convex because we wanted to have a minimum, right? So this function applied over the error, it must be convex. And we call it L. In our previous example, at L is the arrow. And what we do is to compute the expectation of this error. Ipo in this particular, in that particular case, l was the error. In this case, there is any convex function over the arrow. So the input of this function is the label and the output or the response. We put that inside function which is a combat social. And we compute the expectation of this, of this function with respect to x and y. So this is differential of f is equal to the probability, this is the probability distribution of x and y differential of x differential y. This is what we want. And this is what we call a risk minimization criterion. Okay? And this is the beginning of the statistical learning theory. Or one way to begin with. This is what we want. And this has drawbacks. Is this perfect? Well, if we have all the values of x and y, all possible values of x and y's. And we have this thing that will be perfect. All right, Bye. I have two problems here. If I already have all possible values of x and y, why do I need a machine? If I get that next? Since I have values of x are ready and all of it isn't why I just look, and that's it. So this, this cannot be computed. And also we don't have this function. We know that this function exists, but we don't know. All right? So we need to find approximate. So but for the risk minimization criterion to work, the erase function must have a minimum point where a function of the earliest minimize over all samples are right. That means I really need to have something, because minimum is that exists. And I can find it. And it satisfies that criteria. This loss function L. Remember, that was my error. E squared is a measure about the discrepancy between the output and the label. It has to be like that, right? It doesn't need to be a business. It might be something else. As long as I can find that they can deal with it. As long as it's, it's mathematically tractable. They function is a measure. And it measures how different my label and my decide some I decided that I outward on the label and the output that I have, how different they are. And so I have to minimize the expectation of this difference. And so learning is to minimize an estimate of the risk function when the distribution is unknown and the only available information is in the samples. This is learning, right? So something like looking for a black card that will be in another room looking for other than fat, which is not death, right? So. We want to minimize that function, but we cannot do it directly. And the only elements that we have is a set of samples, a limited set of samples. And we don't have the distribution. Right? So then bottleneck and Chairman, I guess they say, well, what is the difference between doing the same thing which is depths, and doing an approximated thing with samples and without knowing the distribution of the data. Right? And so this difference is actually what we have to minimize. The difference between the right thing and the approximate time. The right thing will produce overfitting, which is 0. Our approximate solution will produce an overfitting. What we have to do is to minimize this overfitted. And there is a lot of theory and I'm going to keep everything go right to the results. Okay? So I know to finish this class, just and I'll rub, right, only this new concept. The previous equation. I call it the risk. And it's what we want to minimize, but we can't. And this is what is called the empirical risk, which is an approximation to the risk. Here. As you can see, by ignore one over n. Because minimizing this function and minimizing this function over n is equivalent. So the embedding of risk is the sum of losses forward one of the samples that they have. We did dance with a minimum mean square error, where L is the square root function. Day when we're going to buy other things. This is what we call an empirical risk. And then I made a question. Again. What is the difference? Why did quantitatively, what is the difference between doing the right thing, which is minimizing the risk. I'm doing this, which is minimizing the empirical risk. How can we apply a criterion that minimizes that difference as much as we can? So this is what is called the empirical risk minimization or differences can lead to overfitting. If the estimation function is complex enough. And we will talk more about this, a lot more about that. So the difference between the right thing and this is what we call overfitting. Remember, the difference between the error during the test produced by our machine and the minimum possible error. That's the overfitting. And it is related, as we saw the other day. Qualitatively. It's related to the complexity of my mission. In my machine. It's way too complex than I will have. A lot more. Overfitting in my machine is not too, it's not communist in our complex enough, then I will have the opposite phenomenon, which is underfitting. The machine won't be able, won't be able to learn. So since this is true, what we need to do is to we have to measure the complexity of the machine. And then do a minimization or apply our criterion, which is not exactly this one, that takes into account the complexity of the machine. So let's try to do this and at the same time, minimize the complexity of the machine. In order to get a reasonable solution. In linear machines with In your machines with not that high dimensionality. This is not too relevant. It might be relevant, but not too, not too much. But then what happens is that we will construct machines in infinite-dimensional spaces. And then the complexity of the machine. Yes. High, as high as the number of available that you have. If you have infinite data, the machine will have infinite complexity. And then we really need to limit this complexity. Or else our machine is warranted not to work. Right? So if you use, if you have a problem of Nero, new Emacs with many dimensions and a few samples. Your machine is monarchy to overfit. It's not going to work unless you can control the complexity. If you use a nonlinear machine, it's going to be even worse. If you don't have another complexity, forget it XML. So we will talk a lot about complexity. During the next days. We end up with solutions that are very easy to implement. 12, very easy to understand. So after we finish this block, this is what I want you to be able to do. To be able to implement such a machine, at least theoretically. And be able to understand how it works beyond the least square error. And this thing that we are going to forget from now, right? Because the least squares, the minimum mean square error, doesn't control the complexity. So in general, do not use it. Let's go, let's go ahead and use this ethics pastels. So let's finish here. And thank you very much. So.