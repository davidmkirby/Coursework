 Maybe it's worth it a little bit. All right. So it's it's I mean, I'm in my car. It says 31, so we start today. What I'm going to do is to wrap up everything, right. So then you can ask questions, whatever you did not understand our patient on or or any other thing, let me know and we will work. Yeah. So the first thing that I did is to define a learning machine. Then some notation of vowels, a lot of them. So they're learning machine for the first thing that we have to take into account, this is our start speaking is always the same set of inputs X. And so that can be like that. So in this example, they input, it consists of 33 different features of a given observation and the constant. And then these features, they are multiply each one by a parameter. So we can say that we have innovation which is now what is this one? Let's call it y hat, which is my estimation. And this operation, y hat is equal to w transpose times x. Or if you want the sum of w i times x i plus b, this can be written as X1, X2, X3 in this example, one transpose times w, 123, and B. So sometimes we will say that x is this, sometimes tags is this. It depends on whether a constant, one is inside or outside or a GRU and only if b is inside on time. So we will be using both notations. If you don't see B is because it is inside and sometimes we call it W forth. So this two vectors, they belong to a space, in this case is four, right? So these two belong to, are four. Hey, there's belongs to R3 mistook, right? And so this is about product and inner products, whatever my guide. And it gives a scalar, a number, right? So there's two belongs to I form. This belongs to pump. That is a real number, That's it. And there's real number. In classification task. We pharmacy to get the education does we all may take care of our design. But we might have a regression task, in which case we care about the number. That was a Muslim. I spoke eyes that, that is associated with R4, the x side, and the w's is the social part. This now was, this is what we call Y hat and estimation of wine. And it belongs to our eyes. For these two, they belong to R four. So it's a vector with four components, nothing for that. So what do we want to do with us? In this class? We are going to see two different operations, classification and regression. In classification, we want to do is to assign a label to each one of the objects that we put here. And in particular, we're going to see binary classification. Where the objects, the patterns, the data, wherever the observation, it belongs to one or another class. Okay, so we might be classifying cats and dogs from pictures. Controls ambitions of a given disease. Wherever, right? Answer. Binary. Classification is characterized by the fact that the labels, they might take only two values. And in this class, we're going to use one or minus one. In other gone that's, it. Conducts is better to assign labels 01. But here we want to care about that. We use y. It's either one or minus one, right? X belongs to R, D plus one, or d, d plus 1 because I put a constant inside. So then as you can see, there is a difference between this belongs to R because it's a real number and it's the estimation on the label. The label is either one or minus one. Say we have a training set, a set of data. We must have the values or the observations and the corresponding label. For example, we have patients and controls. We assign minus one to the control was one to the patient. So they sat for a new patient, for a new subject. In the machine outputs a quantity which is positive. That means that the data has to be positive. Where information, right? But classification doesn't need to be binary. Sometimes we have objects of many cancers. When patients of a given disease. And this disease in my half, as many variants. Schizophrenia. You might have people that have hallucinations or they have or the depolarizations, or they have paranoia, for example, right? Have any knowledge on Madison. But I just worry that for awhile and I have shelters. So we have 43 classes here. Right? We might have many more glasses. How do we do that? Well, there are many ways to tackle this problem using binary classifiers. For example, if we have a problem that has three glasses, 0, 1, 2, 3, arbitrarily. Then we might use a set of binary classifiers. One that classifies one against two, another one that classifies 2 and 3, one that classifies one against two. So we need at least two classifiers. And that's it. All right, there are many other ways. We might say, well, I want a classifier which is one against all. So we classify first two against all, then three against all, then one against all. But the trend is classifiers or the specifier here. What they do is to use, for example, labels equal to 1 here, n minus one here, and so on. There is a paper that is called a defense off one against all. In this paper what they do is to try and many variants of multi-class classifiers. They use one against all, 18 against one. They use qualifications came many others. And they conclude that after many tests with different data, that one I guess I'll just go with some restrictions of books. I buy its own. It's a bigger they're either going for everyone. I defense of one versus all. That's the name of it, right? If I have four classes, for example, then I might say, I assign a code to each one of the glasses. Software bus 1, 0, 0 plus 2, 0, 1, 4, 3, 1, 0 and 4, 4, 11 and an a. But basically to only two classifiers together because they have two bits, right? There are many ways to do that. We're not going to study this in this class because it's pretty straight forward. Right? Forward. I can refer you to other edge of paper as diet or books that describe fully this by auditing is I would want for them. So we're not touristic ourselves to binary classifiers because at the end of the day you will be training by August types. Not only in this class, but if you need, you need neural network. And you have four buses. You might say, well, I have four outputs. And then for a given data frame ensemble, they output than happy, that gives me the highest value is the class at ISI. But I may say I have four glasses, I only need two outlets. And my, my, um, we will call it fine. But I always end up with binary classifiers, right? So you have 128 blesses. You only need seven bits. So this is what characterized education in all cases. What else can we do with these machines? Another thing, not the only one. Another thing that we can do is regression. What is depression? Which says, well, this is an application where they are good. Why are they sign up with y is not a label, it's a real number. It can be also a complex number. It can be a quaternion, can be legit. But we're going to work only with real numbers in this class. Extensions are sometimes not too straight forward. But again, they are described in the literature. So what is regression? They're stuck. There will be the same. The output y is equal to w transpose x plus an error. Same as here. Only the one y hat is equal to w transpose x, right? That is my machine with my output y hat. And then what happens is in this case, there is an error between my output and my and my desired output. In this case, the landscape is as follows. Assume we have a problem on one dimension. So I have a magnitude and I want to compute a different magnitude from baseline. Right? So this is my data X. And the output y. It has, for example, this aspect. So what I want to do is given a new sample not seen in this training dataset, I want to estimate what would be the value of y. And so in this simple case where they DO history, just a regression line. That's what is the expression of this regression line? Well, this is y equal to y equal to w, sorry, w, x plus b bias. Quite align. This case I have one probably in one dimension. If I put B inside of W, It's a product of two dimensions. So when I get a new sample, these are my symbols, x one, x two, whatever. And then I want, but I have another sample X, let's call it x star. Not seen by the trainer, not seen during training phase. I want to know one is honesty of my output, that will be, why is this different from a classification? By this structure is the same. Problem can be described the same way. My output is equal to the desired output I would add to an hour and I have to minimize it. I can use minimum mean square error, which is the classic way of doing things. Right? The first one to describe this process easier for us. Tell me the name of a great mathematician. Anyone that comes to your mind? Yeah. Gosh. He was the one the first to describe that thing. I knew that you were going to somebody you are going to say five. It would say Kolmogorov HIV. Because by the one, the first one to describe this and variance of that anyway, so this is a problem of regression. This is a problem of classification. So here the landscape is something interdimensional, something that God would have two master's degree of Maggie and way we classify, we put separating hyperplane. And this gets separating them. Here. We put the separating hyperplane but a regression hyperplane. So in many dimensions, you, you, you might picture a cloud of points and a blade that fits in. And then that goes inside that. Well, if this graph is flat, then my hyperplane will be a good representation. So we know if we, if we knew the actual value of x dy, well, that's the case in the training dataset. There is a difference between my result, which is here and the actual value which is here. This is the earth. And this is what we have to minimize by now for our particular sample that for all of our sector, we want to minimize an average of a given measured over the emperor and his measure should be complex. It has, they have for me questions. So these are the two problems that we're going to see in this class. We will see a couple more. That 32 non supervised and unsupervised situations where we don't have one mutation. Let's start from the algebraic notation and operations. And I want you to know by heart, by doctors. And W, they are always called backups. And then these are the parameters of my, of my machine and the data that they use in training or test. Now, I have a training dataset, the training dataset for, for, for a supervised learning task, classification or regression. The training dataset consists of a set of data, x1, if x2, let's put it like that. X1, Y1. Few x and y are vectors and these are scalars. And in order to represent this data, I use a matrix and a vector. So when I write X, capital X with two bars or capital X in ball, then this is a matrix that contains all the inputs. It contains all the inputs. So since I have n columns of vectors of dimension D DOD plus 1 SMR, then this, it has dimensions the rows and columns. Okay? And sometimes the representation on this, Yes. Like that. So this is a matrix of real numbers with dimensions d times n, right? The pros and cons. And then I have y vector line petro IRA. And you can see, you can see that y is a vector that contains from Y1 through YN in color. So this is a row. Rewrite it again. This is matrix, which is this row of column vectors. Vector Y is a column vector containing all the labels tolerable, very aggressive. This is, is this a column vector, but usually we represent it like that. Hey, my row vector transpose and it has n diverges. So I was going to say something, oh, yes, I forgot to say that here, this, we call them labels y. But here, why? Regressors and x are usually called predictors and y here, I've got regressors. So when we talk about the regressor, we're not talking about the machine that does the regression. That is a regression machine. When we say regressor, we are talking about why. And when we say predictor, we don't talk about the machine. We talked about the input data. Yes. You said it is. Why is the feed will get mathematically, it's just like, Oh, so you should be buying that excellent. Now it's transpose. This. This is actually something that looked like that. So let's say this is r dimensional vector, it's a row vector, and this is an n-dimensional vector to an insect column at, all right? These two are the same because they put the transpose of paper here. But in both cases, no matter what is row or a column vectors and they belong to space far. I've skipped. Right? When we talk about tensors. When we use dancers rather than matrices and vectors, don't care about the position of the matrices. And then we just say it belongs to R n because we don't care anymore about that. Okay? In order to do the operations, we have to put things in a way that the origin's might say, Hey, by, in this case, this is a row vector transpose and it equals to a parliament. And I did not verbalize it because you will see this many times. And I want you to get used to this. Is that Is it clear now? Yes. No questions. And more business. Okay. So that's that's what we have. Here. We have labels. We have here regression regressors, four y plus four x. And this is our regression machine or an estimation function or whatever. And later on in this class, we will be calling this f. This is f a function. And there's a reason for that, but I'm not going to to anticipated. Yeah. So populations, we simply do a dot product here. We compute dot products. And so I can compute one product at the time. Or I can compute a bunch of the products in a single operation. So when I have vector X belongs to R and I have w, w that belongs to the RB. Then the dot product, that is w transpose x or x transpose w of potassium ion, because it does is to match. And these are product belongs to fire, its scalar. It's a number, a real number. When we use something else that we, when we use, when we don't use real numbers. The dot product, this also symmetric, but then we don't use the transpose transpose operator. You have to use the paramedic operator, which transposes the vector and conjugate it. And with this, the dot product is still symmetric because I broke my base image, right? So this is something that only works for real numbers. When we use complex numbers, we have to change a little bit about, right? So we have to always bear in mind that we're not going to use complex numbers here. So I might compute a bunch of dot products at the same time. So say for example, I have a matrix X that has dimensions d times n, n. And set of parameters where they want to do is to compute the outputs of this are corresponding to this input, corresponding to this input at the same time. Why do I do I simply do this? I can do x transpose w. Let's see what happens here. Here. What I have is a matrix of b times. And I mentioned, so since I transpose b, I have n times b. And this is a vector of d times one. This is equivalent to this X1 transpose to kx. And I don't know why this, so this is the matrix X transpose. This, I suppose, is that. So I have row vectors here, and then I have w. W. When I do this product, what do I have? Well, I have X1 transpose x transpose w up to xn transpose w. What, what is the dimension? What are the dimensions now the result, well, this I mentioned they have too much tensor product and they disappear. And there's dimensions. They are the ones that prevail. In this case where my data is just two dimensions here. One. Here, right? And so this is Singleton I mentioned synchronizing. So what prevails is n minus 1. Here we have a dancer with more dimensions than the rest of the mentioned prevailed access this to disappear. So what we have here is a factor of n rows and one column. So what we have here is w. Well, x1 transpose w to x n transpose w, right? These are vectors. And then basically what I have here is from y one, y n will be the output for my training dataset. And it than it appears exactly the way I want because I want my vector. So we always column vectors. But if I don't really like the aesthetics of that, I prefer something else. I can flip this. I put a w transpose x. They will obtain is a set of elements like that, but in a row. That's it. So when you do the product between one vector in one matrix, just take a look at the name and chumps and applied it. Did I mention the common dimension here, here disappears. The other dimensions, period, That's it. In machine learning. Many books, they use a different notation. They use row vectors poly-time. So when we talk about Gaussian processes, and I refer you to the book, Gaussian processes. They always use row vectors because the guy who wrote it is a mathematician. He is, he works in machine learning. And Zeynep, I come from signal processing. So I got used to list. It doesn't matter. Why do I say this? Well, because different books, different papers, they have different notations. But at the end of the day, we use the same rules. And these are those SIM is a symbol of stuff. You take one convention, you stick to it. And then they won't be errors in, in this, in this case. Now, probability. We will talk a little bit more about villages. All right? We have all conversational probability. So probability is not, at least the basics. Probability is not something that is just understandable for a bunch of rows of mathematics. We all can easily understand that in this class, I will assume that you know, the three axioms of probability. Okay? You don't. So by then, I'm not going to have to just go probable or just go to Wikipedia and ask, what are the three axioms of probability? And the name of another great mathematician, the greatest of modern times, will appear, right? Tagging on mobile. Right? So probability is developed from these reactions is a classical approaches, the frequency of proteins, the subfield approach. It's my right side you just solved to find the, the thing. I will put that slide and you will save out is just this SKActions, right? From that, everything is okay. So I am going to assume that you know, what is probability and the actions. And then we will go right to that Bayes theorem. As I said before. You have to know that better than your own name. And I worked with, but I mean, Kulczynski. They've probably the greatest leading mathematician in modern probability. That was something that Vladimir vitamin B, 12 interviews a day. They said he said to me as a poly-time, he talked to him. So I did my post-doc here with him. Guess what you asked me. In order to start the Bosack was the three. Now the Bayes theorem. Right? And I say Yes. Do you understand the Bayes theorem? Says yes, five. I didn't know. I didn't really understand. And after that does that I always ask my post-doc to describe me. While machine learning, which is called both boosting us by posting and describe it. And I was there at some point and of course I was shaking. So I'm not going to ask you about boosting, but basically, yes, you heard right? Now. We're going to introduce that is there. Now, we'll do a whole month. Sometimes we will talk about probability, sometimes basics. So we have the concept of probability density. What is the probability density of a given event? What is the probability that the temperature in the room is 90 degrees? You can tell me. What is the probability that the temperature now is exactly 90 degrees or anytime you, yes, it's exactly equal to 0, right? Because the probability, because the temperature is never gonna take this day, it's going to be a little higher, a little lower. Right? A probability density is a function which is strictly non-negative. Right? So we have a magnitude, for example, the numerator, and there is a probability density. So temperatures that are closer to this point here, they have a higher probability that city, what does this mean? If I say, what is the probability that the temperature is between 80 and 90 degrees? Then this is a and this is 90. The probability is this interval. Alright? Therefore, if this interval tends to 0, the probability that's, the probability density is non-negative, sorry, is yes, yeah, I say it's a negative function. That's it. I don't, I don't need the function it as proper digital fabric. So the probability cannot be negative. And the integral of the probability is what? What is, what is the probability of measuring any temporal derivative wrong? One. There is some tabular always between 0 carrying and wherever. All right, so while not 0 Kelvin, temperature of the universe is 2. It starts there and it goes to somebody. So the integral is 1. That's it. And we'll represent it like that with a PT or RAM or something, right? And we'll represent what we call cumulative probabilities with a capital number, with a capital letter. For example, the probability of one event, which is that the temperature goes between 80 and 90. This is, this is equal to the integral between 80 and 90 of this probability. Density. We put it as how big the cumulative probability is defined as nets. So what is the probability that the temperature is less than a given number t is equal to the integral minus infinity is 0 in this case because it's 10. Return to 0 of this, probably with you. I put x here. I'm not consistent to differential. He's the fraction of people. This is a cumulative progress. And so we do the integral between 0 and infinity. What do we have, in this case 0, but in general that's going to be minus infinity to infinity. Because the biggest one, I now assume we have two numbers, two magnitudes that we can measure or not. See. We have one magnitude that we can measure it. And I know the magnitude we cannot. For example, the temperature and the humidity wherever. So given that we have what is the humidity? Or Oliver run, we can measure the humidity, but not the temperature. Given that we have humidity, what is it? So we have a conditional probability. So that will be probably different than return given depression. The pressure as an observation is something that we know. So it's not, it's not a random variable is when we measure that. So it's not random anymore, it's a constant. It's knowledge. This is the knowledge that we have. And when it is nice, we want to infer the temperature. If there is a relationship between the camera and the pressure, then what happens is that this probability, probability and probability of T of x. They still today will be different. If I have two phenomena that are related but are not independent, then their knowledge of one magnitude will change the probability of the other. Right? If I have two coins, balance coins, right? So the probability of heads and the probability of tails is equal to 1.5. I have two coins and toss them. What is the probability of that I have two heads? It's 1 fourth, because there's two points, they are independent. So knowing one of the outcomes doesn't change the other outcome, the probability of the outcome, right? So I have four cases, 0, 0, 0, 1, 1, 0 and 1, 1. The probability of one and a variety of Z equal and independent. So there's four outcomes, they will have equal probability. So the probability of two heads is 1 fourth, right? So here I have a case in which the probability is discrete, not here, in which abruptly discontinues abroad is discrete. So I have two independent events. By now. I take this as a single event for different outcomes. Now, I toss the coin, I cannot, I cannot see them. Somebody says, what is a probability of one? Why I say 1 fourth? So I'm going to choose, I cannot, I cannot decide. When somebody says I'm gonna give you some knowledge. The number above. So the outcomes are equal. Then what is the probability of 1, 1, 1 work? So again, it would be 1 fourth. Now the outcomes are equal. It's either too has until attempts to do. So then I, they still appear. And then it's 1.5. So the knowledge of the knowledge of the available day as related to the phenomenon changes, the probability of economic or their observation or in general, are animated. All right, So this is what happens here. Under pressure. They are related. Then when I know the probability, when I know this, when this pressure, then I don't know how she's functioning or I have a different one. And this is what helps me to the side. This is one way of looking at machine learning. If I have probabilistic models and they have knowledge about the phenomenon. If I have an observation, then that helps me to decide. If somebody says they are. The number of tails is obvious, even, then, the probability of heads will be one and the rest visit. Right? So that is conditional probability we want this. We will go back to this in the future because and Gaussian processes, we will be using this expression and we will be using the base rule, which is the core of Bayesian learning. It's the power of probabilistic learning, which I have a probabilistic model, and I use it, right? And then we will talk about priors, posteriors, and livelihoods. So basically we will need three probabilities. Okay? A prior, which is a probability like that, a livelihood and a posterior probabilities like that, they are conditional probabilities. We will play with them. You will see how things turn really, really nice once we fully understand what their base theorem does for us. That's itself now. And this one, we don't need anything else about probability and why do you always ask me anything? I'll try to answer. The next thing is, well, we have a destructor, which is the linear factor, and we have an output Y, which is equal to w transpose x plus a number. And so let's talk about the gradient. Minimum mean square error is one radian. We will also talk about for maximum winding, which is a different criteria. We will talk about ridge regression, which is another pretty. And we will talk about maximum likelihood and maximum a posteriori that are different Brady, right? So we have minimum mean square error. We will have ridge regression. This is used for regression and classification, but it has this name. Maximum amount, you will have maximum likelihood. And maximum. A posteriori means after, prior games, before. But this is locked. Here we are the accusative, the pension I did it posteriorly as a compliment. Compliment and plot them. And we'll stereo is a noun, right? So we use posteriori and posteriorly depending how we write. It's important. This is important when we write papers to the right because many are, they are readers and viewers. They speak Latin languages. Arrives. And then we need to be careful. The maximum a posteriori. This, all this gradient or zeta. This two. They are related. And this is not, this two are related. These two are related. And this, and this they are related. The first to the last two and there's two. There's two. On this. They are related. But they put them in chronological order and the way in which we're going to see them. So whenever we have this and what we want to do here is to minimize, minimize the expectation of the error squared, which is approximately equal to one over n, times the sum from I equals one to n equal to one. You probably know m of w transpose x i minus y squared. Now, this is a great area. Now we need to find an algorithm that implements this. Saturday for us to do is to demo things, necrotic debris bit wait for I in order to do math. And then we, what we have to do is to compute the gradient of this expression with respect to w and v equal to 0. All right, So this is what we call a function cost function. And this can be expressed as 1 over n times the sum from I equals one to n of y I squared plus this, plus this squared, which is 1 over n, sum of w transpose x i, x i transpose W plus minus One over n, the sum of y i w transpose x i. And then this is equal to, this quantity here hasn't changed. And here is what we have here is a bunch of dot products squared, right? So we can picture this as, for example, this thing here. W1 X1, W1 transpose x one to w and x transpose w transpose x times w1 times x1, x2 w transpose x. Right? So if you compute this is a vector, this vector which will be at this dot-product. We have this times this, the second one, the second one down to wn transpose x n steps. So this operation here is equal to this one. And now this factor, as we saw before, can be represented as w transpose x and this product here, and suddenly this vector is the same but transpose x transpose w, right? And this is why I put this expression in their lives the other day. This is just notation and easy from vector and matrix operations. This then we just proved in a way more affordable, this. And this then is the same but transpose. So we take these two and we transpose them like that. And we flip it and transpose them off. We flip, we put x, the second transpose, it goes like that, and we put this first deposits. And so we have this thing which is WD, w transpose X transpose W. And here we have minus 1 to 1 over n times w transpose x. While we're saying exactly the same reason. I just, I lead you to alert you to that. So why this isn't right, let's take a little bit. This is one times D because w is v times one level, we transpose it. X, we know is d times n and Y is a column vector with n rows. So we have this much as these lunches and the result is one day form is now 2k. Then what we do is to compute the derivative while the gradient of this to 0 in order to find the minimum of this with respect to W. Right? So we have this shape in many dimensions. And this is a point where the gradient is 0. So what we do is to compute the gradient, find, find a solution. In order to do this, we can get rid of this one did this here because they will appear in the derivative as well. And this quantity here, we can ignore it because it doesn't depend on w. So there's disappears. So some expression which is exactly equivalent to this one is simply w transpose x, x transpose w minus two, w transpose x, y. Well, the gradient of this, the gradient equal to 0. So we don't have to use this expression. We get rid of wherever it's not important or relevant. And we have this. All right? And finally, let's compute the grade. So the gradient of w, r. Let's save our ears. Let me see how I explain this. As popes. Holy McGill, this as an extension of what we do in one dimensional. So it's equal to the derivative with respect to the first term plus the derivative with respect to the last term transpose. Okay? So we have this big or the derivative, the gradient, what I mean is the greatest. So when we compute the gradient with respect to this element here, is lenient, simply disappears, clean up our being. And we will meet again with respect to this. Then what we have to do is to take this away and transpose it. So we have our W again to find the right. And the gradient with respect to w of w transpose x y is equal to x, y, x y. But yes, I kinda lost with the R, with the arts and let's call it x, x transpose. X, x transpose. I didn't introduce angel use this notation. Why not today sub x, x transpose and this is x. Excellent slopes. When we compute the derivative or the gradient, sorry, of w transpose here in this presupposition simply disappears. That's it. All right. If the element is now pre-multiplying by that is post multiplying, that the element disappears. And we have the textbox, right? If the mean is here, we do not have, possibly be enemies. Here we go. That's it. That's the originator. And you can prove that by Sibley taking this expression, taking this expression in computing the derivative with respect to each one of the elements. And you will obtain is damaged. So we have this. And then finally, we have something which is like that are inserted x x x, x transpose w x y equal to 0. All right, we have a two here. But since we equally everything to 0, I take them up. And now, what do I need to know in order to find w? I know this is a square matrix because this is d times n and this is n times. So this is a d times the matrix. And then I can invert it. Again. I can write this X, X transpose minus one. Well, we haven't proven that this inverse exists. Just believe that B does. That x, x transpose w minus x, x transpose minus 1, x y equal to 0. This is our pattern. And these are all matrices are right. And finally, by the definition of inverse, this matrix times this matrix is an identity matrix. So we can just rewrite this expression like that. I'm finally, w is equal to x, x transpose minus 1 x y. There are other ways to find, as there is an easier way. If I take. And this is, and this is important. And it's bad, it's not good. Right? So I have my y equal to w transpose x. This is what I want. I want w transpose x to be equal to my label or my regressor wherever no error. Right? So then I take always a y and an essay. This must be TO, if bovine outputs are exactly equal to the labels with 0 error. This must be true. Then I pre multiply everything times x and I have x, y equal to x, x transpose w. And then I pass this to this side. You're seeing the same trick by pre-multiply by x is minus 1. So finally obtain the expression x x transpose minus 1 x y equal to W. So what I found basically here is something that it pretends to have a solution to that is exactly equal to the right horse. This is not going to be true because this is, I know that my function, I have many, more, many more equations than variables. Right? It's an overdetermined expression. So, but the point is that I found the solution using two ways, day, long way, the corporate bond and the short way, way, which is not going, it's not giving me the right impression here. I think that this expression gives me a solution for our training data which is equal to all my layers, right? By this software to the right. Yes, it is this what you asked us to do the homework? Whoops. There's more scientists helping you. I know what I'm doing. Alright? There's more. And this has been explained in their, in their slides, right? There is more. And let me introduce a little bit. So what are the things that I want you to do is to extend this standards for a reason that we will see the derivation of the fridge regression, grapes ups. So one way to avoid this thing, one way to make things work better is to reduce the complexity, right? Reduce the complexity. So assume that I have. One regression problem. This is a regression problem is people say why it is very easy for error solution. And now let's assume that what they have is a problem in which my late AND function, my underlying function that they can see and I want to express R2. I want to estimate is this one but I don't know, but there is a lot of noise. There's a lot of noise around here. I assume that I have this, but I don't know. Here. Basically, given the noise, the best possible solution is that it will produce a solution that is consistent with what I am looking. Right? Or let's put another example of this. What is a best? Let's put it or make it more evident. What is the solution that you're saying that it is a good one? It's this one, right? This one. That is a good solution. Now let's apply minimum mean square error or MSE branch. What is the solution? Or maybe it's something that minimizes or the errors the same time minimizes the average of errors or something that goes like that. Probably not that much, right? Something like that maybe. Is, this is a bad solution. Whether we need to do it here. What is the most elegant? Thanks. Oh, well, let's find a solution that is good. But it minimizes the norm of the parameters. Here my parameter is just walk right up to about. So we have w transpose x plus b. W is a scalar, be timescale. Say winning MSW. Basically what we're doing is to decrease the slope. All right? This can be a stop all too many dimensions. And this is why the way to do so. Let's put, let's write a solution that at the same time minimizes the error, the norm of the parameters. And this way, we will ignore as much as we can the outliers. So we want something that minimizes the squared error, the expectation of the squared error plus lambda times W and desist bridge regression. Later we will see that there is a theorem that says that minimizing nodules under given conditions minimizes the complexity of the machine. Well, it is true that if w is 0, my machine will only contain a bias. Is the simplest machine. A machine that only contains a bias is a machine that will probably work better than the minimum mean square error, which is biased by this online. Right? So go ahead, start with the homework. Try to find a solution for this. And then we will interbred at, there is an interpretation. There are many interpretations of this, right? One interpretation is a complexity, right? Interpretation is about numerical stability for ill, ill posed problems. Another interpretation is about, there's another one. I just forgot about. Boston ES, complex data analysts, right? We will see all of them. Probably. But I'm going to do is to ask you, what do you think about this expressions given equations at this point, I just want you to find a solution. I need an algorithm. Actually as you about two algorithms to different islands, they use these criteria. And that's all for today. What steps are you overwhelm or you're starting to be overwhelmed? I went to build wealth. I have a question circuit. So thank you very much and I'll see you next Monday.