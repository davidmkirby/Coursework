 For our simple pocket, the teacher does this bargain. It works. Just like obviously improvements. All right. Again, nice. We can Labor Day. So today we're going to start with some concepts about what we call statistical learning theory. I hope you have a great idea or the concept that we have seen so far. And it's very important to understand that trade mission that we saw together with the algorithm that we use for your world that we live in the whiteboard. And I'm your math in the slides is nothing but an easy example, right? In order to see how these things work. Here, we're invited to see a different criterion. Not that different from the minimum, quite. Right. And this is 13 a day. Not because sometimes it's the confusion what we saw before. It's not related to what we're seeing now. So this theory or the criteria, or criterion and algorithms that we're going to see. They are based on something that we call the structure of race. And this structure, please let me know. This already is that the risk is Something that this in turn related to the complexity or the the expressive capability capacity of learning machines. And which is something that we have 2.4, we have to limit. And this is nothing but an example or an expression of the bulk concentration among all the explanations, of all the satisfactory explanations of a given phenomenon that we observe. The simplest is probably the correct math. On this, which is kind of weak. We will see in a film by Vladimir bottleneck. And, and Chairman, thank yous. I never remember his given name for gas. Anyways. So let's start with this. Well, we call them bonded chairman and gives dimension by money goes first because in Russian, the first letter or make is like a peak. And it's first in and, and they alphabetical order. Nothing else. It's not share. Sorry about that. In order to do it properly. Yeah. Yeah. That looks good. It's good. Yeah. Okay. So, so, so this is a concept that it's the beginning of an explicit measure or the expressive capacity of a machine. So let's, as an example, to introduce this lesson, let's consider the space of two dimensions with others. And these others here are three, exactly three of them. And as we can see here, if these three patterns, they are not in the same line, we can shatter down. What does this mean? We can classify them. No wonder what are the labels in this case, we have, for example, a plus here and minus ones here. And this line can classify them correctly. That if we change the labels, we're going to classify them correctly again. And with any other distribution of labels, we can classify them with no barriers there. All of the flags or all three whites, they are also typically what file. So this three points, then they can always be correctly classified independently of the labels that they have. Now, let's add another. For example, 1 here, which is white. Then Align, Baseline and classify all the points correctly. So we say, we say that the boundaries are running is a mention of a space of two dimensions is three. Because in two dimensions we can shatter, are classified in any arbitrary way. Three points. Three points, that three points and describe the space. Why I mean, what I mean here is is the phone yes. Is the following. So we have a space and then it will put three points in a line. These three points, they are not in a space of two dimensions there, and it's based on what I mentioned, right? So here the rule is we take 1 and then use it as our reference. And then we choose two other points. And there's two other points from this point, from this reference. They should be linearly independent. If this happens, then this happens. Then the dimension of the space that button, it's happening as I mentioned after, the space is equal to the number of points that they use, right? And we have a space. And then I choose 1 as a reference. So put, I put the 0 origin here and managers a number of points. And all of them, they should be independent. They should describe it as husbands. And I'm using the maximum number of points that Jeff, that red vectors are independent, then the number of the dimension, the dimension is equal to the number of point-of-purchase. Right? So here, in a space of two dimensions, I can only do this with three points. So the one-meter money, because I mentioned h, is equal to three. If I take a space of three-dimensions, I take one reference. And then I can put three points that are, that create vectors that are non-linear, that are linearly independent. By this three, they are linearly independent. So, but if I put a fourth one, then this fourth one is not going to be linearly independent. So here, my magnesium ion, as I mentioned, is equal to 4. The corresponding, the corresponding parameter when I guess theorem says that in RD. H is equal to d plus 1. In a space of d dimensions. The environmental dimensions dimension is d plus 1. What does this mean? It means that if I have a space of dimension, I can classify in any arbitrary way, d plus one points. Easy. Flow rate for two dimensional silver. I mentioned three. But it has to be proven space of an arbitrary number of dimensions. And it has to be proven that when he does to infinity, this is true. So they want me to run it. As I mentioned, they're very Chairman, I guess theorem, the corresponding multi-language theorem says that this is always true. And when did as they finish, their family, chairman, I guess I mentioned is in feet. What is the what is a consequence of? Let's say you have a problem in which you, you are in a running with 0.5. Let me see. I think I have a question. So I have a problem with many dimensions. For example, neuro-image problem. Where I have 3D images of human brains is 3D images. They have thousands of voxels. Name I have to 0.5 million bucks or something like that. All right. These are the month be order of magnitude of the number of boxes that I have in a 3D image, in a, in an MRI. And so the space of my patterns, it has, it has thousands and thousands of dimensions. I just need to put all the voxels in an arbitrary sort, sort of an arbitrary, arbitrary vector. This vector has thousands of dimensions. And see 20 thousand. This pretty calm. Now, how many patients and controls do I have? Any of you work in in that topic? I usually have 2101 thousand. I can have 10 thousand. The problem was we had to have them 1000 patients and one falls from a single scanner with exactly the same experiment, it's going to be high. So if I have 10000 patients and controls, but from different scanners that pose different problems. If I want to use one single scan in one single experiment with us, exactly the same sequence sent to the, to the brain of the IPO, Then it's hard to have more than 100 thousand patients. Thank 1 fourth, but my space has 20000 dimensions. But it means is that any linear classifier can classify training patients and controls in an arbitrary ways. So if they give me and I've happened to me and something that I was I did with a mind Research Network. You don't forget, they give me the data. Huge file with the data and the labels. But the labels have been assigned in an arbitrary way. Incorrectly. My machine classifies down with 0 errors. Machine that doesn't control anything. Just minimizes the squared error or minimizes some sort of measure of error. Classifies everything with 0 errors no matter what the labels are, correct or not. And this is obviously, but it's a bad thing. Alright, so if you use a machine that doesn't control the complexity or the capacity or the specific capacity. The machine will classify patients and controls during the training with 0 errors perfectly. And this leads to overfitting. This leads to overfitting. Because when you put new patients and controls. In a machine for test patients have not previously seen by the training, then the probability of error will be much higher than the minimum possible. Obey the machine will not work properly. The machine won't be able to better align whether we have good labels. So we have passively in that labels, machine will go wherever we want. And this is not good. And so def, instead of using a linear machine like that, we use a nonlinear one. The problem is even worse. Your machine is did to overfit. It's one thing to overfit. And here is the proof of the magnitude running a stellar. It's very straightforward, right? If you have more dimensions than data and you do not conform the expressive capability of the machine in any way, your machine will overfit, is not going to work. It's going to be useless. Okay? When you put a nonlinear machine, then what happens is that the maximum, the maximum families have an EKG. As I mentioned, it might be infinite. And in many cases, the actual one meter running because I mentioned that you have it's equal to the number of data is equal to the number of data that you have, half plus one. Right? So with nonlinear machines, they actual varnish humbling, as I mentioned, is equal to m plus one. Because your data will span a subspace of n dimensions, where n is the number of data. And then your machine hours, right? And this is why we need to control this complexity. We need to do things. So, so the, so my, my classification a hyperplane cannot do everything. We have to restrict where this classifier moves. And it is actually very easy by actually we cannot control directly davon, each one of these I mentioned here that, but generally, as I mentioned, the streets Pentagon struck the machine, which the manager bank, as I mentioned, is equal to 2. Well, in this case, it will be possible in this case, right, but not easing. And if my burning ceremony, as I mentioned, is infant and I wanted to restrict it to one person. This is my guide myself. Right? I cannot, in general, we cannot measure WHO when it gives dimension of a machine that has been sprayed with restrictions. So this dimension is, is pretty close. I cannot measure, right? I can only measure the maximum. The one meter, I guess I mentioned, of a linear machine that has no restrictions. And that's it. Alright? So, so far, well, this theorem is not constructed by their proxies to that. We need an earth. It was the US will be the extra lines there without this way. If you'd rather than to say that there are three points, then two of them are linearly independent. Yes. And if you keep adding, Why? Yes. Then all of those that will not be very exotic in this space of two dimensions. If I were boys, they want be linearly independent. So if I have this machine is not going to be able to classify everything Providence. But then I guess transforms this. That's a question. I can transform this. I can do some operation over this points. So under linear regression and I pass them into a space of three dimensions. Why do I want three dimensions? Because in a special field matures them. Omniture running, as I mentioned, is four. I have four points, I will be able to buy them. So initially, this four points, they are here in a space of two dimensions, right? So what they do is a linear transformation that I don't know how to do that, but it is possible. So this points, they are transported here. And this point, this point is here and this state here. So now these points, they are not here anymore. And I can use separating hyperplane, right? That leaves is still under debate and this to overlay blend arrow and you see their image here because I'm not a good one. I'm not a good driver. All right, so what I did is to raise these points into the three-dimensional hyper plane. A plane can split them, can classify them correctly. Right? Now probably. When I have maples, I have a cloud of points here, another point here. And there are points that I cannot classify correctly. In this case, though, they'd probably the best classification of hyperplane is just a straight line made up. Right? By, if I have, I don't know, 1, 0, 0, 0 here and 1, 0, 0, 0, 0 point fear here. And I pass this interior space of 2000 dimensions, I will be able to find a plane. Dye, classifies everything we know about. Right? They brought me to want to use. I mentioned he had a straight point. I cannot shatter more than three points. And that might be good because there's hyperplane as the outcome. Yes. I was just curious, is there is there an upper limit as far as like So for for four points, 33 would be a three-dimensional would be the best. Not just like an upper limit before we get to the Cloud where you just want to cut it out? I was kind of it because I mean, any kind of it kind of seems like goes like a pattern with three points to two dimension, four points. The three-dimension is like 55 flights for that insurance and so on. So all yes, yes, until infinity. And this is good in this problem. This problem, I solve it. I read and buy this problem with the Gaussian blurred here, that identical. I say, okay, let's pass in. Next fall into a space of 10, 22 thousand dimensions. I solved the problem of classifying this correctly, but this is not what they want. I want to classify test data with a possible error. Though. If, let's go back to this example again. What I did here is to apply a nonlinear transformation over the data, right? So all the boys here, they might have an IDE much here. So the image of this two points, they get much of these two points. Here are these two points. And the image are these points here are these two forms. They anti-e, much of these two, or these two and so on. Anyone as an anti image and the original space. What is the image of the plane? Ticket to them? And so this plane as something is, is a set of points that live in this space, right? What would be the set of points in this plane here that go to this blank here. That it's not aligned. It's not aligned. Here. We have a brain, right? But the image of this point here is non-linear, right? Is non-linear because it lays these to where they are. By these two, they are put somewhere else. So they die image is also non-linear. These points, these points here of this cross payoff for listening. So if I do the same and we will see, we will see that accounts tab. If I have many points like that. And then I want to classify them properly with 0 errors, which is not what we want, right? But if I do it, then I bust this points to a space of 12. I mentioned whatever. And I bought a hyperplane. What is the image of the high group length here? Well, something that both probably like that. This is a blend in some space of 12 dimensions. This is not what I want because I, to this point, I draw them from two identical Gaussian. So my best possible classifier is something like that. So this classifies probably all the training data. But it's not going to do a good job in test data. Alright, it's approximate here and here, but here we'll have a huge error. In the particular case. It's better not to change the number of dimensions. In other cases, it might be good to use a higher number of dimensions. What is the problem? Devices that need general, we don't know, we don't know how many images we want. So the transformation is in general something that we've done. That right? That information is something that in general we don't know. Okay, these transformations, we know many of them. But which one is the best? We don't know. And we have two. So we have to control complexity. Dave, classifiers in that space of higher dimensions. All right, So here I am constructing non-linear classifiers. But the trick is not to use a nonlinear function on the trick is to pass the data into a high dimensional space and then use a linear classifiers backup. All right, and this is the core of this class. We will do that many times in many ways and we use several different great, good food. But at this point, I only want you to understand what is the problem, right? The problem with the number of dimensions and the data. In general, we have, we will have situations where the number of dimensions is equal or higher than the number of data. So machines without any kind of regularization of control of their complexity. I totally useless. Okay. So the minimum mean square error, it's a big no-no. Unless we can conform the complexity of the machine that we come up with. That her question, sir? Yeah. Here on line. And this might be something that we begin to later, but I'm just wondering if the number of dimensions well can't depend on the different types of classifications that we want to have for the data. So yes, I like that board. Behind you, you have the white and black dot. So that's two different, two different classifiers where we want three-dimensional, something like that. It's just my first type. Yes. So the number of dimensions that we chose, this the optimal number of times. Well, we are a function of the probability that we have it. We have a cluster here. And this is one, and this is fantasy. Something like that. We have three less than 1 minus 1, n minus 1. Good classifier might be something like this. And we can solve this with space of more than two dimensions, but not too high. Right? We have a problem like this, one cluster here and another cluster here. This might be a good solution. This is again, a transformation to a stays with a low number of dimensions and other problems. They might require a higher complexity. And here the trait, another trait, but the techniques that we're gonna see, they take into account this. So we need a higher number of dimensions, but not too many. Right? I don't know if this answer your question. I don't know where to look. Yeah, Michelle. Yeah. I think that's been a question. Yes. So this is the only use two dimensionless the product. So what I don't know what, I'm sorry, I've done them stereo bus. This theory, reusing, it can only be used dimensionless. The Lawrence know, this can be used in. In any number of images. And so what they said is in a space of the dimensions, one dimension is equal to the plus R1. And this is true for any d. And it, it, it holds when the testing phase. So here's the theorem. I'm not going to prove it by proofs and a dipole by Bob make wrote two books and all that. And then prior to that, there is a book in Russian, but there's a translation by about an HMO, chairman eg is where they explain and prove the main theorems. Also in the plot which is recommended as you have a proof of something and still, but I'm not particularly interested in the proof of the theorems. I am interested in you understanding the meaning of them are rusty on it and said that we have said, Well, I use an eraser, that'll be your progress. So consider a set of n points in a space of n dimensions. Then choose 1 as the origin. Then the m points can be shattered by oriented hybrid plants if and only if the position vectors, the n minus one vectors, they are linearly independent. Right? So in general, if m is less than n, Well, if m plus 1 is less or equal to m, then the points can be shattered. So the corollary is the maximum number of vectors that can be shattered in a hyperplane. With a hyperplane in a space of n, I mentioned is n plus one. But they already, I don't know why I put active. It will put the number, the number of points that can be each other's equal. The number of dimensions of space plus 1, right? And this is y. And this is what we call the biometric is language or the paper by Burke's that you have in, in in the supplement that I don't feel that also contains a proof of that. So so what we have to retain as the maximum number of factors that can be shattered by a hyperplane is called the micelle wanting this dimension. If they, if a state has N dimensions, then h is n plus one. Alright? It's easy to remember. Easy to remember. It's just hard to imagine. I feel like, yes, hi, this is, and it thinks might be gone counter-intuitive. So all those things they'd have to, while you have done a lot of math already and you know that everything has to be put, right. And so we have to prove that. And it's proven that the number of vectors that can undo each other in a space of LA manages the EPA. And this holds for any number of dimensions. And it's also one interesting thing, right? And so what, why do we use the parameter? As I mentioned? We then use to sign. It's a concept which is more intuitive. It's fully theoretical. We cannot use we can use it in two or three-dimensions. That's it. But it's not, it's useless because we are, we always deal with prompts that lot more dimensions. So it's a theoretical concept. But we have to understand what happens when h is very high in relation to the data that we overfit. And the risk of overfitting increases when the challenges I mentioned increases with respect to that. So which is why it says here. And from this, we have a constant, which is the structure of risk. Let me see. I don't remember the next slide. Okay, Yes. So let's say this theorem and we need to spend time with the CRM. As long as you don't understand what it means. Remember that we defined something that we call linear empirical risk. What is the empirical risk? Betsy this equation. Here, we have our classifier f. And this classifier. It returns either one or minus one depending on the classification that it did. Alright, so we with this function f with retain, the sign of the, of the function depends on the bottom, X sub n, I need a bank here, x sub n, because I have n data and eight events on a set of parameters, right? And regardless of what does this function, I don't care. Here, what we have is either plus one or minus one. Now, why that should be 12? Y is the label, either one or minus one. They have actual label. So when they machine classifies correctly, then here we have a two. So plus one plus one, minus one, minus one. Hi, this function returns two. And if we divide in poverty than what we have is our one. When it does what it does in classified correctly. This is what am I, right? Let me start over with a machine does me one. And x is minus 1. What we have is a tooth. If the machine says minus one inductor label is one, we also have a 2 here times two. And so this returns a one when the machine misclassified and rhythms as 0 when the machine classifies correctly. If we divide this over the number of data and capital M, What we have here is nothing but the frequency of errors, or the error rate, the percentage or the fraction of errors that we have over an hour day. So this empirical risk, we want it to be as small as possible. We do. Well. From what I said before, we don't necessarily want this to be 0. We don't, we want to minimize it. But taking into account that there is something called overfitting and we know how it works now. So this is the big risk and the minimum mean square error is an approximation de minimis to the minimization of this right here. This thing as absolute value of it is not differentiable at the origin. And here we have plus one minus one. So this is a and this is on the call this functions here, 14 minus one, not differentiable. So this is not mathematically tractable. We cannot optimize machine based on, based on this criteria. So we use, instead of this, we use the minimum mean square error. As you can see, this is a convex function. It's the same in the absolute value is convex, right? But what we have here is just either one or minus one. There is no way we can do math with that. We cannot do optimization with this, but this is exactly what we want. Minimize this error. And not necessarily the same. And this is interesting because we know this. We know the function, but we know the Big Bang. So that can be seen as an approximation to the probability of error. But we cannot not be the brownie of error because we don't have the same distribution of the data. This is an approximation. So the theorem says that with any number of ability with a given probability, 1 minus enough, which is this quantity here. The actual risk, which is the probability of error, is less than the invading race versus quantity here. Right? So. They add two arrays, is bounded, is bounded by two terms. One, which is they, they, they are ways that we can measure. So we train our machine and the machine will whatever to find the optimal value of alpha. And then once we have my machine frame, we do this. Well, they empirical risk. And the risk, they are related. Through this equation. It says the empiric risk is bounded by the empirical risk plus this quantity that we have here. Now, how do we arrive to this expression, particular expression? I'm not going to prove it. But again, you have several books where you can see the proofs. This is one of the one of the bounds on performance theorems. Alright, so there are several theorems introduced by bounded, by Nick and by others that are then theorems out bounds on performance. What is a performance? What is the probability of error? What we call the actual risk? So now, what does this term? This term is not a variable. It's activated. It depends only on the number of data N here and here. And also on the nature wondering because I mentioned h. This is what we call structural risk. Because both in general, both the number of data N and base, they contribute to this factor. My machine, right? But I've been in, in general, h is given by the struggle of my machine. If I have a hyperplane in a space of d dimensions, h is the plus one. Instructor guess a spy plane. And the dimensions, so pages enables this is called struggles. It doesn't depend on the data that I use. It depends only on the number of a duck, and it depends on page. So what happens when my data tense, the number of data tends to infinity. I have many data as much as I want. I make this tends to infinity. And to invading. Here we have the logarithm of m and here is their language. They remain constant. So I can, I have the volume of and here and here. So this tends to 0. When the number of data increases, this quantity decreases. Right? If I have many data, it might be the case that I overpass than the diameter one, I guess I mentioned. In a space of two dimensions, if I have 10000 data, that can be classified, well, my classifier will probably be good. Because I don't have many data in general, I can say that they have a By, have many Beta in relationship with the number of dimensions. Where they have is some umbilical representation of the distribution of the data. Right? If I have two Gaussians, many data represented in each one of the clusters. I can draw the histogram of the data and it will be close to a Gaussian a half, a rubric, I mean big a representation of the distribution of the data. But if I only have three or four samples, I want see two options, right? If I have metadata, I have, I don't have a name formation of the distribution of Theta, but they have an empirical representation. So my machine, my, be able to maybe able to discover this structure of the data and doing one thing, but More seem simplifying things more with and why higher than age. Thanks, work properly. And I don't need to care too much about Dave, amateur running, as I mentioned. So if I have a few dimensions, three for training and many data, say for example, 10 thousand illegal. Maybe I don't have to care about regularization. Things will work probably, don't worry. Use the minimum is we're alright. Reduce it. In my world. Right? Now, what happens if n is fixed? N is small, which is what happens in many situations. You learn how to meditate. And h is high. Well here you have base times the logarithm of 1 over 8. So when x tends to infinity, this thing increases. Alright? So they invade and risk. Plus the tape, the actual race is bounded by the empirical rule. Quantity increases when age increased. So we need to decrease as much as I can or we can H. So we have to keep aids as small as possible. Now, too small. Because if h is too small, what happens is that this quantity will increase in my machine is too simple, right? So for example, I have two dimensions, but I keep my baggage have I guess damage that for one. It's not going to work. So if H is too small, then this quantity will increase. If Debye micelle, what I do is I mentioned is very high, this quantity will tend to 0, but this quantity will increase. Right? So here we see two things. First, they actual risk increases. If cage increases. So we have So we have overfitting. Overfitting is represented by this quantity here. This is the access error over the minimum budget. And on the other side, we have that this quantity vanishes. N tends to infinity. And this is what we call consistency of learning. Again, there is a whole chapter in one of my next box for it, I think to them. And with this consistency of learning is discussed by here. Basically what we see is that if a machine, it works properly, they, they, they actual risk will decrease or overfitting this quantity here. This quantity here will decrease arbitrarily and will tend to 0 when the number of data increases. They, this quantity, what we call the structural risk, decreases monotonically, monotonously improbability. When the number of data increases, this vanishes improbability. All right, this is a probabilistic event. In probability it decreases. Alright, so I mean that if we try the same thing with the same number of data, twice, but with different data. They marble fittings, they will be different. Right? But if we try again with a higher number of data, then the overfitting or this quantity will be probably smaller. Right? So this is what we call consistency of learning. Learning machines. They have to be consistent. They have to be consistently over fitting must decrease in probability if we increase the number of it. Yes. Can you repeat? I didn't hear your optimal value. Estimate. The optimal place, not in general. So this is, again, not a constructive way of doing things. But two things here. I didn't erase that. I can measure. I'm a strong risk that can be convolve if I can cancel h, This can be measure, but this cannot be optimize. So I had to find something which is similar to this. There can be times. This can be measured. And you're going to be on top. So I have to find something similar to this that can be mental. So I am going to put here something that Is monotonic where the empirical risk, so if I decrease that, they've got the graced, the braces. And here I'm going to put something that changes monotonically with the Birmingham, I guess I mentioned. So if I increase that expression, age increases and vice versa, I have to change this by just x, can't be 4. When I say, Wow, I might do have, I might have some probabilistic model of the data and then control in an explicit way H. So something that may complicate it, that implies to have a probabilistic model of a or the observation. But we're not going to do anything like that. This will be the methods that we're going to see from this theorem. They are agnostic, so they don't know anything about the probability distribution of the data, all the labels. And this has a drawback. We will make two interviews. One parameter, and this parameter is arbitrary. And through this parameter, we will be able to limit or to control. H will see that. So this is graphically what products. This is my eraser. I use it on, we want to 10 to minus three. So for a given machine, this happens with a probability equal to one minus two minus three. So we'd have very high probability. This machine, this is what we are going to happen, is we're going to see this is my race. They are ways that I have when I test the machine. Does the machine with infinite data. And then I have a probability of error. Is this one as a function of h on the one meter, one I guess damage. So my empirical risk or the probability of error that I measure, the error rate that they measure during the training for a given number of data. Is this bad thing. Right? So when I place the varnish I want to use, I mentioned wearing this age. They umbilical error decreases arbitrary. If I add more dimensions, there will be a moment in which the data can be each other. So no matter what our labels, the machine will work with 0 errors in training. Right? Then that is a quantity that I can compute. If I know the values I want to give dimension, which is the struggle of us. Because pretty much like that. When age increases, then the total risk interests. I've done mine this units here. I and I didn't happen to divide this by n. So this units, they're wrong. And this is a made up because this is Amelia occur. But this is exactly what will happen in a linear motion that you use where you can evolve your meter one I guess, damage. So what do you want to do is to minimize the risk or the probability of error during the test, right? The probability of error during the test. So that will be something like that. Around here. They test error will be the mean. But what happens is that this is something that you can see. You don't know what is the error yelling at us. Because you then have the labels. You want the machine to guess the labels we don't have. So there's humanity it, of course. But you see this, and you'll see this if it's coming in control. All right, so when you add these two together, there is a minimum. And this is what we want. We want to be here. And the minimum of these two terms. And say that the wrist, ankle weights I have now is bounded or is less, is on, is under this curve. With a probability 1 minus x. We can see on the video and you use that side of the whiteboard. We owe. All right. So this, this one screen here. Sorry about that. So let me run it. Here is the actual risk, or this is about over the backlog risk. In this graph, we see that my actual race, or the probability of error will be less than pi will be less than this, this, this load current with a probability 1 minus that animalistic. Now almost all ones but with high probability, my whisk will be under this curve. Now, the empirical risk is the probability of error during their training. I have n data. I can compute an error. If I do the experiment an infinite number of times with different sets of data or data. I will obtain this curve for different values of h. And this graph is deterministic and I can compute it. This is a struggle. Always be this board for that week, that events or the number of data, not the data themselves. Today's up but that data and then bounce around this edge. So beware, they're still together with others. And we wanted to find them in. All right, So tomorrow is here. First, I said this curve. I can compute it if I have an infinite number of sets of data and this is not going to happen. I have and they don't. But if I do the experiment many times of training day, nothing many times with data and I average the arrival of the whisper. So this carbon, I don't have it. I have an approximation. And then this thing or another. But what the problem is that they don't roll pitch. I cannot be general or drop it. But in an organic experiment in which we can choose and data for training and some other data for test. And then start over with different data and have as many data we have. But we will restrict the number of data for training to be and we can reproduce this curve. And we will have an indirect way to control H. So this curve itself, we don't have it. We cannot compute it as a function of h. As a function of a parameter the console states. And this is part of the homework. It's very important that you succeed in this experiment, in this homework, in order to understand what you're doing, you will see examples. I will run the simulations. You will see how this appears before your eyes and it appears as the number of experimental they do vapour by the average together. Right? So this is a probabilistic curve. So that is an average of many experiment with, with data, different independent experiments that we average, right? And then improbability, my current, my current tends to exactly this when you use it. Exactly. Okay. Exactly is one except here because there are some other effects that are not going to talk about now. Now. I think this is what we have. Okay? Now. So the remarks, this is the main by Michel running this theorem that we're gonna use the Comstock machines that classify and also regression and machine or do other things. This is the main theorem that we're going to use. I'm not going to use any others, right? There boundaries. There are many ways of saying this thing, but I chose the approximation by arteries works that you have in the bigger. I think it's very simple and condense everything that we need to understand in order to understand what we will call super vector machines. So this is a bond on the risk, onto a risk or the probability of a and, and it's a probabilistic bound. So this happens with probability 1 minus, right? And this one, they began to be 0. Because here the thing doesn't work can be, as Mars was. This bound doesn't depend on the probability distribution. It doesn't depend on how the data is distributed. This always works. Right? And this side is something that we're going to compute because h is unknown. Right? In general. And they've amateur mining, as I mentioned on my machines, they are no, I know the maximum possible monitoring as I mentioned, machine, which is the last one. Right? And then if I am, I mean three-dimensional space. In an infinite dimensional space, then the data isn't a subspace of dimension. So I know that my maximum possible by Michelle running as I mentioned, is n plus 1. Not good. I have to decrease them. How will see them? But we cannot control H and we don't note, in general, we don't know. We have a bound of age. Again. We have abandoned us. Here. The empirical risk. The empirical risk is something that we know, but in an approximate way, right? So from this structural responsible, our expression, we will be able to have a gradient, which is call the end of the principle of structural based minimization. That consists of using an algorithm that minimizes as much as possible age in an indirect way. Right? So we will choose a machine for which h is sufficiently small, but not too much. This principle necessarily includes a free parameter. This parameter is a parameter. If it's very high, then they've amateur running, as I mentioned, will be, hi. If we use parameter value of these parameters, birthday, How many small, sorry, than the one in Germany because I mentioned will be very small. But there is no way. I tried time with my colleagues in Spain. We try hard to find ways to optimize see a device this parameter, there is no way that this parameter can be directly optimize. So a reasonable way to find a good parameter is to do cross-validation. And we will talk a little bit about cross validation. Because, because consolidation is fundamental to make these machines work properly. What this cross-validation, well, we get when we said we put a set of data for training and we hold a set of data for validation. We train the machine where the training data and a given value of my parameter. And then a test with a validation set. And then I change the value of the parameter and I do it again. And I chose they parameter that gives me the minimum validation error. Right? So the training and validation data, they are both labeled data. So they are technically training data. I guess I just set aside a band of this training data and I use it not for training but for testing. This test is called validation. I do that many times with different values of the parameter, and then I choose a value of the parameter that gives you the minimum error invalid age. There are many ways of doing this that are more or less efficient, or they're more or less computationally heavy. If I don't have too many data, then the validation might not work properly. So I have to find the most efficient possible coefficient validation. If I have a lot of data. The same efficient validation procedure will take a lot of floating point operations. So they validation that I do, it depends on the conditions of my experiment. We will talk a lot more about that because I want you to do that. And so I think that they still have, yes. This slide. This theorem is not restricted to any particular class of machines. I. At the beginning, I talk about the amateur running this dimension of linear machines by and this theorem, we are not restricted to any linear motion. It's any machine, right? Any machine for which we can compute or we can determine what is the varnish I want. Maybe it's possible to determine what is the running, it's running in the image. But By way, we will restrict ourselves to linear machines, right? The theorem is valid fraction. We will restrict ourselves to linear machines. And linear machines, the maximum bounce around you're actually carrying of Bueller. And they might be trying, as I mentioned on the machine CAN BE months. Right. So it says that it's visit. I mentioned that because we know the maximum possible gametes are going, I guess I mentioned can be computed as easy. It's the plus one period. That's all I got. My, I know that edge exists and I can minimize it. And why are we restricted to linear machines? Well, for this, this is true. But I got was from a linear machine to anna Lilia. I want using that trick, the trick of changing the number of dimension of my data, non-linear, passing the data through nonlinear transformation. So I have more dimensions. And so the machine will be nonlinear. Again, we will talk a lot more about that. We'll see properties of this, what we call the kernel trick. But I want you to have an idea. Even if we are arrested at the linear machines, we do it for two reasons. First, they visit, I mentioned can be controlled easily in any machines. And then we have non-linear important counterparts that are treated linearly but in a different space. So whatever we learn now, it will be valid for non-linear machines. And so the non-linearity is through what we call the kernel trick. So for this, just regain towards dot product. The most important thing here is the dot product. We will use dot-product only two aspects. Press our machines. This is what we did so far. Right? We express our machine throw dot product. We will always do that. By we only use one dot product. We can use many. And this many products. They are the ones that pass from linear to non-linear machine because there's not always that we use, they are not pro-China into another species, right? So dot product, it's important. We will study all the properties of the products. In general. The properties of positive definite functions, which are dot props, costumes are, uh, if, if you're overwhelmed with a dummy or you can send me an anonymous message if you want, right? Same place. A slowdown. Alright. I don't want to further your gear. I don't want you I don't want to prove how smart I am. How much do I know what they're saying? Nothing about. The only thing I want you to learn. And if it's too fast, please let me know. Send flip and add in my myself, saying load on whatever. Okay. That's it for today. Thank you very much. I'll see you next week.