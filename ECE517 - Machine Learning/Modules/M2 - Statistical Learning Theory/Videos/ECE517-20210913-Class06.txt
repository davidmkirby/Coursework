 Arithmetic Smith papers. All right. So let's stop the summary of private thing that we saw. At a glance. I need to remember to stop sharing. Otherwise, they like or doesn't get recorded properly. I don't just let me know. So we know that this last part, talking about what we probably want me to write with diabetes. Remember that this bargains and running as I mentioned, is measure, an objective measure of the expressive capacity of a machine. We, it is BRAF and theorems after restricted to Euclidean spaces or spaces of dimension d. We concluded diet for a space of dimension B, linear hyperplane can show other people as one voice. So in this example, since we have a space of tools, I mentioned then two dimensions. This is 00, 00, 00, 00, 00, 00. A point with my pointer. I hope you can all see what I'm talking about. So space of two dimensions, we can shut the three points. That means that we can classify them no matter what are the labels. We're talking about a binary classification, right? And so in this theorem, it says, the theorem says that if we have a space of n dimensions and we have m points, we take 1 as the origin of coordinates. And then all the endpoints, they can be each other or classified in any possible way. If and only if the rest of the points, they describe a set of vectors which is linearly independent. In this case, we can easily pass that if we choose one vector as origin of coordinates. For example, this one, the rise, they are linear. If this happens in a space of n dimensions, then be warnings can be shattered and a coronary, it says that since the maximum number of points, the bank can be linearly independent in a space of n dimensions is n, then we can shadow n plus one points. And so, but there is a proof in porous paper. That means that in a space of the dimensions d plus one points can be shattered and they publish on it because I mentioned, is that the plus one. Alright. So that's, that's it. And then using this definition of them, I'm learning as I mentioned. Then we arrive to this fundamental theorem. It's, it's one of a set of theorems. Bounds on performance and learning machines. And so in this theorem, why Barney germinate a sad is that with a given probability, this probability 1 minus U, or I don't remember. So this, this, the following bound spot. So they actually are which of which you can think of as the probability of error of a classifier is bounded by the empirical risk, which is the empirical probability of error during the training. Plus 1 quantity. This quantity, which is this square root here, that doesn't depend on the data, but doesn't depend, but only depends on the barbie terminologies I mentioned H. And it depends also on the number of data. So this one, it is what we call the structural dressing. It doesn't depend on anything but the structure of the machine. The stockholder machine is described by the varnish harmonic as I mentioned, and also by the number of data that we use to train it. And so this, this expression says that when aids tends to infinity, this quantity goes to infinity. When n is infinity, this quantity tends to 0. So F S increases. The structural risk increases. And that means in turn, that we have to control this quantity h. Also, we have a large number of data way larger than that damage. Ironic as I mentioned, then this one, it is not relevant. So we have two-dimensional someone's house and data. Probably hear the army shrank, as I mentioned, doesn't really matter. But if we have been miniaturized dimension, which is larger than the number of data, then we have to care about that because this quantity is significant. And this will be the case in many machines where the dimension will be infinite-dimensional, space will be infinite. In this case, we for sure need to account for the complexity, right? So proven. Well, bargaining probe in the previous theorems that when ancient Greece as the empirical risk decreases, right, during the training. If we have a machine with a high complexity, we have a high capacity of expression. The number of errors in training tend to 0. Right? So the reading of this theorem is they add to our risk is bounded by a quantity here which is the bigger risk than decreases with age. And another one they didn't increases with it. So we have to find the optimum tradeoff here. And this made our graph. We see a machine that of which we can control the complexity. This is something that we cannot do in general, rivalry this particular example. Let's assume that we do. And so when we increase the complexity age than the empirical risk here, arbitrarily decreases to 0. Alright? So here we see that the risk is not 0. Here. The bigger it is not exactly 0. Why? Because probably we have more than 100 data or we have exactly equal to 100 data. But if we increase the risk, the linguist age a little bit more than the arithmetic ambivalence. On the other side, we have they start to address that increases. But then the sum of both bounds, the actual risk. So the actual risk is with probability 1 minus 10th when you're staying below this quantity here. So we have to find the value of h that minimises this bound, which is around here. I don't know, 15 or something that general we cannot, we cannot do this directly, right? But we're going to see a methodology to, to indirectly control this quantity here, right? So, and we saw that remodels that says that this is about a probabilistic bound, right? So this is not warranted, but is warranted with a probability 1 minus p-hat, right? Which gas? Which can be a quantity arbitrarily large. This bond doesn't depend on the distribution of the data. Perpendicular doesn't depend on the distribution of the labels or the distribution, or the distribution of the observations. So it's pretty general. It's pretty general, but it has a drawback. I think I mentioned the no free lunch theorem says, write a rough reading are deaf, no free lunch theorem is that if you want something, if you want a given feature on your machine, you have to pay something, right? You cannot have everything for free machine. The theorem says that. Also, you can interpret the theorem that if a machine is the perfect machine for a given problem, which is the best one, then for sure you will have probably what? You will have problems for which the machine won't be optimal. Right? But anyways, we will talk about that more. When they want to say is that this is a pretty general about because it doesn't depend above any characteristics of the data that we had. That where do you see? Right. But that will come with a drawback. So the left sidebar is not something that we cannot compute, right? That's what we want to minimize, but we're going to compute it so we can minimize that quantity. But we can compute something which is to what it is that has similar to this, whoops, that non-monotonic with them. So these quantities, they increase or decrease with these two quantities than this is what we're going to use. Right? So they ended the principle of structural minimization means choosing a machine with a minimum possible dimension, edge dimension, right? So they bond on, the risk is minimized. And this is an expression of them. Aka threshold. Right? That says that among sets of explanations, satisfactory explanations of a problem of, of, of, of data of an observation. Probably the simplest one is the best one. I said, probably because this is a probabilistic up by. So I like to think that from the point of view of statistical learning theory, they are called Fraser is not a philosophical result by a mathematical truth, right? If you want a probabilistic truth. So while that's something that I also prevented, the CRM is not just dictatorial regimes, but we're going to use linear machines only. And this is what it is, is because we will have nonlinear extensions are the machines that we will construct using what we call the pronoun trip to be presented. So now, how do we apply this thing? Sorry, this is not my best day. So how do we use this thing in a linear machines so we can actually support. So the first two, the thing is about another theorem that I will just mention that says that the VC dimension of a separating hyperplane is minimized. If the norm of its parameters is minimized under some non-restricted conditions, that will say, alright, so in order to minimize h, What we do is minimize w. And that is again another set of films that said I, this is true. So let's consider this estimating function. Again, a linear classifier of which we get on the sign. And we have a set of data, the training data and write this paragraph. It means a set of data that we usually used for training. And the labels that we use are one or minus one, which is what this better for this kind of products. And then let's see how do we minimize that. We see damage. So one way to minimize the VC dimension, and this is actually what is not a straightforward, right? So let's do it step-by-step. And that I will ask you or ask you to give a proof in your homework. It is like. So. Let's say that we have a problem which is linearly separable. So a problem for which we're using a linear classifier, we can classify all the points with 0 error in training. That is what we call a linearly separable problem. Here we have two dimensions and more than three points. Hi. So in general, a problem with Mark that we want is non-linearly separable. But in some cases like this, yes. So that for sure there is a distance between B, there is a minimum distance between the two closest points, black and white points, right? And so what we look for is finding a separating line that maximizes that distance d between what? Well, let's say this is separating hyperplane. So this is a hyperplane for which we want w transpose x equal to 0. Refine it cuts off all the points that make here that lay in this black line. They are 0. And then we define arbitrarily these two hyperplanes, which are parallel to the separating hyperplane, and they are at a distance d. So the symmetry here, alright? And we define them as w transpose x plus b equal to one, and W transpose X plus B equal to minus one. So all the points that lay in each one of those planes, they produce an output equal to one or minus one, exactly one or exactly. Right? So we'll find them like that. And this is what we call the margin. And this margin has a wide equal to 2D, right? This is something that we force. So we're going to look for the hyperplane, separating hyperplane that has the maximum distance D. Alright? So this hyperplane is, it has the maximum possible distance between itself and each one of the voids mark worldwide. And that minimizes that works. Right? So that is the ID, right? So if we let me go to the whiteboard during World War I have to stop sharing screen. So we have a situation like that. Let's save this. So this hyperplane solves that problem. But for this hyperplane, the margin, There's no. But I meant to make it symmetric. So this is a distance Tilly, and this is a small distance. But then we have another hyperbola here for which the margin is why there will be two the prime satisfy the date is higher than this one. Then we will prefer this hyperplane to this one. The trick is to find it. How do we, how do we find this black pride? That for a second? This is of course, something that we can solve a problem which is linearly separable by, in general, this won't be true. So we have two soft and thanks a little bit. All right. That will work for a product which is separable by my promise they want, right? So what I have to do is to subdue the brown allele. So let's do that. That here. Well first, this is, this is my definition of the problem with the separating hyperplane w transpose x plus b equals to 0. This are the two planes that define the margin, right? And so they are w transpose x plus b are the one and minus one, right? So all the points here they produce I will get minus one. And all the ones here are plus one are the points that are in between the margins. So inside the margin, alright, they will produce an output which is less than 1, right? And in particular, they produce an output which is 0 here. This is what we call this, the margin. And we want to maximize. So my machines, under this criteria, they are maximum margin machines. And this is the criteria, right? Formula. We have seen minimum mean square error and also ridge regression, too similar to minimum mean square error. Here we have a different criteria, which is that maximal margin criteria. Now, let's see how this works. Let's crop yields for a given position of these three planes. The distance d, the margin. Well, we define a point x 0, which is inside the separating hyperplane. Here. Alright, this breadboard, there's red point. Satisfies the condition w transpose x plus b equal to 0 because it gets inside separating hyperplane. Now, X1 is a point which is inside the 11 of them object plates. And we know too that W is here. W is a vector normal to the plane. So X1 is equal to 0 plus a fraction of W. Is this clear for everyone? Right, So with a 0 and we add a fraction of w, we find with a proper fraction row, we have X1. And so these two points, 0 and x one, they are separated a distance exactly equal to D, which is the minimum distance between two points, these two points. Alright? And then some, we have this equation, and then we have this equation that says that x is equal to 0 plus rho w. X is any point in this line here. And in particular, we want to find X1. X1 in turn is a point that satisfies the condition w transpose x plus b equal to. Alright, so we have enough equations to solve the problem, right? So we have w transpose x plus b equals 1. And then we have this equation, X1 is equal to 0 times rho w. And then using these equations, we can compute the distance. The distance is equal to x one minus x 0, the norm of it. We can see that is equal to rho times the norm of w. And then, well, I say the algebra for you, you can take a look to it later and then interpret it. You do your, do your own, explain it in your homework. As you can see, everything is nine amperes himself here. And what I want you to see now is that the distance is inversely proportional to the norm of w. So if the distance increases, the norm of w decreases. Which proves that maximizing the distance, maximizing the margin minimizes w. Of course we have to restrict this to a proper classifications are explained must be between they are black and white points. Right? But with this restriction, the distance is inversely proportional to the norm of w. So think of it. I say, I'm not going to prove it, but it's proven a theorem by modern era. Terminated. They, they are, H is minimize it. A badge decreases if w decreases, w decreases the ingress. So minimizing w. Is Minster thanks. Minimizing or maximizing the distance. This is why the machines that minimize the complexity, age. They maximize the margin, and this is why we call them maximum margin machines. Not only the support vector machines aren't maximum margin, there are others mandible or the boosting machines. We'll talk about them at the end of this semester. They also maximize the margin. And so this is the main AT write my machines. Minimize the distance and maximize the distance. B, which means they maximize the margin. Now, this is first degree linearized, right? So, so what is my criterion in numbers? I have to minimize the value in order to minimize, in order to minimize h. But I have to restrict it to this. If my my dad is white, then they are, the outputs should be higher than one. In my dogs is blood. Then the output should be higher than minus one, sorry, lower than minus one. So if I multiply their response or the machine, There's one times the label, the results should be always hire another one. Right? So here you have the gradient that allows me to find a machine that maximizes the margin. And I have to find a machine that satisfies this condition for all the training data. And at the same time minimizes w, right? I have to find the machine with minimal w that satisfies this. And then I will, I can start with the machine I end up with. All right, so this is not yet what we call a support vector machine. And then it will be clear why we call them superannuation will modulate. All right. This is not yet my machine. Why? Because this is a machine that assumes that the data is linearly separable, which is not true in general. In the data was linearly separable problems, there will be very easy. But sorry 10 that we just presented a method that minimizes the risk in linear machines by explicitly, explicitly limiting the Barbie children, I guess I mentioned by not directly, right, we minimize the family Germany because I mentioned by minimizing w by the previous classification problem is not realistic because we can classify everything. So then gamma rays WorldCom 0, right? And in real gases, the data is non-linearly separable. In a real super vector machine, is that defining empirical risk? And at the same time minimizing w. And how do we define an empirical risk in these conditions? Well, we have to assume that some of the samples, they are inside the mouth. Are they are, they are outside the margin but in the wrong position. So we have to account for the samples that are not in that that doesn't that doesn't satisfy my condition. So how do I do that? Well, this way, let's, let's see this, this graph. In this graph, we have a problem which is clearly non-separable. So here we have white, white, and black dots that they are mix. So it's impossible to fit a linear classifier to classify everything together. So let's put a hyperplane that we think that it does the work correctly, like this one. And I will say that some of the samples. Or inside the margin. So this samples, they do not satisfy the condition that w transpose x plus b times the label is higher than one. In those cases. In those cases, the label times W transpose X plus B is higher than 1 minus a quantity, which is this distance. And it's always positive. It's a positive quantity because they result of this product is less than one. And this is what we call the slack variables or losses. So, so just don't mind this sentence here. I don't really need to erase this, this one to this. They are always positive because this product is always less than one inside the margin, right? So if a sample is correctly classified and outside the mountain, then this quantity chain would be negative. Right? We do not care about samples that are outside the margin. So we force all these slack variables or losses to be either positive or 0. Alright, so if we see assemble outside the margin and probably classify, then we declare its associated slack variable equal to 0. Okay? So in this case, then Y times X plus B would be higher than one. Perfect. So we define this. And what do we do next? We add all these slack variables together and minimize the sum. Alright? So let's change my restriction from the previous one that was this. Okay, that very naive to this one which is more realistic. All the samples they might have a lapse. They might be inside the margin. Let's quantify the lungs team for each one of the samples entitlement. So let's use this constraint. And now let's minimize at the same time W and the sum of like videos. So the empirical risk in this case is substituted by the sum of is like variables. All right? This should be approximately equal. So this is a proxy to the invaded range. If the sum of these variables is 0, then the empirical risk will visit. With the sum of slack variables is not 0, then the empirical risk might not be. Right. So when we have to do is to minimize this and rights and this increases when the empirical risk interests. And we know also that when W increases, it increases. So then we have our criteria which goes along with the parameter one. It is Theorem, which is this one. We minimize at the same time, w squared plus a constant times the sum of the slack variables. And the slack variables, they are defined through their condition. And this is a, this is a super vector machine. Now of course we need to know how to solve them. Stance. There's a lot of tedious algebra. I will save you some, some algebra I want. Okay? So this is the support vector machine. Why don't we see here, well, C is added because the, some of the slack variables in it behaves in probability monotonically with the payment risk. But it's not equal to the empirical risk. Right? And the same happened with w. So if we put z equal to one, so we just rewrote C. We will have a minimum for this meeting. We will not be necessarily equal to the minimum of the theorem. So we have to find a value of c for which both the minimum or the act of a bound on the actual risk and the MIMO on this quantity. They are similar. How do we do that? Well, we will need cross-validation. We will talk about this played by PEG. Of course, we need the constraints because they are telling me that the separating hyperplane should be between both glasses. And that is the way to define G. And G should be positive for 0. We have seen a problem before with which is a minimization without constraints, minimum mean square error or ridge regression. They are brothers to minimize. Here we have a different class of minimization. Minimization with constraints. How do we minimize things with constraints? What the next week? Any guesses are? Lagrange optimization, right? Lagrangian optimization. It's not difficult to understand. It, not difficult to understand, but we'll explain it from an intuitive point of view. It's a little bit more tedious than just gradient equal to 0. Maximization or larger expression. Somebody might buy. But here we have a 1.5 here. Then to write this here, as we continue to be placed in a maximization of the matter, the expression was a much deeper was after a few. But here you introduce this way. Yes, because I want to maximize the margin. So I maximize 1 over w, which is equivalent to minimize or an expression plus w of u. The W squared is 0 if the norm of w is convex expression. So, but it's, it's difficult to deal with. If we minimize w squared, we also minimize the value. That is way easier to write. Good question. Why do I use w squared here? Well, it's because it's way easier to do it. By. If we minimize w squared, we minimize the norm of knowing that that's an alcohol. Yes. The square root of we have since we have a C there, we can as well put another constant to w. It's every day with one over two because then when we compute the derivative that the today's goes away, right? So, so that's that, That will be the concussion of this class. We will start with the next. We will start with the next module in a few minutes. But let me just coming off of glaciers. These are the things that we have seen in this, in this last, okay, First the vantage, um, I guess dimensional edge. And then VCB theorem that introduces the trade-off between the empirical risk under structural replaced. The empirical risk is the risk that the training is that their risk is a term that increases with the complex a, B, H, or the manager again, beverage. They, a bigger risk decreases with a manager running, as I mentioned. So since the actual risk is bounded by the sum above, we have to find a trade off, right? Using this principle that leads to the support vector machine. So here we first use the concept of average amount because the image. Then we introduce a principle which is not a great deal because we're going to use red this principle as they struggle with minimization principle. And then with this principle, we construct the criterion, which is this. The criterion is minimized at the same time, W plus constant times the sum of slack variables subject to this constraint. And now we need an algorithm. And this algorithm is found by minimizing this using, using black rights optimization. So that's an a. So we found the functional to be optimize. We call this a function, right? It's for us as a linear combination of functions. So we call this a functional and we call it a prime on, sorry, PyMOL functional because it's expressed in terms of primal variables, variables that live in the primal space. The space of the dimensions. W lives in that space, x lives in that phase B. And also baseline variables t, right? They all live in this primal space. And of course, if I say this is because then we will go to a dual space. But this is not a big deal. We will see how it works. Because there's no questions about the homework. Yet. Again, anytime you draw me with questions about an hour in class, all right, that's going to be beneficial for you or your classmates. Also. I try to answer the questions they put him in the formula, right? So let's go to the next to the next module, which is here. Number three. Well, silver metal machines for precipitation. So I assume that the pension expense. So we have sent this, sorry, this criteria where c is a free parameter, it's a free parameter. We call, we call them free parameter. The parameter is for hyper-parameters, right? And this is because they are free array hyperparameters because we cannot optimize them to write their parameters that can be optimized. So we have to cross validate them where to go and try what is the best possible value of the parameter for us, we will find some more. But this is the first one. And this is a trade-off parameter. And this is that trade-off. This is what you have to pay for how many criteria that doesn't care about any particular structure of the data, any particular probability distribution of the data. The moment we are as trumped up in the data, we assume that the data has a given distribution. In particular, the labels. Labels are the regressors y. Then this free barometer disappears. Right? And then we have a bacteria that doesn't need to gradually add a parameter. But then what do we pay? Well, we have to pay that. We're assuming a given distribution for the data. If it's correct or it is good. If it's not correct, then we might have a problem. If it's correct, but it's very complicated than we might not be able to solve problem, right? So we always have to pay something. In this case, we have C. Now, we need to optimize this using Lagrange optimization. You can refer to, to the board by Dan Simons and functional analysis or many others that you might find in the library about optimization with constraints. But all we need here is the basics about lagrangian optimization. And I will explain them. So we have to minimize a given function f that depends on a set of events and a set of variables w. And then we have a set of keyframes, and we call them g of w equal to 0. So this constraint there hit the right. We can put a hero. The higher. Recall that in the limit, what we want is that for these constraints are equal to 0, equal to one minus g here, equal to z. So. We can put this quantity. Here are the other side of the expression. And in the limit, we will have conditions which are something equal to 0. If and when you have some linear constraints, they are part of that. They are linear. Linear birthdays that are something equal to 0. They define a hyperplane. At the same time, we have a function that we want to minimise. And here is why we put w squared. Because this is a quadratic expression. So this functional, this functional LP, it, it is a paraboloid in a space that this paraboloid seen from above. It has an expression, it has an aspect like that. So we have the paraboloid with our absolute minimum, which is right in the middle. This is a family seen from above or from the heart, from a lab for it. Alright? So there is an absolute minimum right in the middle, for example, here is minimal, Is Trigger and we don't want it. What we want is the minimal subject to the constraints. And the constraints are here, there are hyperplane somewhere. Now is the minimum. The minimum of the expression inside the constraints is right here. Right? So if we go in this direction, we woke up. In this direction. We also go up. You see down. Is it clear for us? So the minimum is here, right at this point. And what is special about this point? If we compute the gradient, demonstrates the gradient of this process. It goes in this direction, right? Or the minimum of day, the minimum of the functional because in the same direction. All right, so if we change the sign of them, of the gradient or the functional, it goes in this direction. So this is minus the gradient of f. So these two gradients, they are linearly independent. They are in the same line. So we have to find a point. Simply. We need to find a point where it bought gradients. They are in the same line. They are linearly, linearly dependent. How do we express that these two are linearly dependent? Why we put the gradient of the functional plus a constant or a multiplier, the gradient or the constraints. And that should be equal to 0. So when we then is essentially when willingness to change. I'm constrained minimization unconstrained problem by one which is not constrained by putting the constraint inside the minimization. Right? So this is the idea. We find the gradients above. And we force them to be linearly dependent. Then means that one gradient plus a constant times the gradient is equal to 0. We don't have one constraint. We have many cell death. This is what we will do. In this particular case. We put the functional minus Alpha times the constraint and we compute the gradient. And this gradient should satisfy this. When he does that, we bought what we have found, the value of w. This is a function of w that satisfies this condition. So this is the value of w that minimizes the functional and at the same time satisfies the constraints. Here is just one. What did we have many? Well, we have to put the many constraints together. So what we do is we take these constraints and each one, we multiply it by a multiplier, a Lagrange multiplier. Multipliers that I used for these constraints. I call them Alpha. The constraints for this, sorry, demo device for these constraints, I call them mu. I have N of those because I have capital N data. So what they know is to multiply each one of these constraints times multiplier. I put them there, right? I have to I an additional strain. These multipliers, they should be positive, otherwise positive or 0. Otherwise the solution becomes trivial. Otherwise the solution y, 0 equal to 0. So the Lagrange multipliers, they are always positive or 0. And it's very important, we'll see now, as I said. So this is my dual, sorry, if this is my Lagrangian function. Here we have the original functional. And here we have the sum of constraints, each one multiplied times 1 Lagrange multiplier. And of course, the gradient of this undegraded of this, they are in the same direction. So we have to either minus here, right? Remember that I said here, I should put a minus, a minus here and minus here. It doesn't really matter. But distal radius they have, they're going the same direction. So I have to change the sign of what had been this case. And in this case, alpha will be alpha mu, there will be positive. If I don't put a minus sign, alpha must be negative, which is positive alpha minus sign in the constraints. Or a minus sign in, be. In a primal functionally work as one, which was this arbitrary. Sounds. The primal variables, as I said, our w and cheat and B. And we will see that the dual variables are alpha mu nu. They will disappear. Once the arms. This is the mechanics of what is, by way of explaining in a tutorial. And you have a super vector machines, it's, it's fully explain. Well, fully explain. It's not fully explain any paper anyway, right? I wrote a paper in a while ago. I don't remember. I think it was my second paper and I actually was not my NIH funding it in in in a printer of my environment. I find it. I take it. And I said, No, this is wrong. And then a guy said, what wrong guy behind me said. And I say, well, yeah, so we solve the problem with the paper and we became friends forever. So we published several books and many vestibular, restore it to the right. So in a bigger thing is fully explain. And I think it's daily revenue which they support vector machine is fully explain beyond what we have here. So I will recommend to read that paper by knowledge. So we have this k and then we compute the gradient with respect to the primal variables, which are w, b, and t. And so the first result that we have is the gradient with respect to w. Where w with respect to w, we have this silly expression here. If we go back, we compute the derivative of this. We had, we have, WE saw degree of this after you have done, right. And so we compute the gradient, but this expression, we have simply alpha n y n, xn, and the rest disappears. This. And that gives us this result here with basename pass this quantity to the right side. Then we have this expression here. This expression, yes. It states what we will, how the representer theorem, right? Try to remember this expression representer theorem. Because we will use it a lot. Here we didn't need it because we found, we found a result that matches our original field. If you are asked to read this at a glance, you would say w is a linear combination of the training data. And that's all right. Other some non-restrictive conditions. The solution is a linear combination of the data. We have found is already yes. Yes. Mean square error. W is equal to x, x transpose times x, y. Where x is a matrix containing all the data. X and y is a vector containing all the labels. Right? So even express this as a linear combination of the data. If the data is multiplied times its label here. All right? Then each one of the data is also multiplied times a parameter Alpha. And then you can take out of here. Right? So this is a linear transformation, this is a matrix of a linear transformation. And here we have all the data multiply by its label. So w, a minimum mean square error, is a linear combination of the data eraser brush. We have exactly the same one, sorry. W is a linear combination of the day. And here we have again, same effect on w is a linear combination of the elements or the combination r alpha, y, alpha is the Lagrange multiplier. Y is the label, right? But they like to express everything in matrix form because it easy, right? So w can be expressed as a matrix X, capital X containing all the data in columns. Right here we have column vectors times a matrix, which is a diagonal matrix that contains all the labels. Minutes I LL, right. So here we have, if we do this, we have each one of the Nadar multiplied 9. It's types, it's our label. And then it here. Then after that, we multiply everything times a vector mundane office. So this expression here, why do I have a transpose? I'm going to erase this transpose. This is wrong, right? It does both. I need to take note of this, this takes to correct that. So x and y, this expression here is exactly the same as this expression here, but we do not use summation. It just a matrix manipulation. Rights. Why is a diagonal matrix containing all base labels and alpha is a vector containing bulb and international buyers. Saw that the same way as we saw. So this is the first result. W is a linear combination above the date. Now, we need to find Alpha. What is Alpha? We need a little bit more algebra here. Seven, we can recompute the derivative with respect to this like variables k and with respect to B. So we compute the derivative with respect to t, we obtain, we obtain this similar. Similarly, this is an expression here, right? So if you compute the derivative, your focus is. Okay, the, with respect to t, we have c times c minus c minus alpha minus mu. That's a C minus Alpha minus with the rest it goes away, right? It's just one derivative with respect to one value ci. And so we have this and that if we compute the derivative with respect to B, we have this expression here. So the value of the sum of alpha n, y n is equal to 0. Now, we have to add some ingredient that I did a match, which is the complimentary property over the constructs. That says that mu t must be equal to 0. And then alpha times the constraint there. The other person's name must be equal to 0. Why is this true? Well, we want to minimize the prime of 0, right? So if we go to the duo, to the Lagrangian, if we minimize this, and sorry, this product and this product B are 0, then what we are minimizing is the primer, right? So we must force that. We must force they complimentary products. Sorry about that economic fabric. I spent the last weekend with a family with three kids and they were sick and they are with me. We share for it clearly and spelling. So so these are the conditions that we found. The first one, W proportion of the data. Here is something that we found from computing the derivative with respect to t. This is from respect to B, and these two are the complimentary conditions. And also we need to bear in mind that alpha new machine, they are positive or 0. Right? This is what we call the Kuhn Tucker conditions and is still 56 are complimentary Tucker conditions. Right? And that comes from the theorem. Is, this is part of functional analysis that we're not just saying, we just need this mandibular I'm sorry, that this report today. So this is what we're going to use now to find the final solution that we want. So from 24, which are these ones? We say that it is positive. That means that the sample is inside the margin, for outside the margin, but might misclassify. In this case. In this case, alpha n should be equal to 0. Why is that? Since T-n is not 0, then mu must be 0. In this case, C minus Alpha equal to 0. So for those samples, for which for the samples that are inside the margin, write inside or outside but misclassify, then alpha will be exactly equal to c. We say that the Lagrange multiplier is saturated. Saturated to see. Alright? And so with number five here, we know that if the sample is any margin, then. Alpha will be between 0 and see why? Because in this case, let's go to five. If somebody's on the, on the margin, then this quantity here will be equal to one. All right? And then will be 0. If the sample is on the margin, then ci is equal, equal to 0. Then mu doesn't need to be setup. And in this case, see, sorry, alpha is less than c, but we know that alpha should be positive or 0. So in this case, alpha is between 0 and c, right? And so if somebody is properly classified, then team must be 0 because it's, it's not positive. Alright? And in this case, Alpha must be equal to 0, right? So if the sample is probably classified as quantity, is 0 as well, right? But this quantity is not one. So this one, sorry, this quantity is not 0. If the sample is properly specified outside the budget, this quantity must be 0. Alright? But this quantity is not 0. So alpha must be 0. So let me summarize this and a whiteboard from the girls from the conditions. We have that classification hybrid plane. And we have samples, product declassified until the marginal probability classified for not inside the margin or outside of London. For example, something like that. Let's put our sample here. So let's put a sample here. So if the sample is probably classify and outside the margin, Alpha must be 0. If the sample is inside the margin, then alpha is equal to C. A the sum bulb is outside the 12, but it's not, it's misclassified. Alpha must be C, right? So for all the samples, Alpha is equal to c. With assemble, these are the margins on its margin. So for example, this is why this is blood. Here we have Alpha is between 0 and c. And this is the basic property of them on Lagrange multipliers. Now, remember that w is a linear combination of the data. Exactly. This linear combination, this one. So alpha for a super vector machine is a linear combination of the data inside or on the margin. And it doesn't use the data which is probably classify. So a support vector machine is constructed of support vectors. And the support vectors by those samples. For weights, Alpha is not 0, so they are on the margin or inside. There will always be samples. On the right. We will have list the plus one samples on the right. So this is the expression, but super vector machine, these are the properties of this barrage motorbikes. Let me finish out time. So let me finish with this expression here. Well, it's probably not easy to see tablets. So why is, why is w transpose x plus b? But W is equal to the sum of alpha y, x. So here we have w. This is w, right? We have two Phi. We still need to find alpha. We just know how they behave, but know how to find them by this is w. So we change W by this expression. What do we, what do we find here? What is the most important thing when we have here? Well, yes, w is a linear combination of the data. What operator that we see here? The only operator that we're gonna use. A lot relevant. It's other prompt, right? So my classifier is a linear combination of dot products between the training and the test data. And this is a dual expression of a classifier is a lot more. I can be one month explaining this time, but let's just go through the important details. We will not run whatever it is, right? And this is the way in which I'm going to express all my machines in this class. That's it for today. 00 00 00 00, 00 00