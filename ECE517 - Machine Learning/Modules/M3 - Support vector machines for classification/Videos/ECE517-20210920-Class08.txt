 As part of the Tibetans, maybe that's part of our body uses estimates. You already share with graphics. I'm sorry. Yeah, that's kind of the structure and the audio. Can you hear the classroom? Okay. You can hear now. Yeah. Very good. Thank you. Control that up a little bit. That makes a lot of them, so I think we're good. Any questions? Okay. Thank you. Alright, so let's start. So this share screen. So well, we were talking about so, so we were, when we left it. The last session was in many ways we develop all these conditions like oxygen conditions in order to, in order to develop the final support vector machine. Right? So here, in order for you to remember what we did, we basically 1 fourth. So basically in order to record this, stop share. Anyways, so what we did is first write the primal function, which is something, which is function of W, b and cheap T. W is the set of a set of parameters or my classifier b is the bias. Ti are the slack variables or losses. And then this cost function is equal to 1.5 of the norm of w squared plus c times the sum of slack variables. Where I defined is like variables. These wave, why I times w transpose x plus b should be higher or equal than 1 minus t and t I should be positive the porosity, but these are the conditions. And the way it works will remind. We'll go through it again if you don't remember, but that's, that's the primal constraints questions. After the plus sign first, like what is that? After the plus here? Yeah, that's a C. Yeah, C. And it's, it's a free variable. We call it a free parameter or a hyperbola. The hyperbola is something that we cannot train based on these criteria. So for us is now it's a constant and we have to, I have to find an adequate value later. And so the way it works, this is if the sample is properly classified, then these two, they have the same sign, so their product is positive. It is somebody's properly classify. I'm outside and inside, sorry, inside the margin. Right? So we have declassification hyperplane. And this is their margin. Classification. Hyperbola is w transpose x plus b equal to 0. And here we have w x plus b equal to one, for example, minus one. And here w transpose x plus b equal to plus one, right? So if there's someone is properly classified and inside the margin, then this quantity is higher than 0 is positive, so this is less than one if the sample is misclassify, right? So this is the part of the space where this white samples. And then here we'll have negative samples. So in this sample is misclassified, then they're labeled as one side. And the Upworthy has a different sign. So this product is negative. That means that she is higher, lower. And that will be where we bought cheaply. So when somebody is properly classified and outside the margin, we force our team to be 0. Right? So we drop all the values of t for the samples that are properly classified and outside and watched. So then what we do is to multiply this times, times a coefficient, this and this. And we put them together with the primal. And we got as now and optimization, which is not one strain anymore. But that has additional variables that are what we call the Lagrange multipliers. First, what we do is to rewrite this constraint like that. And with less, we multiply 9 times Lagrange multipliers, which must be positive horizontal. And we did that, sorry, We did that. In order to compute the gradient of L B or the resulting classic functional with respect to the primal variables. And so the gradient of the primal plus or minus the gradient of the sum of this quantities. It should be 0. That means that both gradients, they are inline, they are linearly dependent. And this is the point in which we have the minimum possible value of being constrained to this. And for this limit to be true basis, we must force that either, that's wrong, that either alpha i, 01 of one of a has to be 0, right? So we force this same here by either view or key, is if we force this to be equal to 0 and themselves. When we compute this is gradient and we, and we equal to 0, we have all these results are these four results plus this. So these results here, right? This is the day buddy or the gray with respect to B. The w's are a derivative with respect to achieve derivative with respect to B. And then these are the two conditions. And then the additional constraints are alpha and mu n and g n was, they have to be non-negative. Now, when widow is to put all this, he used to use all these expressions in the Lagrangian, which is this plus the sum of this. And this is they developed. While first. Prior to that, we extract some properties. The first property is they constrain one that says in that folder that says that w is a linear combination of the data. Again, I insist the model is, this, is, this is an example of what we call the representer theorem that says that under restrictive conditions this happens day. Their solution is a linear combination of the date. This is, this is very important. It, it will become of paramount importance for all the machine learning that we are looking at, that we're learning here by its very name. If you want, It's very reasonable. So W lives in the same space as the data. So the data span the space. Right? If we have a space of d dimensions, the data has d dimensions. This data spanner space of the dimensions, and M and W lives in the same space. So this is true. Or is it? Well, when we have where we are in a situation in which we have infinite dimensions and only and data, it turns out that this might not be. And that is the tricky part here. If I pass my data into a space of infinite dimensions, then w might be anywhere. Well, they generalize representer theorem says that this is true even in infinite dimensions. The solution that I have is a linear combination of the data. So the solution lives in the same subspace spanned by the data. And that property is very important because it solves a lot of problems that we would have if this weren't true. So then using the other 223, we can conclude from this that if, if a sample is inside the margin than Alpha is equal to seat at this parameter c. And if the sample is on the margin, for example, here, here I am. Today I mentioned, we will have situations like that. Then for these three samples, Alpha is between 0. And see. Here we have an examination and we have to solve one way or another. You're seeing an approximation that makes a lot of sense from many points of it. So in this case, C is non-saturated. So also we will see a relationship between, between C and the error, right? Between C and a cost function that we're using. So when at this point, I want you to have this property clear. For the samples, Alpha is 0 for the samples, or if I see for these samples here or here, alpha is between 0. This is what I want you to remember now. And then for the solution, that is, that is very simple thing. Years or so, we have an estimator that we know very well, whose expression is y equal to w transpose x plus b plus an error. And then we have a solution where w is equal to the sum of alpha i, y i, i. So alpha is the Lagrange multiplier corresponding to the constrain that involves x i and y i is the label, corresponding label. So we can write this as x. Again, let me thing as y and x. Where y is a diagonal containing all the labels from Y1 to Y2, capital N. This is from one to n. Here we have zeros. S is, as usual, from X1 to Xn is, are these vectors. And alpha is a vector that contains from alpha to alpha n in a column, right? So this is a column vector. Now, if I combine both, YI is equal to w transpose, which is the sum of Alpha I vi, XI transpose, because this is transpose times j here, j, x, j must be, must be late. And so my My estimator, my function, is now expressed as a linear combination of dot products between beta and the space of linear functions on the data is what we call a linear space, dual space, a space of linear functions on the data. And in particular, if we do, if we compute the projection of my sample to all the training samples, then I obtain a different expression for x j, which is the 0 subspace because it's a linear function of it. So here we are looking at your space already, but we will see it graphically with more detail. So this is what we call a dual representation of the data. This is a representation of the data, right? And this is a dual representation of my, of my function, my estimator, where alpha i, they are called the dual parameters. Okay? And so we can observe here, we can, we can interpret this dot product from two different points of view. But now let's just, let's just in this expression that can be also written as alpha I times Alpha transpose x transpose y times x j, right? And so what they do here is use, I'm using this, this four and this expression. I transpose it. So Alpha transpose x transpose y transpose butts in seats, are they? I don't know if the transpose is equal to the matrix. So B was the, right. So this is another way of expressing my, expressing, my, expressing my estimator. And so here we can see the doer, the primal and dual dot product. Here we see one dot product between this vector, which is d times one. And here we have 1 times n, n times the events. Let me see. What did I do here? Times 1. That's n times the time. So they should be There's something wrong here. Yeah, I'm doing something wrong. So w is, let's say here, I have n samples. So this is n times n, and this is equal is n times d. And this is n times 1. So this is wrong. You can say himself. In order to express this. Let me see. Okay. This should be x times x is the rows and columns. And this is y, n times n. This is n times 1. So this is the right expression, right? So we'll have this x. And here, this is exactly what they want, right? So here I can see the dot product between the test sample FJ and this is a test sample or any sample I can now, between the sample and all the training samples. This is our product between this sum, all the other samples that they have inside. But I can also see this as the dot product between this sample here and this factor here that has the dimensions to. Or if you want. I can express this as the following. First here, we have a linear transformation of sample X that has the times 10 dimensions with this matrix that has n times the dimensions. So when we do this product, when we have projection of the sample that produces a vector that has n times 1 dimensions. So this is a representation of x j. Assume now that X a j is a test sample, so it's not inside. And here what we have is we compute a transformation of my data chain into a vector that has n dimensions. And the vector is on the elements of the transformation or the training data. All right, so a pass from a sample that has the dimensions into assemble that has endometriosis. And it can be proven, It's easy to prove that all the properties of the sample, they are preserved here. In this particular case. And this is the dual aspects of your space has or that your subspace it has. And I mentioned where n is the number of training data. Why is this important? Well, here it doesn't seem to be too important, but in general, x will be transformed into a space of infinite dimensions. Not good. But then if I construct a dual representation using dot products book, I have n dimensions and these are finite quantity any computer and the oviduct and the properties of samples that are preserved. So this is this, why do I have here is a dot product. Many, the, it has infinite dimensions. I don't care because I end up with NO. And then where they have here is a vector of one times the n, sorry. And I've magnetics and bags. And so you have a vector of wine text and the images. So here I have another dot product. So if they are linear coefficients of my, of my machine in the prime of space, the original space R, W in the dual space. The coefficients on my machine, our alpha, right? And so this is alpha transpose and there is my colleagues, k, j, so plus B. Yes, So thanks Angelo space work. Just as in the primal space. Right? I have a linear classifier, individual space for, right? In this always happens. And I said this is an oxidation that we sell. Others. I have half a transpose since it's a, it's a scalar. Well, it's not, it's not a scalar. X. X is a vector that lives in D dimensions. If x is my vector, I always forget who the tax is on vectors of, vectors of the features. And w is a vector of b coefficients. All right? It's another scalar. Now, it's clear, if not, we insist on x. X is my observation and it has the features, whatever layer, the numbers. So I have to transpose, I have to use a transpose. Now. Let me, let me share my screen again. Another question on that last formula you put up there. Yes. Here. Yes. Yeah. So it looks like you had you combined Alpha transpose and the y matrix and take went down to Alpha transpose again. And the last one, yes, you are right. I need to I need to use this. You're right. So the difference between Alpha transpose Alpha plus y is the sine alpha transpose or Alphonse Is. Thank you for pointing out there is because I forgot about it. So this has exactly where they have. The difference between alpha and alpha times y is that as the sine alpha is always positive. But then I will decline every, every alpha by a plus one or minus one, depending on the sign of the label. Right? So this is still a vector that contains alpha, but with the sign positive or negative 11. Yes, you're right. That's the correct expression for the dual representation. And so on. So then there's the Maidan can be written like that. That's exactly how I represented or after I corrected the Arabs. Now, this is the most Cumbersome and boring by Johann here, right? So we have several. What they do is to break the, the functional, the Lagrangian functionality in several pieces. Here we have just the part where I have the, the norm of the parameters. So w squared is equal to w transpose times w, Right? So what they do is to combine w transpose w and a half. There's result here. This result, we can express it in a compact way and we will have a new matrix that we need to define. Then for B, B, they are the constraints. And so what they do is to put himself w transpose b expression as a function of the data. And then we have again, we have again this dot-product and as you can see, W disappears. And so if we keep developing this, we obtain this expression that condensed this double summation with all those flows between data within some holes we sample is multiplied times its alpha on its label. And well, then we have the rest of the elements. And then C and D, They are quantities that are part of the, of the Lagrange are functional. So they started to turn amber. See here, this current can be removed because eat a meal or a chain, they are 0 by the hydrogen. Complimentary colors from the conditions which are number 5 and 6, 56. And then then this terms, the terms a, b, and d, they can be added together. And so we finally, because this term, we have it twice. And then we have this. And this is basically the solution of the dual solution of my functional. But then this term here, this term here is nil because the sum of alpha and y n is equal to 0. So we have this and then we have to, we need to remove this till the following way. So, so we can say that this quantity, this, they are 0. So if alpha is between 0 and save, then Ci is 0 because somebody's inside of AWS. And otherwise, alpha is equal to c. So there's two quantities. They become equal and they can be remote. And then with this, we only have this expression here. These are the terms that survive. And so this term here is the sum of alpha n. So I put alpha vector, Alpha unit vector, and I multiply it times one. I do the dot product of Alpha times 1 is the same as putting I want you to write algorithms for stage 1 is equal to the sum of, of, of. For this double summation here. What we have, this, we can change it by this product. Here we have x transpose times x, which is the set of data on a set of labor transposed times beta line. And alpha is the vector with all they are, they do other efficients. Undergone science, becomes dead. So all elements on our farm, they are positive or 0. So my final dual function of is on this one. And we have to optimize this. So we have to find the alpha, that alpha that maximize this dual function. And we have to maximize it because this, this and this as a single solution. The good thing of this expression is that there is a single solution of alpha and it always exists. Although we think of this, well, roughly speaking, we can think. Of this as a generalization of second order polynomial. Second-order polynomial is alpha squared a times alpha squared plus b alpha plus c. It's a polynomial second-order bioassays, or perhaps it's a parabolic function that has a single maximum. This is a generalization of many dimensions. Right? But we can think of this from a more rigorous point of view. These matrix, It's a matrix that has n, n in rows and columns is a, is a, is a mated with dimensions n times and it's constructed with and different vectors. They are different, right? So we assume that we are not repeating data here. So since they are different, then this matrix is full rank and it's symmetric. It's a symmetric matrix. With this, we have enough to, to say that this matrix is positive definite. In order to, in order to stay date, we need to prove they put on Fisher's theorem, says so, right? But in my venture saying that this is true in this major because for run and it's a matrix is definite, is positive definite. What does this mean? It means that when I do this product, alpha times a matrix times alpha. No matter what is the value, whether the videos of powerful that I put here, that product is always positive. If I have repeat vectors that is not full rank, then what happens is that the matrix will be the product. This product will be positive or 0. And at any rate, it's always non-negative. Non-negative. So meaning that this quantity is always negative. And this one you think is right. So this is a quadratic function. So this is something that depends on the combination of alpha and it's always negative. And this is linear. Which means that there's always, there's always one solution, one maximum, and there is only one. Right? And the key here is the thing that this matrix is positive definite. So this quantity is negative definite. Or do you want, right? So this quantity is always negative. Dissolve the salt is linear, so this has a single solution. The solution exists. Is this good? If we are dealing with non-linear approach has not based on kernels. What happens is that the landscape of my cost function might have several maxima or seminal minimum. And when I try to find a solution using gradient descent or whatever, I might find a minimum which is not the optimal one. It's not the absolute minimum. And I get stopped there. There are techniques to read, initialize my algorithms, right? To find different minima or maxima. And if I'm lucky, one of those will be the minimum. The absolute minimum. By that has increased computational cost and risk of not finding the absolutely. So I have to live with what they have. In this case. This is not true. I have one single extreme and it exists. And I don't really need to use gradient descent here. Again, just play around. So different strains with this unary constraints. We used a technique which is called quadratic programming. It's part of their theory and it's find of functional analysis, right? I'm not going to go deep inside here because it will take a lot more. But at any rate, in order to optimize this function, we use a special case of quality programming, which is called signal minimal optimization. And it's an optimization that takes one value of alpha at a time. Alright? There is a paper by I think the guy is called hacks. But if you type online, single mammal optimization, and then maybe you SVM, you will find it. You'll find that if you're interested in how we optimize this and this has been program in the program and optimize. So this procedure is actually verifies this much faster than using the standard quadratic programming that you might find, find in Matlab or Python. Python. You can find functions for quadratic programming, non-linear programming, the general linear programming. And you put your, your data and you have a solution. But there is a special case for super vector machines, which is called single minimal optimization, which is faster. It's been programmed in Python or in C. For MATLAB, for example, you are spoiling. See, I don't know, in Julia probably and in an optimized way. So it's very fast, is the fastest possible way of optimizing a support vector machine. And when you try, you will see that it optimizes. In no time. I still, it might be, it might be tricky in some cases. So this constraint is not going to read here, but we know what is the other side of this? This is the minimum value of alpha and the maximum will be C. All right, so I'll face constant between xenon. Let's see. They package that I recommend you to use in in, in MATLAB is, is called SVM. And it contains optimizes for all the versions of super vector machines that we will see in this class. This is the first one. But if you work in Python, that is their equivalent packages. So if you use the main thing you can use, you can use storage. Storage has a library for machine learning and it contains the functions to optimize a super vector machine. It's straightforward to use that. Or you can use Cyberduck. Secular and it's the easiest. Right? But again, there's no free lunch, right? If you want easy, the performance is not as good as yours towards the performance is better, right? Faster. Torch is optimized for parallel processing. It is. This is not a gametocytes past have anything. I don't have any idea of POSIX. I'm not going to make any further comments about this. This is what I use in my research. This is what people use in their research nowadays. So let's convert this product x transpose x. Previously, I introduce matrix X star puffs, which is a sample of it's proportional to the sample estimation of the auto-correlation matrix, right? Or I could take the mean and covariance matrix. This is a matrix that has similar properties. Milan Scholar Gram matrix. It's a matrix of dot products between data. If you do this operation, X transpose X, assuming that x is formed by column vectors of data. And what your mouth is a matrix that has n times n dimensions. Every entry is the dot product between beta x and beta theta I theta j. So the entry I j of this matrix K is this dot product. This is a Gram matrix of the problems. In chapter three or four around one of the chapters, we will prove that this matrix X transpose X and the auto-correlation matrix XX transpose they have, they contain the same information. Right? So we can use one or another. Here we use this one. But we're not yet condensed exoticism information. Our B matrix of the other operational metrics. And this is another operational made, sorry, this is about product. Are we using data in the primal? Inspects your prime on space, time or space is the original space. Or if I take the data from it into our infinite-dimensional, it's still the environment space. As other diol is, we can usually like that. Where k is this matrix and downdrafts. So this matrix is positive definite because we are using different sample, different data inside. Say we have and different samples, then pay, it has rank m, forefront. And then we multiply, we print one divided by this by this two matrices. The situation doesn't change, right? Because we still have different samples. So doing this is equivalent to multiply each one of the samples types. It's label. So we change all the signs and whether the operation so that they symbols will still be different. And this is full rank. And by the current efficiency, this is possibly write this quantity is always positive. No other one is a video of. And that is what ensures uniqueness and existence of the solution. Because what we have is what we call a quadratic form. So, so this quantity here is a hybrid paraboloid. Function of alpha is a hybrid paraboloid. That's it. Right there. This is a linear function. So what we have is a hyperbola it by plus a hyperplane. This has a single, a single maximum, right? So we have, yes, a generalization of some hyperboloid plus a linear function. We've put between these two together. Since this is linear, this is quadratic. This the sum of those, it's, it decreases, right? So, and here of course we have the same. So when we have S, So what we have is something that has a single maximum. So the solution exists and is unique. It's very important for us. Now. What does that cost function that we use? This a success or a right? So this is just to understand how things actually work. Our mind, my cost function is this one. If k is negative, that means that assemble is our side. Are they in the margin and the days and the sample is probably classified and she will be negative. So what did you just drop it to 0? We don't compound for it. Right? But in principle, if we don't have this constraint, G will be negative. We just don't care about function for samples from which TEA. And it is negative because they are probably best by outside. So we draw them for samples for which t is positive. And then what we do is to add of all this, it's like videos together multiplied times c parameter c. So this is my cost function. Right? I have gn here. And then this is my function, g quantity by a times c. This slope is C in the positive side with 0 here. So yes, yes. Question in chat, there's a question of edges. Or they cannot see the word knight. Basically, now I am using, I am using the screen. If you can see the screen. I think that the old one. Okay. So let me finish this and then we went to the whiteboard and I'll repeat whatever I said. All right. It's, it's it's site for versa wealthy were out. At least next time. Raise your voice because I don't see your comments arrive. I have a hard time to the time that somebody posted a comment. Right? So just raise your voice. Doesn't matter. Okay? So this is my first function. And it's this slope is C and here is a, right. And, but whatever the Lagrange multipliers, they're eligible the price. I see. If my aim. Cheese, pass it then. And 0 if t is negative. So the derivative of this would be the derivative of this. This is 0, this is C. And it is a years. And this a is the area for which the error is exactly 0. Because the samples are on the margin. When the samples are on the margin, she is 0, right? But then we are between these two limits somewhere. This is clearly, I mean, the termination. This is an inner determination. So and the superego machines, such an act of Betsy, they are linear or quadratic programming. They cannot be, cannot solve this. Well, this is a loss function and this is the interrelation. Values are the values of Alpha AR, the derivative of the function at the optimal point. When we arrive to the optimal point, then this thing is true. Alpha is the derivative of the loss function. And so going back to there, see that right away. So that was the that was the thing that probably you missed. They align section. And is that my my functional? My functional is 1.5 minus 1.5 of alpha transpose y k y Alpha plus Alpha transpose 1. So this quantity here is a hyperboloid of Alpha plus a hybrid paraboloid of alpha. So it has a single maximum, right? And then this is a linear function of alpha. This is our hyperplane. So when we are in this side, things are mega that when we're in this side, things are also negative because it seems this is quadratic and this is linear when we add them together, their stance to a negative to minus infinity. So the sum of these hybrid paraboloid and this hyperplane, it produces a solution with a single minimum. Right? That's it. And this is good for us. Single minimum or maximum, sorry, single maximum. And actually the thing of like that, because this quantity is always negative or 0, right? So that's it. I don't know if there's anyone at any further explanations. Then this case, let's keep talking about staying up on pleasure to guide the gradient with respect to what variable we pick an angle T. Let me see, let me just go to the screen again or IPA. So the gradient with respect to t In order to find the maximum. No, no, no, she is not here. She is with respect to Alpha. With respect to offer yourself all the primal variables and disappear. And then we have, we have alpha. And we know that w and alpha, they are related. W is equal to a linear combination of the data through alpha. So alpha, alpha fw, they have exactly the same me. Alright? Same meaning. And so minimizing with respect to w is equivalent to maximize with respect to Alpha. So while we have this thing here that cannot be solved when we do, whether we do. So. How comes from? So great, I cannot tell you what the way we do. All right. Let me let me do it in the white board because I missed I missed slides here. Anyway. So oh, by the way, this is fully explain in one, I think that's the only paper that I coauthor in and support vector machines or by share with you a di, as in. This paper is in the supplementary materials and the thing is fully developed there by, so there is an explanation of what we do in the following terms. So. If we have this cost function like that, then I need the dimension of course. And they probably cannot be solved numerically. We have a numerical problem. So what do we do with machines do in general? From blackberries? Well, this is 0. We want something here which is differentiable. So what we do is to add quadratic cost here. So we have something that goes like that. And then by keeping that anybody equal to 0 here. And then here we put the linear. So here the derivative is c. So we further quadratic section. A function of to be a quadratic function of the error here. So the derivative here is 0, and we keep going until the derivative here is C. And from here we put a linear dependence. So here we'll put this is 0 and this is a given number. So this is that's gamma, gamma c. So if we put a quadratic invertible between 0 and Gamma scene, then we can fit a function, periodic function that has these two properties. Right here. The derivative is 0 Saturday around the origin here that even if this is a C, right? And so then instead of using this super vector machine, we use desmos, then what happens? What happens is that instead of using that primer functional, we need to flip this a lot more. And in order to include in that bright non-functional this section here. And then we do the same. We computed the gradient with respect to the primal else we find collision conditions, we put everything together. We will repeat the process. But with a cost function which is a lot more cumbersome. To obtain the following solution. The solution is minus 1.5 of alpha transpose y k plus gamma1 i y Alpha plus Alpha transpose. Now the solution, the solution is exactly the same except this term that we have here. Alright? And so what does this mean? Well, k, k has a problem when the samples are right in the, in the numerical problem when samples are right in the middle of the day, what right on the margin, right? That we have this omega problem. We're having determinations. And then if we add this quantity here, we solve this numerical problem by basically increasing the eigenvalues of K by a value of Gamma. So then my derivative, the derivative is 0. If the error is negative, then linear. When the air, when the samples, they are an arrow which is between 0 and Gamma c and then constant. And then we have something that has now ended ammunitions. This quantity gamma, gamma c and write this quantity gamma can be very small. Today's more quantity. So basically here we're saying that, well, I don't know if I'm going to be able to feed, to put mice some balls on the margin by what defines relax a little bit the margin. So instead of having a line, then I have a band with y equals to Gamma, very small. And everything that falls inside. Then I applied to this value of alpha, this is off. That's it. Seems alpha, gamma is very small. My solution, this solution and the one that they present that in, in the screen, they will be very similar. This is a perturbation to the solution. So the solution here is not going to be the optimal. I think what happens is that the optimal, I'm not going to be able to find it. So I put a little quantity, gamma here. And then the protein can be solved in an approximate way. Right? So I alpha will be here, like here or here. So these are values that are clearly inside the margin. These are vectors that are clearly outside the margin. And these are vectors that are very close to the much how gloves well, quantity. And this is a function which is how it's 0.5. And then it has a small quadratic, and then it has a linear quantity. Hey, this is not new. This is classic, and it corresponds to it and subversion of them. Who were cos function? This is bind off classic robust statistics. A hearing loss function says that if I don't know any, any any any model for my data, the best that they can do is drop my boss function, which is not what I did, but it's not the unto given extent and then it's linear. And this works well when I, if I can, if I can use this instead of the minimum mean square error. This is something which is going to be robust against outliers. If I use a quadratic function. The coefficients that I multiply my data with in the solution is exactly the derivative of the cost function, right? And then so what are the function of the samples? They are multiplied times this quantity here, the derivative of the quadratic in my sample. If I have sample which is really far away, then somebody will have HIV. It will have a high influence on resolution. But this is going to be totally useless because the sample is far away. Where they want to do is to limit their contribution will be sample. If I compute the derivative of this function when they have is this. So the Heuer cost function is our cost function that limits the contribution of each sample to the solution by a given limit. And this is similar to with whether we support vector to chimps in the sample is very close to the, to the margin. We apply it, whether the function, but if it's far away from you, not from the margin, then we apply a linear, a linear coefficient or linear cost function. So we have a coefficient which is constant, so it limits the contribution of each sample. And that limits the possible bias due to samples that are outlined. Right? So in this aspect, the super vector machine is robust against our layers and it's similar to what they do when they do robust statistics to solve regression problems. And so that's it. So this is what we're really do. We apply discuss function by, in an implicit way because we won't write to the dual. We apply this simple identity matrix, multiply it times our very small quantity. And this is exactly equivalent to have a linear or quadratic section here. We will also see that if the data in a error is Gaussian, then measure that we can do it. We have injury data are many data the best that we can do in the optimum is a quadratic function right away. Right? But what happens is that when we don't have too many samples, then our layers I and III shop. So the best in general we can do is the quadratic cost function for samples that are close to the, to the margin and a linear one for others. So this is a good trade-off between what is the optimal when we have many data is Gaussian and many other situations that we don't know. There is a field and my fewer, it might expect the whole thing. Let me share the screen again. So now let's just go and see examples. Alright, next class or pragma computer, and we see these things in action. So here, we have here a linearly separable problem in two dimensions. And separate. Our problem is solved by the machine this way. So here is the separating hyperplane and I certainly my camera again, sir. Second, sorry. But would you mind changing the camera again so we can see which one? Sorry, the home screen. Okay? So here we have a linearly separable problem. And then since it's lithiums have our old machine finds the margin with no samples of time. And we really have free support vectors. We only need three because it's made in two dimensions. So it's fully characterized with three points. Three points, which are in this case the history. And in this case we have three threes coefficients, 3 duo, duo multipliers that are known, they are less than C. Right? And so here we have another problem where the data is not separable. So we have black and red points that I made. And this is science possible solution by a super vector machine in which we chose, we choose c equal to one arbitrarily. This point, it just arbitrary. We see 100. And then the machine tells us the 7000. And that we have 56, six silver papers. These two, unless they are on the market, there's always drainage. In this case, two dimensions. And then we have these three that are either inside or outside the margin might misclassify. In this case, these three, they have values of alpha that are between 0 and Z. And these three, they have saturated barriers of alpha, as we will see here, is two points correspond to the values of alpha of these samples here, they are negative and this alpha multiplied times the labeled. So we have minus one and 100, and then we have 1. There's one which is misclassified outside the market. So they, they alpha is saturated to, in this case 2 plus 1, 100 because the sample is positive. And then we have three samples, these two ideas that are non-saturated. Right? And so here we have this line minus 10100, but actually, actually this line here, not a vertical line around the 0. See now it has a little slope. Well, now the Little's Law as log, which is less than infinity, as lobe here, is exactly equal to one over Gamma. So if I put a gamma which is very small, this slope is very high. So the solution is pretty similar to the optimal one. But here we have values that are quantified. It can be fun. And we do that using quadratic programming. So let me see. The summary. Is, is the following. The super vector machine is a linear machine. Does many neighbors. Neighbors, or say the contrary, support vector machine is a nonlinear ship wrong. This is what the machine is always in all cases, a linear motion here. And change my mind. I'll write this here. A wrong. This B doesn't exist. This is probably, I don't know why there's a b here is probably bad coding of the back. And then we have these constraints. And there it is, is equivalent to minimize the damage or wanting is diamond, right? So this minimizes the banks are like as I mentioned, plus a term that minimizes the empirical risk function. Where these constraints that we find it to margins and the primer after a lot of algebraic operations leads to this deal. And these deal, can we optimize using whether def programming, my economical factor is expressed as a linear combination of thing. So all this summary as intended for you to write and to read it, try to explain each one of the points, right? You have to try and explain why these things are true. And so we know that we have several properties or they are the Lagrange multipliers, right? That the developers violence and imagine I have new videos. Otherwise they are either C or between 0 and C. And the machine is linear. Why? It's easy to solve nonlinear versions, as we will see next in the next class its name. But either C is freed and we have few. Optimize it one way or another. We have to find a reasonable value of c using cross-validation. And that's, and that's it. We still need to talk about b. The bias might say I just missed it, I'll talk about that next class. All right. All right. Please take a look to the confessions or to this summary, and please try to understand each one of the points and our deck or any questions that you might. But we are finished for that. Thank you very much.