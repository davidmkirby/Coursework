The first thing we do is so nice. It suppose that you make all the questions related to innovation, related to that theory. Because part of the difficulty or this part, well, this class is to understand the innovation when we scale up, when we have our vector, when we have a matrix. This stuff, all right? And then I will show you a couple of examples that include particular one solution for one of the homework. If we find navel to plug this share screen, I don't know. Probably I'll be able to it. So again, from the beginning, what we have let me see. Yeah. So I separate the machine. All right, We have seen super vector machines for classification. That's a very particular type of support vector machines we will see now we're after. We will see them for regression. And also type of support vector machines that are intended to detect anomalies, outliers, right? And this is an important part of machine learning to detect things that have a low likelihood. For example, when we are detecting faults in our narrative are great for Infochimps network, something like that. Alright? Or I publish a paper in which I apply that to faults in in the heating and cooling systems. And we applied it to Mechanical Engineering Building of fear, the UNM. All right? So this is important. So right now we're just talking about classification, right? And so first, again, notation. We always have three types of elements here, which are scalars. So things that maybe are vectors that live in spaces are p-dimensional. We have matrices. Right? So how do we organize this scalar 0 for Campbell of Jake, it's going to be a scalar. What is whether vectors x and w? X is my set of observations. And they are column vectors. So x is on the rise like that. Right? It's array, it's an array of scalars and organize them in columns. And same with w. W one to w be in when we talk about Gaussian processes. And in our applications we add a constant to X and thereby arms to W. So this will live in a space of d plus one dimensions. So matrices, for example, we have x, which is organized. An array of column vectors. So this, let's call this x j. And this. Then we will have here we will have two indexes. So here we have the rows and n columns. The rows and columns. So this is why for the innovation d times n. Now we have a fundament elevation, which is the dot product, dot product between X. I'm WO, this is a general mathematical foundation, right? You know, it is something that I always write like that. Inner product is symmetric. So I can also write this. Sometimes people put a point here. Doesn't matter, is a product of two vectors. This will be 0. Also true for our spaces of infinite-dimensional, because there's spaces they are, they are complete and they have a matric, a half ordinates. And if a guardians, but they have coordinate. So this can always be expressed as. So. For example, from y equal to one to d, w times x I, where here, these are two scalars and they are the components. Each one of these vectors. The dot product, It's an elegant ways element-wise, a sum of elementwise multiplications. That's a dot product. So I want you to understand properly, perfectly what I put in a whiteboard. Because it's fundamental to understand, right? For some of you it's going to be trivial for some of you, it's going to be nontrivial depending on how much algebra you have studied so far. But that's it. That's all that we need. Now, we're talking about classification. So how do we define occupation? Well, we have a set of data. For example, this is data Is it belongs to one of two glasses. For example, here we have first plus one and here we have plus minus 1. These two labels being arbitrary, but it's convenient to label them like that for support vector machines. And then the idea of a classification is this, to place a hyper plane that separates, that lives at once. I wonder, as I said, the other glass. Some dance. This is possible for, for all samples. Sometimes it is not. If I can separate two classes with no errors. We say that this is linearly separable. We create our hyperplane. We say that this set is linearly separable. But if we have something like that, then basically this dataset is, is not linearly separable, right? So sometimes the classification, we'll, we'll have 0 errors sometimes did want. Now, how does this thing work? How does a classifier? First? We have to bear in mind that the classifier or this hyperplane is the set of vectors in the space that satisfy this equation. So all the points inside this plane, they satisfy this equation. The response, if this is a classifier, responsive, resume this exactly. What does it right? Now? What is x? X is any point in space. In particular here, we have all the points that have, that live inside this subspace. This, this is not a subspace. This is find an affine subspace because it doesn't contain the origin. And the general x is any point. In this case in two dimensions. What is w? Well, this line here, which is parallel to this one, is the line w transpose x equal to 0. And it contains the origin. So this is a space or a subspace. This is an affine subspace and they are parallel. Now, what is W here in this space, in this hyperplane? Is this is a vector. W is a vector that is normal to all the points in the plane. And you can assess that by taking any point x in the plane. And so if you do the dot product between x and w, you will say that the product is 0. That means that w is normal. Let's write this as. W is the characteristic vector on the plane or hyperplane. It's a vector which is normal to the hyperplane. In this case, a hybrid plan is aligned, right? But you could imagine at hyperplane, ah, hybrid space and then a vector which is normal to it. And they find that it is normally, it's, it's proven here. Right? Now probably the least two of these two vectors is 0. And the dot product. The dot product is International to the cosine. The cosine of the angle between these two vectors. And the angle is 90 degrees, then that product is 0 in this particular case. And this is important, It's important for us. Now. W is a vector which is normal to the plane, since these two planes are parallel, because they are the differences just a bias b. Since these two are parallel, then w is also perpendicular to this plane here. Now how the classifier works. If the point is in the same space, which is pointed out by the vector w, diversification will be positive. And otherwise it will be negative. Let's take a point inside the play anymore and it's got a 0 calling exit. And then let me see if this works. This works because I'm just making it up now. And then let's call this x1. And these two vectors, they are this one and this one. Right? And that is our segment that separates both vectors, which is Z. Let's call it z. So x one is equal to 0 plus z. Let's see what happens if I know the operation f affect one equal to w transpose x plus b equal to w transpose x plus b, right? So this will be equal to w transpose x plus b plus w transpose z. And since by definition of the S plane, all the vectors inside the plane, they provision response which is 0, then this is 0. So this is w transpose. Right? Now. What is the angle between x between C and a, U? The angle between z and w is higher than 90 degrees. This is, as I said, proportional to, if you want equal to the normal vectors times the cosine of 1 is proportional to the cosine of the angle between w and z. And it's higher than 90 degrees. So this is going to be less than 0. This is going to be negative. And this will happen for any point and design on the blame. And it will repeat the computation for a point. And this side of the plane, then the cosine will be less than 90 degrees. So the response will be positive. And this is a linear classification. Understood. Now, now I forgot what was the next thing? I have found an outline for my classes, but I forgot. Alright, so now let's go what the support vector machine. Support vector machine is rooted is based on statistical learning theory by N terminus. And a main principle concepts that we use the level of the theory. Yes, Debye major learning is the image. And we define it as the maximum number of points that a classifier is able to shatter in, in space. So if I have or VC dimension, this dimension, so I have a set of points. There is, this is right. So of course, this is not, This is not separable linearly, right? But then I have a classifier that make up that places a circle around. It's one of the pons and the cerebellum. It has any, any radius they want to, they want. I can always construct such a function. To classify this. What I do is to record all the points. Follow the whiteboard. For any white point, I compute the distance to the next point. I. And I use a radius which is half this distance. So I have this massive enough and this is y. So this is my go-to via nouns. What does my children, I guess I mentioned this was fire. Thank you. Bye. You have 30 seconds. Tell me. Any number. Will be able to see again, sorry. It's not suitable visit bind. These ones. Don't. They are nonlinear, separable or not. So I don't usually know classifier. I use, I specify that I made up. My mind as I mentioned, on the plane is linear classifiers. Do we know what is, what is a bunch of rank, as I mentioned, of this fire? Is it something? I said three also. But again, each other. Here we have 10 times. We shadow time points. What is running, as I mentioned, this classifier and is it a loveliness three-dimensional? I can classify ten bonds regardless of their label here against other points. So I'm Chairman, I guess I mentioned is higher than higher than three. It's at least 10. Bye. If I'm learning, as I mentioned, is equal to the number of points that are given. Classifier is able to shutter or to classify arbitrarily. How many points can I classify this way with my classifier? And then what is the magnitude one, I guess I mentioned of this. When you say ten, you get classified. But then I have another one. Okay. Now it's 11. So probably half of you are thinking out the right answer. And you don't dare to say 20. Again, if possible. Sorry, I couldn't hear your answers. I mean, if you are way more, why isn't it so it's not 12 is hired. Somebody has an infinity. Infinity is infinity. There's no limit to the number of glass of elements that I can classify here. No limit. I can put any number. But then it's infinity. Alright? And so gives me an idea that you don't have a mature idea of one dimension running a dimensionless yet. And this is why it's in its infinite. So, as I mentioned, is a general fact. Hi, and we will say classifiers that they have a mayor. There were fires that have this property. You give me any number of data. I can classify them with 0 errors, any bigger with any random labels if you want. Yes. I mentioned that I guess. Let's say, Well, here are the confusion comes because formerly I particularized devalued Javadoc because I mentioned to linear classifiers. All right, For they said, and it's written. If I mention running as I mentioned, is not a particular thing for linear classifiers. We just use device or mining, as I mentioned, of linear classifiers. Right? So if I cannot make any, any classifier, but I am restricted to linear ones. For linear classifiers, bed bugs. Well, what happens is that this is not possible to do if I have four points. So three points. These three points is crime. They span the space that I can classify them arbitrarily. I have five possible classifiers here, and they classify the data no matter what is the distribution of labels. So if I have two ones here and here and minus one, that is a perfect classifier. And so but if I put another point here, then there is no way I can shatter them. So for a particular distribution of labels, here, here, this is non-linearly separable, so I get Uncharted four points in two dimensions into dimensions like Amish other three points, three points that describe it, that span the space. So the magnitude is I mentioned of a linear classifier is at most d plus one is equal to, is equal to the dimension of the space plus one. If I have three dimensions, then I can do the same with four. With four points, say two points here. And I have these two points here, right? And then I can put a classifier that does the job. And I can do that. Again, classifier with no errors. Now otherwise and this divisions of the labels. But if I put a fifth that this is not true anymore. So in three-dimensions, again each other for points with a hybrid wave with a length. All right, In the dimensions I can shatter d plus 1 boys with a plane. If I have something else, I can't even shadow onto infinite points. But we are restricted to plagues. That is the difference of hybrid. If I have hyperplane in my space, I can shatter d plus one points where d is the dimension of the space. And this string of certainly points. We can do that with planes. We need infinite dimensions. So we will be able to map the data into spaces of infinite dimensions. If we have entering dimensions, then thereby micelle running, as I mentioned, with the infant with hyperplane embedded in that space. So at any rate, it's s is a scalar. H tells me. This is the expressive capability or capacity of my classifiers. If the number of dimensions is higher than the number of data, then I can shatter the moment. I can classify them, no matter what. No matter what are the labels, no matter what is the distribution of their lives, right? And this is not good. Right? This is not good. Because if I classify all my training points with 0 errors, probably when, when, when I change my dataset, that I will be classifying with an error rate which is less than the optimal. Okay? And I were the example in, into that mentioned, right? So if I have a distribution which is Gaussian around the eye, around two to, let's say around two means two centers. These two Gaussians, they are exactly equal. It's straightforward to prove that the optimal classification machine is a hyperplane. Two Gaussians then suddenly equal except for the mean. The optimal classifier is a hyperplane that is between the two means and normal today, segment. The segment that between both. Right? It is straightforward to prove it. You only need to plot the distribution of one of these above religious divisions of both datasets, then you will see that the intersection of both is align, right? But now, if I use a dataset which contains only three samples, for example, they sample the sample and the sample. My classifier will do something like that. This is the best possible classifier for the training data, but it's not going to work properly for the test. So when they need to do in general is to limit, to decrease as much as possible the complexity or the expressive capacity. Or in other words, I guess I match. In, in the examples. We were in spaces of 10 dimensions. So we can see that better because in two-dimensions, we don't see much. We see the main space, but we don't see the effect of the myometrium on an image. We will Sierra. So we need to limit divided into running these I match. And in order to prove that, mRNA came up with one of his theorems on generalization files. If we define risk as the probability of error of our machine. The risk of AMR him, that is defined by a set of parameters. Alpha, is bounded by the umbilical arteries, which is the risk that they can measure over a set of training data. I can measure because I have obtained the data and the labels plus a quantity that we call structural risk. Events on age and events on. So if age increases, the expressive capacity or the machining process, and then the empirical risk over the train data tends to 0. By this term increases. So this bond has a minimum. And this is what we want. We want the machine that produces the minimum found as a function of h. So we will have the best possible must classify linear classifier for our problem. This is the risk, depends on the complexity and also depends on the data. If the number of data increases, that this quantity tends to 0. Say we have many, many data, then this total risk is not report. Right? Here we'll say, you are asked, how much is many data? And the answer is, I don't know. It depends on the number of dimensions of the problem or the complexity of your data, and many other things. In general, many data is more like. So a number of data a lot higher than your dimension. The dimension of the space. So the data should be very high. So it should be much higher than B. For this, not to be, for this to be negligible. All right, so you have two dimensions and 100 data. Probably everything's going to be where the device, right? As we will see examples. But this is usually not the case because we will be working in an infinite-dimensional space. So the number of data is always, smile, always. Alright? So even if there's not always decreases with N, N is always going to be small. And in general, when we talk about infinite dimensional spaces, will be spanning a subspace of n dimensions. So what happens is that we can data. Our department chairman, I guess I mentioned, will become n plus one. Alright, so we want to add more data in order to decrease this isn't going to happen because h is going to chase us, right? So in general, we should not trust in a high number of data to decrease stockholders. They start to address is something that we can call overfitting. It's similar to overfitting. This is a probabilistic thing. Overfitting as deterministic, right? There is a difference, but the idea is the same. If our, if our machine is way too complex, then we will have overfitting. So there will be a difference between the error in training and Eric tests. They are, and that will always be higher. And the difference, difference is what we'll call the overfitting. In order to in order to limit the over fitting, we have to limit the ammeter like establishes western else. We can do that in many ways. We always, in machine learning, we always limit the complexity of them are shaped in a process which is called in general, regularization. Regularization is what we use to limit this complexity and in turn, the limit demography, right? And what we see in support vector machines is a kind is a type of regularization. In ridge regression. You used another kind of similar way of referencing the machine. So how do we do that? Well, there is another theorem that says that if we want to decrease age, then we have to decrease the norm of w squared. And I brought or your group in your homework, that W, the norm of w, is equal to the inverse of the bed when we call them the margin, right? So let's define the margin. We have a classifier. Still munching lines or hybrid plants. Here we have w transpose x plus b equal to 0. And these two lines, we define them back down. So this distance, d is the margin. And related that this is intended to classify where the minimum possible number of errors, two glasses is equal to 1 over w. And this is proportional to 1 rich. Well it's not proportional, but eigenstates proportional, white when we decrease w with increased age. So when we increase date, we decrease h. Fines. Are we increasing? We decrease h. And this is why these machines, they are called maximum margin my shapes. So what we use is the machine that has the maximum possible margin that properly classifies it if he has decided not to fix the error, the true with this little w to be, oh, sorry. This is the All right. This is right. So my machine, my supervisor machine is a machine that uses the maximum possible Monday, but let's define it more with more detail. If I have a classification problem, to find the margin, then I have data. Which is okay. I cannot talk. That happens. I was a singer in Spain. If you try high will find, you will find me singing. To design for a public, public Spanish TV. And, and there's my was, was perfect. I do not experience this. And then when you stop than your voice is not train anyone so much time on it. So this problem as a problem in which I can classify all the data. And there is a margin between, if there is a margin between the closest data of each class, right? And end up two-dimensions. Over the margin, you can always fit three points, right? Is three points. They define this subspace, which is a subspace equal to the nominal dimension of the space minus one. But then you have a bias. So you need three points to define this. And here is the margin. That's the right. And then what happens is that this is the perfect classifier for the training data. But this is probably not going to be perfect forecast. So I want to aggressive amateur running, as I mentioned. And in order to decrease the monitoring, as I mentioned. And I do that by increasing be. In most cases, the problem is not linearly separable. So I, my house situation like that, right? This problem is nonlinear separable anymore. I can still fit a linear classifier and I will have some errors like that. Maybe some beta will be inside the margin by probability classifier. So what happens in my margin is very, is very high. In the margin is very large. I will have a lot of samples itself. That means below manager running this image, a machine that doesn't do it very well during the drain. If my manager, Wang Yi, as I mentioned, is very high, then B decreases. And the machine during the training, we'll probably do it much better. But we don't want a very high dimensional, our very low, because in both cases, during the test, the performance is not meant to be good. We need to find the best, the best margin be. So what do we do? We want to control the ammeter running, as I mentioned. And at the same time, we want a machine to work. And how do we do it? Well, we have this r is less than or equal to the empirical risk. Plus there's two torus. This equation can now be implemented because this total risk, we can compute it, thinks it's running because I mentioned my machine is not something that we've built or control by now, that controlling W, I Control H. So instead of implementing this as every video, this is just a principle. In order to implement a criterion that is in line with this, what I do is to compute something that is monotonic. We departments are running, sorry. X plus something that is monotonic with pitch. Something that both, along with the bigger risk is the sum of TI. Ti. This numbers here. And these numbers, I can compute them as y i times w transpose x plus b. Like that. If I put the equality here, then what happens is that, well, all the numbers that are here in this line, we'll use our response which is equal to plus one. All the points that are here, they produce a response which is equal to minus one. So these three points, they produce responses that are minus one here. The last one here. The rest of the points, they produce a response which is higher than one or less than minus one here. And all they were not inside the margin. So this point here, police response which is positive, we have plus one, minus one here. The response is positive but less than one. So t is equal to one. So t is equal to the response times the label minus 1. G is the response of a point here, minus the response of this point in absolute be, right? So here what we do is we multiply the response times the label. If this, if, if there is a sufficient condition is correct, this is going to be positive. And then quantity minus one is equal to Ci. Right? And so I can define my losses or slack variables like that. So I want my response to always be higher than 1 minus t i higher IPO. And for GI, always positive. Now, I want to control, I want to control the complexity. So what I have to do is at the same time as I minimize the sum of slack variables that control the bigger risk. I have to minimize w that drove the van. And seven, I guess image. I put a 1.5 here because it's convenient for the computations with the seed here. Because these two are not equal to L2. But I know that for a, for a given value of c, This minimum and the minimum find the same. Right? I don't know what you see. So this is my criterion. If C is very high. If C is very high, this one, it is going to be negligible. And then no matter what is this value, since is very high. What I'm going to do is basically to minimize the empirical risk. For a very high maybe or C, I basically minimize the empirical risk. I do not minimize the complexity, I don't minimize h. So Hi value of c will result in a machine with a very high by Mitterrand angers I match. On the other side, if c is very small, well, this quantitative will be very small. And then what I'm going to do with them when I'm doing it seems very small is to minimize this quantity. So if I put a value of c that is small, my beach I want, I guess I mentioned, will be very small. And that is a meaning of see from practical point of view. If I increase C, I will have a machine with a high pitch. All right? So no matter what I know here, I have to put the right value of c. Now. This is something that I have to optimize with respect to w, b, and g. So I have to compute the derivatives of this. Anymore. There's the zeal, but I have constraints. So using Lagrange multipliers, using the techniques that I, that I described. Then. We can convert this, which is a minimization with both sides into a minimization without constraints. And we end up with an alternate again. Problem is formulated as one of limiting Alpha transpose y k, y plus Alpha transpose 1. This is, this is a vector, and these are matrices. So minimizing this quantity is equivalent to maximize this. Right? Here, he kept all the computations of the Lagrange optimization route right? Now. When I optimize this, what happens is they value of alpha I is equal to 0 if x is outside the margin. And, and correctly, sorry, my writing is bad. In my previous life, I was probably about there. But it says outside the margin, I incorrectly classified. Right? If, if phi is equal to see if x i is inside the margin, and alpha goes between 0 and see if she is on the margin. And this is something that we brought. You are seeing the results of the lagrangian optimization. You see that using the colors from the conditions, the KKT conditions. In order to optimize this way as wide programming. And we obtain a result. So beyond that, we have the problem of see, what do we do in order to choose the correct value for C? Well, we have to do a training for a given value of c, will have to set aside data. For a test. We are training with a value of c. And then we were tasked with data, which is binding the drain for which we know the labels. We compute the error. We can see we do it again until they validation error or the error on the test. With this data, we said the sine is the minimal possible. All right? And with this, we will obtain an adequate value of c. Instead of just setting aside 11 portion of the data and using the rest for training. We can do. We can use a technique which is called V4. Does this mean? Well, we take the data, we divided in three parts. We take minus one for training, one for validation. And then we'll repeat the process. And we average the validation errors or the false or B portions of the data. And with that, we will have a better estimation of the test error. Because instead of doing one death, every time we change c, We will do it V times. If we have n data, we chose v to be equal to n and false. Every time will be the process. We will use just one sample for computing the error. All right, bye. We will do it n times. And this is called, leave one out presented. Let me put it in the wild. So we have a set of Beta bead, which is labeled. And then I break it into training data, which is from x1 to x n P tumble training, Y1 to Y2. And PR. I chose for our training with TR, PTR. And that's given a set of values for Alpha. And then do a test with the rest. And I compute the diver. Right? Then I change C and I repeat the process. And this is a simple validation procedure. You understand how it works? I do this as many times as needed to sweep different values of C again. And I will let you know what those mean later. I can do the same. Let me put this graphically. Again with the same if I have my set of data here. So for example, my data here. And I break it in the sections v false, right? This is the full validation. So what they do is choose a value for C men to choose V minus 1 portions or sections for trainings. The training, and then do the test with the remaining section or fall. And then repeated for all the false. So if I have four false here, then I will do the process four times, right? So we'll have four errors. I average. Then I change C and I started also be due to keep Julius II. While I have a finite number of barriers of C. But I will talk about later. So if I have 10 videos for, say, I will do this ten times, 440 times. And I will have 10 values of error. I choose, I choose the best. When v is equal to the number of data, then that as well we don't run with just one sample. And this is called leave one out. Cross-validation is a particular case of before. And finally, how do we choose C? Let us hear that I started with c equal to one, right? Then probably, probably a good choice is to repeat it though we say equal to two. And I want to go all the way out to 100. All right, then, well, we are more than C will be 90. I chose C What one and see what the two. And then when I'm at 19 is, what is a good policy to choose 91 for the next? Remember that when I make my mega question, the answer is always no. So I start with 12. They go on. When I went in 1990, mine is not relevant. Between 12, what happens? Well, I just see one-to-one and I double it that later. But I just see what the two I doubled it. So probably. Or maybe you might see a difference between the behavior with C one and C equals to two. But in C is equal to 90, 91. Command, it's, it's just, it's 90 is approximately the same. So the behavior is going to be similar, sun and butter. Whether we, whether we need to do that. Well, if I choose one and then 2, then I have to choose 48, 16, 32, and so on. So I have to feel, modify my parameters in an exponential way or logarithmic way you want. Alright? So by instead of choosing c equal to one and then adding, and adding a variation, I had to what I have to multiply it by a quantity bit. So I choose c 0 as my starting point. And then C k is equal to c times a quantity a raised to the power k. And a, of course, should be higher than one. The aim is to, then every time I change my C, I will double it. But that seems to radical. So instead of two, I can choose what we'll find. Something like that, right? That's it. I will be. There are ways to do that in MATLAB and Python, you will have the log-space, log-space function in both MATLAB and Python to do this, right? So today, our documentation to generate an array of values of c that are spaced logarithmically or exponentially as the SEC. Why do I say this? Well, when, when we do the homework and I'm going to show, and if you don't choose as baby or C space logarithmically, what happens is that what is happening for low values of c will be concentrated in a small space. So you want to say it, right? So if you choose C from one to 100 and revising thing is between 12 and your space linearly, you won't see anything. But if you space logarithmically for low values of C, you will have higher detail. And you want to say, thanks, proud. Please show you. And I believe that I have two public session here. Let me see one of the homework. So to say, It's time. They always just what the computer and that's it. So let's say we have to ask them questions. Well now wait 10. So I am here to share screen and I share the desktop. So here you see my MATLAB and where do and it does work. Stop. Share screen. That's top of mind. So you're seeing, I hope that people in the remote students, they can see my screen with a MATLAB. You can see it. Yes, I didn't see it. Thank you. So all right. So let's just first do this example, which is not. All right, Yeah, This is so here I have oh, they're not. I'm sorry, but they can share either this or that. So I hope you can see this counseling. Alright, so here I have a solution for super vector machine, right? And this super vector machine, it classifies red and black points. And in this particular case, while we have this super vectors, here, we have support vectors on the margin, these two and these two vectors inside the margin as support vectors that are misclassified but outside that much. Alright, so give we go to the other graph. There's one. Where do you see are the values of Alpha for the super vectors and what the blood types they label. Points are positive. They're red points are negative. We have three points that are non-saturated. These three points are the ones in the margin. So on-demand, right? And this points, they are saturated. And they are the points that are inside the margin or outside and misclassified. So you can see that this situation is 10. Situation is that because I chose a value of 10 for my support vector machine. If I do it again, I will have a different solution. And again, and you can see that it's the same solution because because of the demo effect. So here we have a different solution and a different one. But always three points in they, in three points that have an unsaturated right beside the necessary once they find the TIMIT. And sometimes I have a case in which the machine that the problem is linearly separable. Here you can see that this problem is linearly separable because there are no points inside of Watson. I only have three points on the much. And they don't look don't look up line. But if you go to this Access, you will say that the scale is 10 to minus 4. So there's three points they have on an error which is almost settled by the machine, doesn't really nail that. There is a small error, right? So that means that these three points, they are not suddenly on the margin. They are separate away from the origin by a small quantity, 2 minus 4. Say we do it again. We'll have different, various, different values of the data and different solutions. So let me go back to this problem, the original problem. And here we have machine that has quite a big month, right? So if we increase seen the 100, we have a solution that has pretty much the same margin. And if we decrease, then we have a solution that has, well, in this case, you can see that the margin is higher, right? And so what we see here, with a very high value of c, well, c by 10, we're really reached a machine that has the maximum, whichever one I guess I mentioned, which is 93. So if I keep increasing the mind into 1000, the solution is going to be the same. All right? But if I decrease C, then I decrease the one meter running. As I mentioned, I have on my machine with higher put. Right? Now, they call. It goes very simplistic. We generate the data. The data is given, you can just download the script. And this is your support vector machine. The rest is just plugging and stuff. This is your support vector machine is still here. So inside this line, there are many weeks of me giving you a hard time to understand what they're talking about. All right. And now you do you have an idea. What is inside, what has happened in time, right? This is not what happens. You know, with, in many cases with developers use this kind of functions. They just use it a black box. I don't want you to tears by boxes. I want you to see, to know what is inside, right? I'll talk a little bit more about this, but here we have the options and here we have the training that gives you a variable, which is the variable that contains B, which is, it's called mother ra, but it has the sign a minus sign. And alpha is called model dot S. By this, alpha if it contains its alpha times y, all right, It's already multiplied. That's why. That's it. So there are a couple of videos which I explain the process of installing the thing in MATLAB in, and an example of how to use, let's take a look to them. It's not difficult. Now the other sample has this. What I'm doing here is an experiment many times an average. What is the experiment? I choose a value of c. C is in this axis. And as you can see, it sees a space in and lowering the way non-linear but love anything from C equal to 0.3 or something like that, until down to c equal to 10. Now, I choose a value of c. I do a training and I compute the training error. The training error is they read. Then I compute the test error, which is this brown line. And the difference between both, which is the black one. So this is a measure, an empirical measure of the empirical error. And this is an empirical estimation of the actual risk because I do this with test. And this is an estimation of the structural bag. Right? But I do the experiment many times. Why? Because they've amateur manga theorem says that this happens with probability 1 minus Beta. I do the experiment one time. What would you see is what you saw in the first grade. When you So let me say here say a noisy thing. I saw when, I guess say that it happens with a probability in what we do. What we do here is to average, they will average the realizations. And so we have an average of the errors. How did you choose that either way? But this will be, how do I choose are in the domain. Because the probability is one minus theta 0. I don't, I don't, this isn't bit about what they do is an experiment. And this is not a probabilistic measure. This is this amine. This is a mean value of what happens. Right? It has the same shape as what they show you in theory. All right? And it's important to choose the values of C logarithmically. Because the interesting things happen here. When C is 10 to minus one in my experiment, 10 to minus 1, 0 want. So you chose Beta from 0.1 to ten. And you plot this not logarithmically by linearly, then this thing happens in a very small space. You don't see anything, right? And this is something that you will need in many other experiments that you need to see things logarithmically because you want to see small things with the same scale as big thing. So here things happen between one and 10. Between one and then nothing happens. Between 10 to minus 11, nothing happens. Important thing happens around 10 to 12, right? So you went to space and along the way. And this is the right outcome of the experiment. Of course, now I want you to do it. Okay? Please ask me questions about the homework, whatever it doesn't work. Show me basically the graphs. It's difficult to, it's difficult to interpret. They go by. After many years, I see the graphs and then I know what heights and see the results. And I can be creative and saying, this is what you're doing wrong. That's it. I see you next week. And if you have questions, please me right for us.