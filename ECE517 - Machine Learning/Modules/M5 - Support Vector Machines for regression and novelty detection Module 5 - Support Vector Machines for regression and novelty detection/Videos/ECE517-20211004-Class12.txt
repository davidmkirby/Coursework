 Alright, so we're gonna start this module today. First of all, I have to say that I am going to extend one. So we bathe homework for module four, alright? Because I, I realize that it's too short, right? So it has seven more days. So is that this model is good to read the overview of rights. This is something like y by u and under people online learner, wherever however they are, call. So by it, It's good to know that we have some on a dime and figures. All right, In this module we have we have three. Let me see. We have three papers or tutorial. I'm super vector regression by the author of original author of the support vector regression, Alexander smaller and by his colleague Bernard Shaw cup. Then we will basically study this paper. I'm not going to two read or to show it to you. All the materials that we're going to see they are taking from this tutorial. And the same for super vector data restrictions for vector method for novelty detection. Here, first, we have to see the concept of regression. What is regression? Now, I am pretty sure that you know what it is. Then we will modify the criterion that we use for support vector machines for classification. So it fits into a brush. So we will study the primal and the dual solution for them for regression. And we also, we will also see a couple applications that are very similar to each other, which are super vector data description and support vector method for novelty detection, which are two methods of an unsupervised learning. They aren't useful to detect anomalies in the data. So here, the idea is somehow different from what we have seen so far. And then this, this support vectors, they are usually call one class support vector machines because we assume that all the data belongs to one given bus. And with a low probability, we have data that doesn't fit the distribution of the data that we usually see. And these are the analogous. And these two methods, they offer a way to discover this date. By. First, let's go to super vector regression. Let me start with how do I go back? And I can go back. First. Of course, we need to see what is regression and I believe that store share. So somehow we have same classification, linear classification. And I hope you have a good idea of what or why they, I mean, when I say binary classification, right now we're going to study different concept, which is the concept of great brush, right? And so their relation can be defined as the estimation of a continuous variable from a set of observations. Right? So we have some silly examples, but they are two examples that are real. The right. For example, if we want to be and a rheostat stay in business, we might want to know. What is the price of a given house which is not in the market? And what we can do is to answer the variables that are related to the houses around. For example, we might see a force, its size, its distance from downtown. And here give us constructor and also the prices of houses around and many other things that we take for the data or the information that we can about the observation of the house. And with this variables, we put them in an estimator. And what we want to obtain is the price of health. In order to construct the, in order to construct the estimator GUI training data. Well, we got to what is this famous website with? For example, Z lot, right? It's, it's a web website that contains many houses of many cities around the world with their data. Quite fit this to the downtown. Also, the the zip code, something important, many other things. And with this we can, we can download this data by calling them. There they are, and exploring their website. And this is something that can be done. And while my colleagues in Spain and I, we did it for his obligation and others, we just call websites and we pull our information and we have we can train a classifier by regressor that estimates the price given the observation. Another example which is also real. What is the energy consumption of a city for the next hour or for tomorrow? For example, writing my current research with some students. What they do is to take observations or what, what's happening today and the day before. And with this, we estimate the consumption bit, the power load, the power consumption for tomorrow. So we have a regressor for every power or a joint regressor that does the prediction of the 24 hours at same time. I answer, these are estimations of real value variables. Sometimes this is more an art than a science, right? But it works. And sometimes is this, these methods, they are surprisingly good. We also, we also estimate what is the total solar radiation for the next 15 seconds, or next minute or the next four minutes. Just looking at this guy. Right. We take pictures or bay infrared, Victor's big clouds. And with the machine that follows the sun all the time, all day long, farmers design and it takes Victor's endospores. We also measure the current value of every region. And with this, we estimate the solar radiation for short horizons, which is useful if you want to, what do you want to do is to store solar energy or you want to know when you are not going to have solar energy and then you have to switch to another source, right? The battery or gas or whatever. So in this case, we are not specifying anything, right? By what we do is to observe something or rather the environment and we predict a linear, sorry, another layer. But as we all day horse the PS2, they can be mixed, right? In particular, what we do is to classify by what kind of sky we have. If we have a sky with a given type of clouds or others, or a sky with more or less vorticity, then the behavior is going to be Alright so we can mix them together. So this is the simplest possible example of regression in one batch. Right? Here we have two dimensions, but it's because we represent they, they magnitude that we want to infer or to estimate in the y-axis. Essence it's, it's a real value. Then we need to represent them. Nonlinearities by the observation is in the x. This is, this is. It's got my anyways. So they observations in the x-axis. And the idea is where the observation, which is some value of x, we want to estimate what is the value of y. And this data, this breadth. The ways possible to tip is up here. With this red points. What we do is to train our regression machine. A little bit of Nomenklatura axis the observation. And sometimes it's called a predictive. Write the observation is the thing that we use to make the prediction about the magnitudes of interest. So then predictor is not the machine as the input data. And then magnitude that we want to regress or predict is y. And this is when we got there, regressor, right? So here we have the predictor. Here we have the repressor. And regressor is not the machine either. And this blue line is the function that we construct in order to make the prediction. That is an estimator, a prediction machine, many names, but it's not a predictor, is another regressor. Data. Data is predictor up with data, it's great. So what did I do here? Well, this is clear. A function that we can estimate linearly. So the relationship between the input and the output, it looks like it is linear, right? But there is an uncertainty in the training data, which is what we consider the noise or the eye. Though. There is an easy example which would be I have a sensor that gives me a voltage depending on the temperature. Right? So this device, it has a bias. So at 0 degrees Kelvin, it will still give me about it. And that is a linear relationship between the temperature and the differential voltage between what we have and the bias. So when we have a thermometer that we are not measuring temperature, we are measuring a voltage, or a current, or an impedance, or some magnitude, which is not a number. And say if we are given such censoring, the first thing to do is to be adjusted to construct the function for which the input is the magnitude, whatever, for example, the voltage and the output is the temperature. That will be an easy exam. And so this thing that measure separator, then what happens that is subject to thermal noise, because it is a gift. It is at a given time. Is thermal noise. We consider it pretty much constant in this interval. Like for example, if we are measuring, just invite them, interpret their state, go from 0 for define height to 100 Fahrenheit. It's short interval. And so that is thermal noise. Not much, but it's something that we can, that we can measure, that it's, it's not an Aboriginal, right? So with this, assuming that we have certain measurements. And these measurements, they correspond to the Walters. And then we have and barriers of a temperature that we measure using some other some other method. Then we can compute this regression line. All right, for example, I might have in my lab thermometer made of mercury, right? So the manager is very, very stable and it computes, it measures temperatures with a high accuracy. The problem of theirs, which is big, you cannot store the measurements in a computer even out, we see that, right? Its slope if the temperature changes them. Mercury has an inertia. So given a measure temperatures very fast, right? So if you use one, then you can do it provably. So willingness. And so what do we do in order to, in order to compute this regression line? One thing that can be done, and it will not, it will do a good job is to apply the minimum mean square error. Right? In Chapter I don't know much others. When we talk about Gaussian processes, we will see that one can do if the noise is thermo, is to apply some version of the medium of minutes where because it's the best estimator when the noise is Gaussian. So one can say, well it's applied minimums for tenders. What happens if we have outliers? While it well, the same thing as, the same effect as happened in classification will happen here. If we have an outlier and we don't know the sets, so we don't remove it. Then this line might be biased. And my thermometer will have a log button it. Super vector machine is a good method to just ignore this two enormous outliers, right? But it's going to be done in a weird way. It's a counter-intuitive way of doing it, we'll see now. So this is the simplest possible criteria. Meaning means minimizing the mean squared error. This is my model. And why? The magnitude that I want to estimate is a linear combination of my observations. And here we are not talking about one-dimensional problem, but x, it's, my observation is function, is a vector that has, that has several dimensions, d-dimensional, well, right? And there is a bias B. And between the observation of X. So between them, this operation, this linear operation, and y of n, there is an error. And this is what we want to minimize. So we, we can put B inside W or together with w. And we put, it says here dummy variable, we don't say dummy variable name or a constant, right? In, in x concatenated with x, provided that this is a column vector. Then this expression is exactly equal to this one. Then to this concatenation W, B, we'll call it w and accommodation X one and X. Where doing something that we did in the past. So I can just write y n equals to x plus iy. And this is the arrow that way too many marks. Sometimes we do this because the computation easier, right? Sometimes we could be upside for the same reason, become equilibrated and that we're using. It's easier to put the outside, but both expressions, they are equivalent. And so in simpler, better regression, we will put B by outside. So we are going to use this for me more minutes per hour. We use this. And so the first thing is sublime, degraded, and minimize the expectation of the expectation of the error square. And the arrow squared is equal to y squared, y minus w transpose x, right? So this quantity here. But if we develop this expression, what we have is this equivalent expression, y square plus this quantity squared, which is this minus twice y times w transpose x. So this one, y is a scalar, so I don't care. I mean, we can put it before or after. Now, minimizing this quantity with respect to W is equal to minimize this one because y doesn't depend on w. So we can take it up. And minimizing the expectation of this seems the expectation is a linear operator is equal to minimizing this. So we put the expectation of the sum, is the sum of expectations. And then since W is a constant and here the random variable X, we can take it out here and here. And so. And then taking into account that the expectation can be computed because we don't have the probability distribution of X and Y. Then we change the expectation by the by Dame sample mean, right? And so in the sample mean here we means one over n. But it turns out minimizing this quantity with and without one over n is the same, so I ignore it too. Finally, I changed this expression by its equivalent in major notation that we already know. So we have here, we have here our criteria. This is what we have to minimize. In order to minimize it, we compute the gradient and we equal it to 0. In order to compute the gradient, we take the gradient of the value of this expression with respect to w, which is this. And the gradients of the value of this expression with respect to w, which is this quantity here. This is equal to 0. And that's it. So this is something that I passed from here to here. We have done it, right? But it's a good exercise to prove that degree them has this form, right? And you can write to this expression by just computing the derivative of this difference here with respect to one singular value WE and then picking the right. So in order to prove this, you can use this derivative. So you compute the derivative, each one of those wives time. Now we have this, and from here we have to extract W. And we have this expression which is exactly the same that we obtained for classification. The only difference is that why there's not a set of labels, but it's a set of progressions known because this is strain rate. And that is an example in UNM. Learn for them. So again, this approach can be approximated by a grand descent method. So instead of using this result, we can go back and say, well, this here is the gradient of the love of their squared error with respect to w. So what we can do is to start with a given value of w, arbitrary one and go decreasing, go changing this value and a direction contrary to the gradient. And so here we have gradient descent algorithm, which is a recursive approach approximation to the result that we had before. And it will change matrix x and vector y by a single sample. Then instead of having made exact stamps matrix X transpose, we have vector x at times vector x transpose. And here we have the same. Then we take common factor of y, of x here. And we have them this equivalent expression. And here what we see is the error. And this is the minimum that, that is their list means is blurbs Louisburg, which is our way of saying right here up. So this algorithm is the easiest. I will end up. We can have in order to find the optimal parameters w for this problem. So, well, by haven't fully examples, but you will be able to see them in, in number, right? So that is an example of regression. Of course. This is one criterion, minimization of the minimum, minimisation of the mean square error. But what we want is to apply another, a different criteria, which is that maximum matching criteria. And as I said, this is done here in a way which is that the DNA is not totally intuitive. But this is what we have to do, what we have to take all of our, There's less, right? So first, we have for this edition of rough definition of linear regression and an example of optimization using minimum mean square error. The particular assertion using LMS lib, placement of squares, which is a recursive algorithm, and the data db algorithm. So now let's see how can we do it using this symbol tables. So I have to say this works. So how do we do support vector regression to prove that somebody doesn't change arbitrarily. The right. We see the initial value of w using what least squares is arbitrary. So we have to choose a given value, right? In one dimension. So this is my length, three or less. Learned this from my right. One. I mention if this is the error, this is this, whatever. This is the error, the squared error, and this is w and beer and has a minimum for a given value of w. Let's call it w star. This is the optimal bay, but they don't know it. So I start with an arbitrary value, for example, a value here. And then I can compute the gradient of the error squared with respect to w. A gradient of the error with respect to w. It goes in this direction. So I have to move my present value, W k minus one w in the direction contrary to the graded. All right, so then W k is equal to w k minus 1 minus the gradient of the error with respect to W. By I don't want to apply the gradient. I want to apply and small quantity in the direction of the gradient. So I have a value of mu here, which is a small quantity. For example, 0.10101. I'm starting with an arbitrary value. Then iteratively I go approaching two name of the hobby. When I arrived, when I arrived here, the gradient is 0, so the RNA, when moving on. And we can apply this gradient, which is proportional to x, x transpose w k minus 1 minus y k. Sorry, why? I'm, I don't remember it. So it's X. Why? Right? Or I can apply an approximation which takes just a single value here, similarly here. And after operation, it's equal to the sample times DR. And so this works, and this works to four values of meal which are sufficiently small. But we're going to apply this. This is an array that can be promising for regression. For example. In his resignation, you are a quantity here that is proportional to W. Here. You saw that failure to apply a different criterion, which is a super vector machine approach. So first, we need to do this too. Creative, things that will clarify, fire. For it. We have to find a set of constraints. Work. So we have to find a set of constraints, define a set of loss of variables to define an individual rights. Right? So this is something that should remind you of what we did for classification. We have to find an embedding of risk. And this empirical risk needs to be something that we can treat. We can something that is mathematically tractable. So we find a set of linear, slight variables are lasts. And then we have to go find an expression that gives me that is proportional to the complexity or the gametes have only because I mentioned, which is the square root of b of a. And we have to minimize both firms together. This is exactly what we want to do. But then now the regression model is defined as, as in the screen. So with a set of variables, parameters plus abides, right? So we put divides outside and we have an hour. So the error is also, is a continuous variable in R. And so they, this is like variables can be also defined as a function of this error. And we do that like that. Okay? Let's see. First, they, they, they expressions and then we, when we want to interpret them. So they, slack variables are defined like that. Instead of having a set of slack variables, we have two of them. That the error is positive. Big the error is positive. We want this error to be less than a given constant plus as like able. And discuss time is an error tolerance. Right? So we will tolerate an algebra error. Why? Because if we do not tolerate an error, then we will have the maximum possible expressive capacity of the machine. So if insulin is high, we will have a simple machine, a machine with a low botnets are wanting because I imagine. But if we increase, if we decrease epsilon, then the capacity of the machine interests NSF or positive ours, we want now to be less than Epsilon mass plus this slack variable. And we will integrate our, in our, in our drawing. When the error is negative. Then what we do is to change the error, they sign of the error, we change the sign. And then this error with a minus sign should be equally less than good salon plus as like variable. In order to distinguish between the slack variables for positive and negative errors. We put a star here for negative x. Alright, went with this definition, both nape slack variables for positive and negative errors. They both, they're both positive or 0. Right? That is, if I have yes here. So this is the interpretation. Assume that we have a set of observations and big X axis with a corresponding regressors in the y-axis. And we want to set up a regression line around this, this data base. This is my regression line. And then what is epsilon? Epsilon is the tolerance on my data. So, so if this is my progression point, I tolerate an error that goes between 0 and epsilon or between 0 and manage the issue. And this is what we call the epsilon tube. As this band here is called an epsilon 2. And whatever. Higher than epsilon. We assemble whose error is higher than Islam. Then we compute this value, which is the difference between the error and the reason. Why. So this is the excess of error beyond the epsilon. And here we are the same for negative Arabs. It's important to remember this and this, they are positive quantities. So when we have a positive error, we compute the error. We subtract the epsilon, we have shape. When the error is negative, we first change the sign of the error. Then we subtract epsilon. T star is epsilon, that is, that started at the same data constructed. This margin, SVM, It's a margin, yes. So let me see intuitively how this thing works. Let's assume that we have this set of data here, like this. And we have given machine with a value of epsilon. Let's see this. So epsilon is this. Say you have samples in size samples, I'm sorry. This is epsilon. All right? And so in the, in this line we have a regressors. This is epsilon n is my error tolerance. If this is y, this is y n plus, well, this is not who I am. This is w transpose x plus b. And so here we have w transpose x plus b plus Gibson. And this is w transpose x plus b minus epsilon. And there is another quantity that we should care of, and this quantity is the margin. This is my margin. This is B. Right? Now using the same epsilon, the same. Let's construct two more, two more examples. Let's see. Let's do it in another graph. So we have the same epsilon here. Let me see if this works. Let's assume that we have the same value of epsilon, right? Let's construct a line that has a higher slope. Right? A higher slope, that means that W has a NOR, which is higher. And so let's see how this works. This is my margin. So here with the same epsilon and align that has a higher slope. We have a smaller margin. But Wittgenstein, another regressor. Let me see if I have another color. So let's, let's do it here. Let's say we have another regressor with the same margin, with the same monitoring but with our blast like this. I'm sorry, but I got well, let's, let's just do the limit based on Start. Now, a repressor with as low, which is 0, That's going to be the minimum possible. The minimum possible north of my set of parameters w. In this case, B is higher, right? So if we increase w, we decrease the margin. If we decrease w, We imprisonment. Alright? So we have an idea which is similar to the idea of margin in classification. But here is adapted to regression. But we need an extra parameter which is the error tolerance. So here we're going to have two parameters, c plus this error tolerance, which is x. And this two, they have free parameters. We can now train them. We have to cross validate, right? Yes. So I mean, for the minimum margin then our curve was pretty much 90 degrees. Yes, that will be the minimum voltage, which is going to be 00. So as I basically get spider the work. Yes. So yeah, I would say we increase this to 90 degrees. Then we have that the norm of w is going to be infinity. And emergent. It. For instance, you write this margin, increases or decreases with the Arno. And it's only in the green range? Yes, because if I go beyond that, then that's a full cycle. Exploits one. Here the tangent of the angle is 0. Tangent of gamma is infinity. Then if I go beyond the tangent of the angle increases again, right? So the tangent and the norm of W, they are proportional. What I am saying, right? So volunteer proportion. So that same idea. Let me go to green here. All right, so insulin is a fixpoint the day. And then the margin, which is this distance, increases or decreases with an orbital on so days. The original, I said the original because we have two possibilities for support vector regression. That is that first one. And the first one, what we do is to minimize the norm of w dy is y on false Debye micelle running this dimension and two dimensions, sorry, one I mentioned, this will be directly related to the slope. And then whether we want to do is to minimize the quantity, be off some of it's like pebbles. So we want to put inside a budget, asthma, asthma as many data as possible. So inside the margin or the Epsilon two, we want to put as many data as possible. By that the effect on the other side, we want to minimize the value and minimizing w means living samples outside. I made good samples. They are the ones that are inside the axon. And the bad samples they are the ones that are outside. Let's compare this diversification. What are the bad guys in classification? There was the, the inside. Then these ones are the samples that define the classification boundary. So conversely, here we have regression function that is a function of the samples that are outside, including the outliers. Right? So this is my minimization. And to minimize w, to minimize this sum of slack variables, they push in, in, in opposite direction. And so this is why I had to put constants, see that weights, how much importance they get to the empirical error. How much importance I gave to the meter running this damage. And they deem slack variables are defined like that. So for positive errors, we want the excess error to be less than TEN. For negative errors, we want the error minus the error to be less than g n squared dx as error, right. And epsilon S, as I said, a constant. This is the paradigms that I give to my guess is that every pound of deselect that you will see. Now, the slack variable is just positive because from 0 to infinity. Alright, so what do we do that? We want to find w and b. And by the way, I never talked about B. We have to have hard to find me. It's extremely easy, but we never matures so much. So what we know as multiply, we multiply every constrain by Lagrange multiplier. For this constrain, the LMS will do babies alpha i 4 over alpha n. This case. For this constraint, we call it alpha star n. And for this one stays, we call them Mu and Mu started. And we'll repeat the same bevel omit that we did for classification. So we compute the derivative of the chance of this plus this multiplied bys concepts that we minimize this Lagrangian with respect to the primal variables which are W B Ci, Xi star, W, W, t and t is time. And so here I'm saving you. Well, this is the optimization, as I said, the positive constraints. We want to find the ones I star, mu star. And so we have, we doubled the number of constraints here if we double the number of rides with the buyers, but the process is exactly the same technique that we use is exactly the same. So we have, finally, we have this dual, dual functioning. Where alpha is a vector containing all the multipliers for positive constraints. Alpha start for negative. K is x transpose x. K is a matrix that contains all of the dot-product between input data, between input vectors, right? So here the entry i, j is the dot product between XI and XJ. And so this is multiplied, pre-multiply and Muslims, but here we have alpha minus Alpha star transpose times I. And we have an extra element, which is epsilon times one, that's alpha plus alpha star. This is the sum of parental the labs times epsilon. So when epsilon is 0, that disappears. And so these two components, they are a linear function of alpha. And this component is a quadratic function of alpha. Since k is a positive definite matrix, that means that this product is always positive. And it's going to divide by minus one. This quantity is always, that is always negative. But it's quadratic, so it pass or maximum. And this is linear. So quadratic function with a maximum and a linear function, it gives me one single maximum and only one. Writing, it gives me a maximum and olive oil. And so we obtain additional constraints that say that Alpha and Alpha and started data between 0 and C. This is a valid form, so it has uniqueness, existence and uniqueness of solution, which is good. I only have a solution. There's no local minima or local maxima. Local maxima because this is what the laggards are unassigned. So it's okay for the matching mindset. And they, optimization of this is exactly equal to the optimization. Or they're super vector machines for classification, we have to apply quadratic programming here. And so with this technique, we obtain a set of learning for the buyers. Now, how do we construct they? They regression function is easy. We know. We know that w is a linear combination of the input data. And in this case, for this case in the garnishment do recommendations which are the derivatives of the Lagrangian with respect to w. We'll take this. In order to obtain this, we just need to go back to this expression, compute the gradient with respect to w. And so here we have green or this is, oh, I missed that one out there. Anyway. So this is w squared this way and a seal. And then here we have alpha I times times x. And here we have minus Phi squared times minus tax, right? And the rest of the terms a sub b, I compute the gradient. And you obtain this expression here. When you compute the gradient of this, what do you obtain as W plus the sum of alpha i x i minus the sum of our five-star x i. And we have two equal this to 0 in order to obtain the optimal value. And with this, we simply get the value of equal to alpha i minus alpha I start times x. All right? So once we have our set of alpha and alpha star, mind solving this dual function out. And we can construct w with we can construct w by applying this equation here. Then you will answer that one. This one. The biggest one, discipline, yes. So whichever shouldn't we use fast-food certainties? Which equation do you use to? We had to insert vz minus GitHub if minus signs in front of the animation. And these mindless, yeah, so we assume that alphas are bigger values of alpha or possibly, right? We could have some negative. But the idea here with the following. So we have this base of W0, W1, W2, for example, in two-dimensions. And we have an absolute minimum for w squared. And then we have a set of constraints which are this. And so what we want is that both gradients and in the optimal value here, at the optimum value for the value, it's the minimum that satisfies the constraints. Both gradients, they are proportional. So what we do is to compute the gradient with respect to w, which is this one. And minus the gradient with respect to. So f minus the gradient of, of the, of the W n. Sorry, let me start over, right. At this point, what we want is what we see is that both ways are proportional. So what we do is to compute the gradient of w squared with respect to w, which is this thing. And the gradient of the constraints with respect to w. And we multiply times then banks minus 1. Then we multiply that times Alpha. And we solve for Alpha. So fast, sorry, w with respect to W2, W big W. But this is the gradient of w squared. And this, right? And this is the gradient with respect to the volume on the constraints. Right? So the first step is one. We computed a Lagrangian, which is equal to W squared. Of course we have our firm. We don't have just the value. We have W, N, and T prime. But if I want to, if I want to representing, then I could then tomorrow we'll just w, but we have W are cheap. So here GI, GI, science, right? And then here we put minus alpha I times a constraints. All right. And then we compute the gradient of this, which is equal to the sum of gradients with a minus sign. Because they want an expression where both gradients, they have different sign, a different edge. And then one way at this point, we do this. Then for a given value of alpha, they did this linear combination is equal to z. And this is why we have a minus. Thank you. We have a minus sign because we're assuming that alpha is positive. The minus sign, we have to put it externally. And so we have an expression with respect to Alpha and this, and we'll select off, it, needs to be maximized so that we have this, this expression. So if we have the values of Alpha after solving this optimization umbrella, then we need, we, we need to do, or we can do this operation in order to find w. Of course we don't need it. We don't, we don't need that because since my expression is, my estimator, is y equal to w transpose x plus b. Then I can just change it by this equivalent function. Where these ads is a test data for example, and eggs. And they are all. This is the set of training the right. So this is my new estimator function I hadn't really done. Now, how do they support vectors behave? Well, the samples inside the margin or inside the epsilon feel they have a value of Alpha which is exactly equal to 0. So if I am in a problem like that, where this is epsilon, if our sample is inside the margin, here, alpha is equal to alpha star and it's equal to 0. For assemble with an error which is positive, it is greater than epsilon. For example, this sample here that has this error with respect to the regression. Then we have a value of Alpha which is equal to C. Then a of alpha star is equal to 0. And the in the sample BS, it has a negative error. Then here we have TI's time, which is different from 0. And then we have alpha i j. Alpha j is equal to 0, and alpha star jane is equal to c. And then we have a situation in which two different situations, when were they samples? They might be on the margin. We will have samples on the March for the samples, Alpha I is equal to 0 and alpha star, It's pretty k. K is between 0 and same. And here we have the opposite situation. Alpha L is between 0 and same. And Alpha L star is about the sale. This can be easily proven same way as we did in regression and classification. We have, what we have to do is to compute the corresponding conditions. One of the conditions give me this result that is in the projector. And with bother the other kind of conditions, we obtain this product. What happens with B? Let's go back. We have two motive line. Stop here. Nice Now, sorry, I'm sorry. I'm sorry about that. Share screen again. What happens with b? Well, we know how to compute the gradient with respect to w of this arrangement with respect to w. What is that? Each gene and with respect to B, right? And so we would, let's, let's do it as an exercise. I'm going to write, well, and I got to write this expression on the whiteboard. But I gave you the derivative of the Lagrangian with respect to B. And this is equal to, as you can see, is equal to, well, this here is 0, here a 0, and here we have minus Alpha I. That's it. So we have minus Alpha minus five, minus the sum of five. And here we have minus of i started. All right? And so with this, we're seeing, right? There is a sign here, positive. This one is positive. All right? So we say that the sum of alpha is 0. Then what happens if we compute the derivative with respect to two Epsilon? While we have seen we have derivative of L with respect to epsilon I is equal to c, right? And here we have minus Alpha I. Find that minus P0. And this is equal to 0. And this is one of the conditions that tell me that if the sample is inside the margin than Alpha I is equal to c because mu must be 0. Right? So if with, with, if somebody is outside margin, then mu is equal to 0, and then c is equal to alpha i's. And mu i must be 0, because t is not 0. And the product says that the product must be sued, right? And this is one of the corresponding, be additional correspond to the conditions. Political BI, team I must be 0. So if this is not 0, meaning that the sample is outside the margin, mu i must be 0 and that graphite is equal to C. Okay? And what is the other colors from the condition? The other one says that alpha i times y n i, whatever, minus w transpose x minus b minus epsilon plus, let's see, minus I. This is equal to 0, right? So when I is 0, this quantity is 0. In particular. We have that. In particular we have vectors on the margin, right? And for vectors on the margin, for, let's say if x is on the origin or the epsilon tilde, then GI. Ti is equal to 0. And then we have that y i minus W transpose x minus b minus epsilon, sorry, plus b plus b minus epsilon is equal to 0. Because TI, TI is 0 and Alpha is non-zero. So this quantity must be 0. Do you follow me? Alright, so we know that GI is 0 and we know the sample x i. So from here, we can extract b. B is equal to epsilon minus y i plus w transpose x plus b. So we take one symbol that is on the margin, and we will always have samples on the margin. We take one. And we're exactly, but it has fibers that work. It's like an upside like that a little bit. You might expect this one. So if TI is 0, that means that the sample is inside the epsilon tools or on Dave, So until then mu i must be, can be different from 0. Right? So if the sample is outside the margin, t is 0, t is nonzero, sorry, some u must be 0. But this is not a condition that they care of. I, I meant this one. This should be 0. So it's either alpha is 0 or this quantity is 0. If Alpha is non-zero, that means that the sample is outside the margin or ponder much. For the mandibular case, when the sample is on the margin, their chin I is 0, but alpha is non-zero, is between 06. So this must be 0. So in this case, this quantity, this expression is true, right? When somebody is on the margin, actually, the error is exactly equal to Epsilon. So naturally this is 0. And from this sample we're going to be biased. We just need or want. So yeah, I was going to talk about, so usually we have more than one plus the optimizer is not perfect. It will stop when the error in the day when the convergence or write to Emergence is below a given limit. Alright? When samples from one edition to another don't change too much values of alpha. They don't change the minds of the day. The machine stops optimizing, so they will be an uncertainty. So what the machine does and the software, software does is to take all the values of be computed with all the samples on the margin. A computes the average. And that decreases the uncertainty. But by a baby which is equal to the number of support vectors on the watch. Okay? So we have number of support vectors on the margin equal to the number of dimensions plus 1. Right side we have the dimensions. We will have what? D plus 1, because on the margin, we compute b for all of them, they will be slightly different. We compute the average, That's it. That's when the machine has Yes. Absolutely. Not readily and flip them. So it's, it's a free parameter. Now. So when we compute, when we compare, optimize the machine for a given value of c and I get that. Then we have to try it for validation. Sample, change the values and do it again. So the cross-validation here is a little bit more complicated. We have two dimensions. And so we will see the examples in the next hour, how this thing works, how the super vectors place. And we will talk about outliers, which is an important property of this super vector machines. They are robust against our lives. Right? So I'm stuck here. Can I see your abolished? Yeah.