 Great, I hope today. So we'll start with the slides. So before we start, hello everyone, where we started. Let's allele. And so this is the threshold using the supermesh, the machine graded, right? Well, remember instructor that the algorithm writers like that is the same. Our estimator is always y equal to w transpose x plus b, where b, yes, For this case, found in Gaussian process, we will put it inside W0. And here we have an error on y belongs to. So it's continuously evolve rather than on labels. So provided we have a set of training data. And here will represent x in the, in this axis and wife in this axis. Remember, we call this, well, this is the input patterns, the data vectors, the instances. There are many names, but here, mostly people say verse. And Y is a set of real valued numbers. These aren't regressors. And then so what we do is to construct a function that approximates they if they, if they input tuples. And what we do is to establish what we call an epsilon tube I or no one. Why this is called a queue. Because actually it is not. It's the movie until only in three dimensions represented one line in one dimension and on till around. But here it's not the case anyway. So this is called the epsilon tilde because this distance is plus minus. And this is what we call the error tolerance. Epsilon is the error tolerance. And the way this works is is the following. We, we, we describe an optimization functional with constraints. And after that, we obtain a solution where we have vectors inside the two vectors, upside and letters in on the epsilon margin. And my solution is always W0 equal to the sum of alpha i minus Alpha I. Start. I were W is the set of parameters or the estimator. X by r, the training data. So this is from I equals one to capital N, assuming that we have and data for training. And how fast they are the Lagrange multipliers of y. Lagrange optimization way give if x by as a positive power. Then it is higher than epsilon, then it is outside the margin. And they've all where the regression line. And then Alpha I is equal to c. And alpha I start is 0. And otherwise, if x i has a negative error less than minus epsilon, then alpha I is equal to c. Sudo, sorry, alpha i star is equal to see if the samples are on the margin than the error is exactly equal to epsilon. And then alpha i for the case of positive error, is less than c and the other alpha star is 0. And otherwise, if the error is negative and exactly equal to minus Epsilon, then alpha is phi star is between 06. So these vectors, they have a value of Alpha which is saturated. This vectors, two vectors, they have values of alpha that Amnon Saturday, the ones that are on the margin. And finally, the vectors that are inside the epsilon, do they have a value of alpha i analogy of a five-star equal to 0. So if, if the error is less than epsilon, the absolute value of x is equal to three is less than epsilon. Then alpha I, alpha star is the partnership. And so beam. So they set of parameters w is constructed with a subset of samples X i. So this machine is sparse. They super vector machines for classification is also spots. There are other super vector machines that are going to list the square super vector machines that they use all the values of all of the samples. But I'm not explaining them because I don't think we need that. But you can take a look to the list. Score is super vector machines and seeing how they are constructed. So This is the way this thing works. Fine. So I said in the other day, I'm, I see the other. So last Monday I said that this super vector machines, they are robust against outliers. Why? Because no matter what is the error produced by assemble, its contribution to the solution is bounded. It's limited to see are secondary In the second line. Here. On the second from the top. And the second line is one. But this is what, yes, positive, positive, positive power n, which is higher than epsilon. Right? So when we have a sample that is very far away from the regression line, or better said, when a sample is far away from the rest of the samples. Then if we use our minimum mean square error, then what happens is that its contribution to the solution is equal to the error. And so there's my buyers. The solution if we don't have to make assembled, it will have many samples. Then a single sample we contribute with a quantity equal to the error. But if we have many samples, this contribution might be negligible. And whereas we might have positive and negative outliers that will basically cancel each other. But if we don't have too many sandbox, it might be the case that we only have one outlier, just one. It might be also the case that the noise or the error that we observe is not Gaussian here will not make any assumption on the error. By it's, it's very usual to assume that the error is Gaussian. And so in this case, all the samples or many samples, there will be a ROM. They, the classification regression. Right? If we're very unlucky, we will have a sample with a low likelihood that is very far away because it is possible days as a low probability that in a Gaussian distribution we get the sample with higher. By these maybe not you for that. If the error is not Belgian, then the situation may be reverts. It might be the case that we have somebody with a higher error and with a high probability. For example, in ongoing communications, we usually deal with Gaussian noise because the noise is produced by the antennas and there. And the elements of my receiver, right? They, they, they amplifiers and the rest of stores. They produce thermal noise, which is in nature Gaussian. All right, bye. We may have interferences that are not Gaussian. For example, if we have electrostatic interferences, they produce spikes. And these spikes April is high noise. Okay. And then say we have one of those. Yeah, and we use our gouache, we use Gaussian assumption, we use millimeters per hour. Then we might have an estimator that doesn't work problem. This is a way to avoid the situation by limiting the contribution of each one of them. I've added my look counter-intuitive that we are using the samples that are far away from the, from the regression lines to construct them, to construct the estimator. But that's the way it is. And it's again in this situation is do all to the one in classification where we use the samples that are very close to a classification of 9, which are the samples that they have more risk of misspecification, right? So in classification and regression here, we basically use the bad guys that will stop it there, their estimate. Now, how did we do all this? So the way we we do that is applying their criteria that was applied to the subarachnoid machine or a probabilistic occupation. Um, so we, we minimize a measure of the day empirical risk and a measure of the struct or a risk at the same time. Remember is a theorem that says that there is actual risk is bounded by the empirical risk plus a quantity that we call it stuck to our risk defense on H and N. And quantity worthless was enough. I going to be. This quantity is arbitrary and it tells you what is the probability that this happens? Age is the ammeter wanting because I mentioned that tells me what is the expressive capacity of the machine. And that is the number of samples. If we have many samples compared to their botnets are running because they're mentioned, we probably 1.5 a problem. So sometimes using support vector machine for regression or classification, it's worthless. And it may lead to an increase computational environment. By it, we don't have too many samples. Then this is one approach to cancer because it will limit this took their risk by the optimization minimization or they buy me to run, I guess average. What happens is that the bigger risk and it's the doorways, they are not, is it computes actually, they cannot be, this can be computed but not optimized. So what we do is to construct some linear cost function over the earth. And in the case of regression, our primal functional is constructed with an arbitrary quantity c times the sum of slack variables t and t star. For a empirical risk plus 1.5 of the norm of w, which limits h. As we saw. If we, if we decrease the norm of w, then it decreases. So here we define T and G star. So now the error when it's positive, y minus w transpose x minus b. Yes, should be less than a fixed quantity, which is a tolerance plus this error, this error which is the excess of error beyond b Epsilon. So what we want is still live and this quantity here, if the error is positive or somebody is negative, then we change the sign or the arrow. And we do the same thing. And in order for this to work, we have to make sure that we follow this tolerance. And we achieve that by just forcing that Xi and Xi star are non-negative. So when a sample is inside the epsilon 2, then GI or GI start, they would be negative. And so what we do is to force it to 0, right? So since we do not separate positive and negative errors, if t is positive, then she start would be negative. I. So when widow is to discard the one that is negative, we set it to 0. Now, that is a great deal. And they've algorithm consists of some minimization of this primer functional subject to this constraints, we have three sets of constraints here. So usually we write this as minimize this subject to this constraints. And so what we do is to multiply each one of these constraints by a Lagrange multiplier. So each one of these constraints, we multiply them by alpha i. This constraint, we multiplied by my alpha i star and this two by mu and mu star. Actually, when we though is this, we put all the variables to the right side of the equation and these are my fav stats. And we add these constraints multiplied times Lagrange multipliers to the primal. And intuitive way to see why this thing works is the following. If we have, let's, let's repeat the example the other day. If we have a regressor, W has one-dimension and when we have two here, right? So we represent here two of the primal variables that we have to optimize in order to minimize this constraint optimization problem, right? So we have THE star W and we also had to be, these are the, these are the primal variables and so two of those, they're here. And in this case w. So in this case they, them, this primal. We'll have a minimum somewhere. Right? So if I minimize with respect to achieve, to start w and b, we will have a meeting somewhere. And this is something that is quadratic with respect to w and it's linear with respect to t. And she'll start. So we'll have something like that. And then we have the constraints. When they're Constanza equal to 0. Then they describe a hyperplane. And this guy, in this case, it will be aligned. So these are my constraints. Quarters if so, what is the minimum of this cost function or a function that satisfies the constraints. While it's here. It's here, It's a point where both the gradient by these constraints, which since this is a claim, the gradient will be in the direction of it as a vector associated to the plane. And then we have the gradient of this cost function. So they will be in the same line. They will be proportional. Both. They point towards the same direction. So what we do is to multiply the gradient of the constraints here by minus alpha, where alpha is a positive quantity. And then we compute the gradient that we compute this gradient of this gradient with respect to these parameters. And we find alpha. So these two, they are equal. And they are equal only in this point. And this is true because this is quadratic function. So when I cut this quadratic function with this Plane. I still have a function and it has the same mean, which we can see here, right? And so this is the intuitive explanation of the Lagrange of the message. Right? So when we do this, we obtain an equivalent problem to optimize, which depends only on, on alpha and alpha star. And this problem is what they call the dual problem. And it works like that. Let me see if the signs are correct. Because I never remember if I don't do it. If I don't do it, I don't remember. Which. Yes. Indeed, this is positive. This is negative, right? So this is my deal functional, which is a quadratic function of alpha minus alpha, where alpha and alpha star here, this is a matrix, they are vectors, right? And so this matrix is positive definite. Then this product is always positive. And this is linear. Yes. It's possible there. There's a transpose here. Yes. And this is transposed to all right? So all my doctors are column vectors. So this transpose, and this is another good way to write it as epsilon 1 transpose. Yeah, that's much better. For me, it was correct, but epsilon is a scalar. So this is a positive quantity. And this are, this is a linear equation. So we have a quadratic term and a linear term that gives me another quadratic surface that has only maximum because this is multiplied times minus one, right? So this quantity is actually always negative. So we have something that works like that. This is the flow of product. And so since the primal problem is quadratic form too, but positive yet. And so minimizing w and b with respect to w, b and c is equivalent to minimize with respect to alpha. In a space where we have all the variables, the primal and the dual. What we have is a saddle point. Is a saddle point is a point where the function is minimum, respectful some, some directions and its maximal with respect to some other dialects, right? It's three-dimensional reconstruction of the dimensions. What we see is like a subtle to write courses. I still, it might be that exactly that because there might be a gap here. But anyway, so Let me see. Yeah. So this is my dual and mind your pets. It's optimized using optimization. And one of the problems is that we find when we constructed you all is that w is equal to the sum of alpha i minus Alpha star times x i. So this is proven during the optimization when I compute the derivative of the Lagrangian with respect to w, This is what I find. Right? So w is a linear combination of the day. And the same happens with key. But in order to find it, we have to use the procedure that I explain. The other way if we take values of x i that are on the margin, and then we isolate b. Bye. B is at any rate with it is, it is a linear combination of the data. This is something that we find during day of the optimisation, but We can say that this is always true. By Bill 2 of the generalize representer theorem, that we will see some wave right? There representer theorem says that under non-restrictive conditions, what the conditions being that in order to solve my optimization, then I have to use convex measure of the error plus nondecreasing function of w. This is what we have here. If we use in my optimization some convex function and non-decreasing function of their norm of w, then always happens that w is a linear combination of it. There's no other terms that doesn't depend on the day, right? We will see that we will prove the theorem by it. This is something that big bear in mind. All right. So this also gives me an approach which allows me to work in the dual space, right? We will have to plan to talk about the dual space a little bit more today. So my estimator is y equals to w transpose x plus b. And assume now that X is any sample. It can be training and the test. But then I can change w by this expression here. And then I have the sum of For I equals one to capital N of x i transpose times x plus b. And my estimator, s2, sorry, I forgot to put something here. Alpha I, alpha I started paying for that slide. X i, x star from xy plus b here. So my estimator turns into a linear combination of dot products between the training data and test it. Right here with all the samples. By, in a normal situation, we might expect the, expected that most of them, they have values of alpha that are 0. So this is, except in pathological situations, this is a solution which is Inspire. So even if I have many samples, this sum actually contains a few of them. So this is a solution that produces our computation on partner. Like one-off. With my students, we are lying super vector regressor for an application. Well, we try many different ways of doing regression for prediction or forecast. And so the standard support vector regression, well modified for modify for multi-task, for predicate many texts at the same time. This will be status of a vector regression. Yet it takes 0.1 milliseconds in a normal computer, laptop to do our prediction, 0.1 milliseconds in Python. And then the rest of predictors and we have that are not despise. They have, they use all the training data. They, they take one or two orders of magnitude more for the British. And this sometimes is very important. And of course, we compare that with deep learning. And deep learning approach might take 1 second. And when I want a prediction now, and then second hour before those next. So this is another probability of the first proper. Then there's approaches, they are robust to outliers. Second, they are sparse, so it passed, they are fast. We're going to say the same for the training in general, because we have to solve this optimization problem. And it takes a day and it needs the use of K. K is a matrix that contains all the dot products between data. So if I have data here, my matrix that contains n squared numbers. And manipulating matrix for such a problem or for an inversion, for example, it has a computational burden of n to the third power, right? And to the third power. So if I 0.5 million data, I have for products to optimize this, it will take forever. And remember that in my optimization either half, I, it's not enough to do this once. I have to do this for every value of c and every value of epsilon in a range in order to find a meaning. So in this kind of machines, the problem, the computational finance is in the train, right? We will see also regression using Gaussian processes. Gaussian process. It takes a model for a probabilistic model for the app. And it doesn't need any sort of cross validation on the parameters they are optimize using maximum likelihood. So I don't need validation. So basically I have to do optimization once, well, with, with some constraint here, but I, I don't have to do the optimization many times. In order to optimize many parameters. We will add at least one more parameter. On the other side. This solution is not spiced. So here we have all the data. So we have many, many samples. Here. Then I will always have a computational world enter the ND equal to enter third power. Plus. Then I need to do and dot products for every sub y. There's no free lunch. Remember? So? Well that when I read that, I say that many times, but this is my particular interpretation of there's no free lunch there enough last theorem says that every single classification or regression machine, it has exactly the same error if we tested with absolutely all possible problems in the universe. Right? But then consequences are these ones. Alright, so let's see. This already downloaded. Yeah. Yeah. So this is my cost function. I need to share screens nerve. Whereas now by grandmom, have a power to teach you because not be able to do all that. So she didn't speak English. So this is my dual function out to be optimize. I'm not going to both Ibn Sina, this quality programming, there are versions of by the branding that are a fast compared to a standard one. So what happens here is that while I included with respect to the original solution there, this quantity here. Why do I do this? In general? Why do we do this? Thanks. Why do I add a small diagonal? I is an identity and gamma is a small quantity 2. Why do I add a small quantity to my matrix? What do you think? It's kind of files? Let's build this. When we optimize this. Next option is delegates. Well, here, no, it's not this. Any other guesses. It's more mundane. It's born. It's easier. Why do I do yes. This table, because he began it began to you in the video games because of the gap. That lets it says paragraph version. Well, it's related to the error function like so obvious or the optimizer state of this one. Well, that is an explanation that is a reminder what we want to invert this, or we have to establish a quadratic programming algorithm. This k has to be positive definite. If it's not positive definite, we want half a single solution. Right? If, if k is not positive definite, then this one will be this pre, I'm proud of Mencius. Part of sometimes will be positive, sometimes will be negative. So in general, we will have more than one, more than one maximum. It may still work, right? It might work if I use a very matrix K, which is not positive definite in my work. But if we want to have a single solution and we will have a hard time to find a solution in the particular case where k is positive definite but not much. Let me explain myself. Okay. This is a square matrix, so I can represent it in terms of eigenvectors and eigenvalues. Is it, right? So k can be decomposed into Q Lambda Q transpose Q is the basis of eigen. Vectors are vectors, they are orthogonal and they have no Not equal to one. And lambda is diagonal matrix containing a so-called IN phase, right? And this eigenvectors and eigenvalues, they have a nice property is that if I multiply k times one eigen vector, where they have is the eigenvector again, won't delay banks a constant, which is the Eigen value. Eigen in German means something like self or something like that. So this also adds about outdoor batters because when we transform them with the matrix game, they reappear, right? So they are invariant with respect to k excuse me, Server Report or the sorry, yeah. Bates. Yes. So here, k can be represented in terms of eigenvalues and eigenvectors heal. They are the set of eigenvectors and lambda is a diagonal matrix that contains all the eigenvalues and they have this property. So if I multiply x cubed times x cubed here reappears scale by lambda, which is the corresponding eigenvector itself. With this, it's clear that if I multiply alpha transpose times Q, q transpose Alpha, then this quantity is always positive. Only in the case where alpha, where the eigenvalues inside they are positive. And this is, it's easy to prove that, but it'll leave it to you, right? Well, actually, there is a theorem that proves that day. But I don't remember. I don't feel I have remember anyways. So K Alpha transpose Alpha is always positive. If lambda i, they are positive or 0, right? If and only if for any, for an arbitrary vector here. So if this is not true, we have a problem here because we don't have a single solution. Let's put that in as an easier problem, easier handouts that we, that we already solved. A problem where we have to invert the matrix. What happens with k minus 1? This is something that we don't do here, but we do it for a minimum mean square error. For example. And we have exactly the same issue. K minus 1 is equal to Q Lambda Q transpose to what, minus 1, the inverse of that. And using the properties of matrices, what we have here is Q transpose minus 1, lambda minus 1, q minus 1. But it turns out that Q transpose Q is equal to the identity because these vectors are orthogonal and they have norm 1. This is the dot product between any pair of vectors. Someone a vector is, when in this product we compute the dot product of one sample with itself, one vector with itself. The result is one and otherwise is 0. So this product turns into a mega into the matrix. And then it's easier to see that Q transpose Q times Q minus one is equal to I minus one. And then this is obviously the identities of Q transpose is equal to q minus 1. And so this is equal to Q plus or minus one. All right, So inverting this is as simple as inverting each one of the eigen values here, because this is RNA, there is a matrix. So here what we have is it's one of the eigenvalues, invert it. What happens? If an eigenvalue is very small? Then the inverse where we Betty Hi, Is that a problem? Is that a problem? Well, my questions, they have our CSM as an answer. It is, it is not because if lambda exists, lambda minus one exists. But it is, it produces a problem. Yes, it does. When we are computing this inverse in a computer. If we have a very small number, we're not going to use too many bits in the representation. So we will have a high relative error in that representation. Right? And so when we Ybor that will have a very high number with a very high error. And this very high number with a very high error will produce a batch solution. Something similar happens here. If we have an eigenvalue with a very small number, then we will have a bad solution. And this is what we call a meal posed problem. This is a type of a meal posed problem, a problem that doesn't have good solution. So what do we do here? Instead of using, Hey, minus 1 or instead of using k, we use k plus gamma. And this is equal to q transpose, or sorry, Q Lambda, Q transpose plus gamma pi, which is equal to heal than that q transpose plus gamma Q i Q transpose, right? Since Q transpose is equal to the inverse of Q, then the identity is equal to this. And then we can say that kb wasn't camera I is equal to q times lambda plus gamma times Q transpose. So if I do this, they eigenvalues, eigenvectors, they remain the same. But they eigenvalues, they are added a small quantity. And this, this way to see the regularization. Regularization. This is something that we do here, is something that we do again here, and this is something that we do in ridge regression. So we erase the values being eigenvalues. The eigenvalues very high. That contains a lot of information. And this quantity gamma is negligible. If the eigenvalue is very small. Since we have gamma when we eat it, then we want to have a high error. So this value lambda should be small, but not too small. So we put ten to minus 900 if it's not going to work because we will have a narrow Monte Verde. But if we put 10 to minus 6, then We are using 64 bit floating point. It's not going to be a problem. Right? So this is a way to see this. We put a small value of gamma here in order to produce solutions. To produce the problems that have a good solution. A solution that doesn't have a new medical problem due to the rounding errors in my eigenvalues here. Right? In order to invert matrices, we don't usually compute day if we don't compute the eigenvalues and eigenvectors. But even if we've done, the problem persists, right? Because inverting this matrix is exactly equal to one, so the product will persist in time. So this is regularization and new medical regularization. And it avoids this kind of problem. This is something that is always done on the machine, doesn't tell you what parameter you want to put here. Inputs are small numbered enough for the machine not to have high running Arabs. But if we have very small but negative values here inside, then using that will turn them into positive values. So we also, we also avoid error here, right? And that is it. So with this, we did this with the hope that this quantity is positive definite width of eigenvalues sufficiently high to we deal with. All right? So this happens here, like it happens here when we do not emerge. Because when we multiply a quantity by an eigenvalue, which is very small. This eigenvalue still contains a high tariff federal property. So we'll have problems with established. Now. They said that is on, there is a an alternative explanation of that, right? And so this is well known, these are 30 of expansion. Well known since I was in kindergarten and probably before. Alright. But for surprisingly, nobody published it ever. So my friend, this guy, I will publish this. And so you have this paper in the, in UNM learn to take hold. So what is depths? In silver labour machines or regression? We implicitly apply a cost function which is linear. Remember that way month my primer five on 1.5 of W squared plus C times t I plus GI transpose, where TI, it has a definition. It's the error minus epsilon when this quantity is positive and 0 otherwise. So if we plot the error, the error. So this should be fine. It's floating. So deliberately error here of a sample. If the error is less than epsilon, 1 is positive or in the earliest higher than minus epsilon for a negative Enter, then our cost function doesn't count the error. Right? So t, we're dropping to 0. This is t, i or she starts. Right? So when they arise between these two, these two limits, which are called the epsilon insensitive band. The epsilon insensitive band, then t is 0. So we ignore that happens when the error is higher than epsilon. Then what we know is still multiply the access of error times c. And this is this lobby see, and this is the loss that we apply to the sample. My loss is equal to the loss or lack of GI. For positive errors and negative errors, we have this. This is my cost function that we implicitly apply in support vector machines. And as you can see, it's complex. But it has a problem. The problem is that when I compute the derivative of this, then I have another dimension here. And I have fun in that admonition here. And this is something that will cause numerical problems. Wife, while it's proven that it's proven my corresponding conditions. And we did one for classification way I didn't do it for regression. But it is proven that when I optimize Alpha, the values of alpha, they are exactly equal to the derivative of the cost function. The derivative of the cost function is C here and minus C here. Right? So this will be alpha and this is minus alpha. I start for a positive error is higher than epsilon Alpha I is C. For a negative error lower than epsilon minus epsilon alpha i star is minus C. Sorry, alpha. It's that easy to see, right? But remember that alpha star, they are multiplied that. So there's this epsilon, epsilon for those samples that are on the margin. Then the values alpha and the value of alpha is something between 0 and C and between 0 and 90 see four main page. So this sandbox, which are very important because we always have samples, random samples, they have a value which is the end of the reaction here with materials. Right? So what do we do? Well, when we don't kiss the following, instead of applying this path function, which is 04 samples inside each on two n plus minus C times g. This, this equal to times t, sorry, the L minus epsilon. And here we have the same c times b value, absolute value of minus epsilon. Right here. Instead of applying this cost function, we apply a different one. Small variation. And this variation we are between the symbols inside the epsilon 2 and the samples that are outside this until we are small quadratic component. So let me see if I have yes. So instead I will blame this function. What we do is to write a function that has this shape. If the samples are between mitosis or an accident, because function is 0. If the function, if the error is between epsilon and epsilon plus Gamma c, where gamma is the small quantity added and C, S and C is this quantity here. Then the cost function is quadratic in a way. And we adjust the parameters so that anybody here is 0 and here the derivative is exactly equal to C. And then if the symbols are outside epsilon plus Gamma c, Then we apply this linear function which is modified. So originally this was c times b epsilon, sorry that, that was the error. This is R1 plus minus epsilon. And we add this quantity here in order to adjust, in order to adjust this curve. So here we have a daylight equal to 0 and here the width hypotheses for both. Right? So then we compute the derivative of this. And it's very easy. If the error is between minus plus minus epsilon, then alpha and alpha star, they are 0. If we are in this band between 500 and needs plus Gamma c, The derivative is linear. And then here we exactly know what is the value of alpha. When the machine is optimize. The value of alpha. Between these two elements, these two points is exactly this function is a linear function and outside is plus or minus c. And that solves the problem. Now, we use, instead of using this cost function, we use this one that has an epsilon is as the band Nirvana. We put that in the optimization. And after a lot of and after a lot of algebra. And this is exactly what we obtain. And proving that adding this small quantity here adds a quadratic section to my cost function. So we have something like that. Gamma is very small. So if c is not extremely high, gamma c is very small. So this quantity here we said, we see a significant bye. Bye usually is very small. We don't see it, right? But it produces a deterministic value for alpha and alpha star. And if we don't do this, we have fun in the damage. Right? So instead of having an epsilon 2, we have, I need salon volume here. So my, if my sample is inside this epsilon volume than they value of a value of the alpha is proportional or is equal to the error term proportional to the ER. Right? And that is the more complex interpretation of why this is important. Well, so in some cases, my error is Gaussian, is Gaussian and then all the Gaussian values, they are around the Earth. And that provides an additional property here. If I use as Malvolio reasonable value of epsilon and I'm not so small value of gamma c. Then all the significant errors, they will fall in this band, quadratic bumps. So for this errors, my machine, it has an optimization which is optimal for Gaussian noise, right? For all the Gaussian samples of the noise, I am applying this optimization of quadratic one. And we will see that this is the best that we can do. But then if an outlier appears, this outlier will fall in this linear band. And so we will be applying or limited contribution, I'll be sampled IR to a construction on the estimate. So this applies an epsilon band, a tolerance. And this tolerance helps in the way in which the way we saw last Sunday, Monday. This helps to minimize for you optimize the parameter running is DynaMesh because this is exactly what defines their margin. Then we have a band that optimizes the solution for Gaussian samples whose resignation class and also limits the contribution of a outliers. So this is actually a very good solution. It is a very good solution. It works very well. Of course, in linear problems, but we will, we will extend this to nonlinear and the effect is exactly the same. All right? So that's it. That is exactly the function that you optimize. What do you use az vm layer for? Scikit-learn Pytorch? They are, there are other libraries that do the optimization of super vector machines. This is why they solve with this quantity here. This quantity gamma is very small, but you can add an additional value of gamma if you know that you have Gaussian noise. All right? And so the solution is a good bite. Again, there's no free lunch. The solution, it's better than the standard support vector machine. You laugh Gaussian noise. But I needed to modify it. We better. And the way I modify this, adding an extra parameter gamma that I need to, they need to tune. So I happen to know whether the nicest question we've asked, because block predictor that SOS and do this regression and find the error, sum of the errors. And then black arrow, definitely see how they are distributed. Well, basically you are saying, why don't we do some test in order to see how much samples are Gaussian and what is this brand? What is the Bayesians? Yes, this is something that we can do and we avoid tuning gamma. And actually, this is, since I wrote many papers around, this thing will get, we figure out ways to do it, right? So you my, heuristically say, well, this must be equal to two sigma. Then you will put healing sine 95% of Gaussian errors. And then these are the outliers. Right? There is also, if we assume that this quantity is very small and we don't care about it. And we only use epsilon Deus paper. That says that there is a relationship, linear relationship between epsilon and sigma when the noise is Gaussian. And I found this, I wrote this paper this morning, but I don't remember anymore. Well, it's a paper on the linear relationship between Epsilon and gum and lumped Sigma, which is the variance of the noise by the standard deviation of the dose by a guy called clock. With miliamps still have it presented in mind. I did it. My second linear dependency between epsilon and the input. My spirit, the data is longer. Epsilon and epsilon support vector machines by James walk. Very nice guy. I met him in Spain. So that's that, That's it about super vector machines for standard support vector machines for regression. By, of course there's more. This is the standard super vector machine. But then chop off and co-workers, they had the idea that this epsilon, which something, it's something that I have to, to tune. And it goes between 0 and infinity, right? It's not bounded by anything. It can be changed by another, a different variable that is bounded. And we can bounded between 01. And that has advantages. I. In this case, I have to sweep a value between 01, not between 0 and I don't know what. So I still have to tune C and T. And another parameter, this parameter is change it by another one that is alternative, that is bounded between 0. Right? So that was disadvantages. But besides that, this alternative super vector machine that I will present not today. But they will present it. This valley, this, this parameter that we call, we call it the deal. This parameter, it has an interpretation in terms of how spies my machine should be. So we know that a machine is represented by a limited number of super vectors. Some of the support vectors are saturated and some of the support vectors, they are not. Alright? So meal that was between 01 turns out to be a lower bound. On the number of Saturday, the support vectors in a, in a higher bond of the total number of support, fraction of superlatives. So I might say, I want a machine that works with 1% of the data that they both. So I just need to say nu is equal to 0 module. And that if I have n vectors in my training data, then I will have just 1% of n vectors in my solution. I still eat it. You're right. Because 1% might be about. But at least I can't, I have, I have a method to limit the number of subgraphs. So this is the alternative super vector machine invented, introduced by the same authors and ended the best. Still there's more. We can apply the same trick to machines that don't classify or do regression, but machines that basically what they do is to find outliers. And in this case, meal. Yes, abandoned the number of support vectors, but also bound on the number of outliers that the machine will fund. So if I say, You know what? I have a problem with, the number of outliers is limited. It's limited to 1%. Well, I put gamma, sorry, nu equal to 0.01. And the machine will find at most 1% of samples to be a blindness. Is this useful? It is very useful. Say for example, I apply this to detect faults in a grid and electrical grid. I know that I have 14 per year. So I can easily tell the machine, the tag me, this person Peytchev out life. And so this is one we're going to do during the next text. And that's it for today. I know it's very difficult. It's not difficult. It's very high. All right. I know it. Right. So again, if I need to go back to slowdown to change something, please let me know. Alright, let me know. And I'll do my best to watch the class just a second.