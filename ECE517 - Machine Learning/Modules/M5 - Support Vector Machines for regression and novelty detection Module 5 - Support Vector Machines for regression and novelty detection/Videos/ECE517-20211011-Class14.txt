 All right, the last session, a spark at expression and we saw that essentially follows the same, same idea and as a support vector machines for classification. Except that we need in order to control the market. And so they said, All right. All right. So I'm going to guess, right? So, so if you remember, we have the regression line in the machine for regression, we establish some parameter epsilon t, Which that our minds what samples are half an error that we can delay, that is a better color. And this is called the epsilon 2. So they are the samples that are outside the margin or on the margin. They have a value of Alpha which is none, which is non-zero. And whatever is inside pass value of Alpha, which is right. And remember also that we have different values above a positive or negative value. So they do of the dual functional to be solid business or function of alpha minus alpha star, where alpha is the positives. Said it was a bit of a set of multipliers for positive errors. And alpha star is from negative Arabs. And so that produces a cost function which is linear if there is higher than epsilon. And so what happens with this is that we have to cross validated because there's no way to optimize it. There is a paper, that paper that I commanded last session and width stays and analysis of the dependency between epsilon and the input noise. So the noise of a input values, but it's not too useful in, in normal, in UFO cases. So that is another way to there is another way to cross-pollinate epsilon through different parameter that we call knew. That doesn't go between 0 and infinity as epsilon, but it goes between 0 and m1. And that's the right areas to, instead of having to cross-validate this parameter of which we don't know what are the limits between 01. So what we do is to change it by another one that goes between 0 and 1, right? And so this is rough, okay? This is summarized in a single line there and no criteria. So the so-called new SVR, the new support vector regression machine, is a way to tune epsilon by adding a parameter that is intended to impose a bound on the fraction of support vectors, right? And this is a paper that you have in that you have in your name material, supplementary materials of this, of this module. That equation explains it in for, but essentially, what we do is to minimize w squared, which is. Screen, which is, you know, I turn that controls about meteorology dimension. And here we have a term that is related to the building of error. Here we have four epsilon grass, sorry, g plus g is dy. And then we add this tag here, which is n. N is the number of the number of training samples mu as a parameter that goes between 01. And then we can, we have gave you an antiderivative of baby for capsular. And so this is intended to optimize at the same time this quantity and this quantity here. Right? And so they, they, they, let's say I fairness and work today. So they, constraints are the same, right? This strain is from the positive error. So we have positive error. We want this error to be less than Epsilon much, Bye, bye. And for negative errors, we want the error minus the error be less. We want it to be less than Epsilon plus t star. And this two, they have to be positive or 0. So when we have sample inside the margin, since t energy and start a will be negative, then we draw them, right? That's the meaning of this being positive. And so here, n times nu is nothing by a number which is less than men. I'm just going to be the number of support vectors. We were apes. So if we multiply each one of these constraints by Lagrange multiplier alpha, alpha star mu, and use that. And we put it, we subtract that set of constraints. Where did my sense from the primal function of the mouth, Alexandria and functional which has none constraint, we compute the gradient with respect to the primal, the primal variables which are w and b. And we basically play with a results of this, of this gradient. One of the results is the same as w is going to be a linear combination of the data, right? So this doesn't change. And so our diol, it has these aspects where epsilon disappear. Right? And so we have, but in change without a new set of constraints, well, the sum of alpha minus alpha star is equal to 0. They may use Alpha hostile. They are between 0 and C. And the sum of alpha plus alpha star is equal to this quantity here, C and D, right? So essentially Epsilon is done. And we don't have to optimize it. Now, this has the way in which we optimize these dual is exactly the same as, as in the previous cases. We can use quadratic programming, embodied learning, how to use a Hollywood programming algorithms that we have. They work the same. So we have two inputs, the equation constraints in matrix form and there are three hour, it doesn't work, right? By the important for us is not the optimization is that once we have our functionality or functional optimize, this is true. And it's particularly for this second constraint that the sum of Alpha plus Alpha, alpha is equal to c. And how do we interpret this? Let's see. So since alpha or alpha, alpha or alpha, they are less than c. And at the same time, the sum of alpha plus alpha star is equal to c and mu. We have to bear in mind that. Only one of the values of alpha, one alpha star is non-negative, the other is always 0, right? This is what tells me that since the maximum value of this sum Alpha plus Alpha and start the maximum value is c. Then this quantity here. Let me go back to my board here. So we have the sum of alpha n plus alpha n type is equal to c. And new clothes. Since only alpha or alpha and style is, is, is non-zero and the maximum value of alpha is c. Then this sum, the sum is less than this quantity here is less than the number of support vectors times signal. All right? So this is something that we can say, right? So C goes away. And then Neil can be a wisdom like that. This quantity, this fraction, the number of super vectors over the total number of samples for the training is higher than GIF. Which means that nu is an upper bound on the number of supervisors. And using the same reasoning. We can say that. So here, we only account for the number of support vectors on the margin. Taken into account also that, or is it best to protect us on the origin? This quantity is less than c. We can also say that mu is an upper bound on the number of support vectors on-demand. So this is a way to control the computational complexity or my machine, which is a direct way to control the Omniture wanting this dimension of ownership. So we can make two different uses of this. If I want a machine where I want to control the sparsity. So if I have n samples by I only I, I've, I want to use at most a machine is expressed as a function of given quantity of super vectors. Then I have to do this. I have to compute the ratio, and this is my video. All right? And so if I do that, I know that the number of support vectors is limited to that I was. Right. But this is not going to be the optimal machine. The optimal machine is one that has a given value of Neil and that is not dependent on my my requirements. I'm a computational complexity of the machine. So in general, what I have to do is to cross-validate, you, buy diapers. Validation goes between 0 and 1, right? So new cannot be 0 because in that case, I am, I won't have any support vector. Machine is we want numbers to the trigger, right? And if nu is equal to 1, the number of support vectors back-to-back. So I have to cross-validate between these two. Which is a task which is easier than when I do the cancellation with respect to Epsilon. So for example, I can't start with a given grid values between 01. You and I can compute the error. One measure of error, for example, the sum of alpha or the error squared. And so I will have an optimal value. But then it seems my lead is probably to force. Then I can do a second pass that limits the values between these two. We start over. So I can start with a course grid and then do a final one and so on. So this is why in general, this is easier than feeling with epsilon because epsilon, the minimum value of Epsilon is 0. And if you put epsilon equal to 0 in a support vector machine for regression, it will work. But in summary, abdomen. And what happens is that we don't know what is going to be the max. And that's an example here. So this is an example in one-dimension. Right? So where here we have x. I love to go on. So I just gaze is x. And this is one thing here by generated an example in which my function is a linear, linear over x plus a constant. And I added notes, this is Gaussian noise of a given. I think it's, I don't remember Wednesday, our elastomer. So I find a optimized to work with nu equals 2.5. So the total number of vectors is 16. And the number of sub a, the ones destructive for training of 30 training data, which is probably not correct because Neil is 15. So I should have 15 failures or model for the data. But anyways, so three of the vectors, they are saturated or unsaturated and arrest data. And in this case, in ways we have a line in two dimensions. One for iPhone, I will always have three unknowns. So if my value of mu is, is side, side, the number of support vectors is less than three. The machine has no power to convert, right? There, isn't it? All right. So how this machine, when false, the complexity or the ammeter running, as I mentioned, they're seen while, if it, if I decrease the value of nu, then I'm going to have less support vectors. And this will mean mandibular and insulin will increase to include more vectors inside. They each look too. Right? If I knew close to one, then all these other doctors will be outside of a margin. That means that epsilon will be very small to include all these vectors. Isn't this right? As was the position of the alignments are very upset by this daily. And and that's it. The only thing that I'm not explaining. This is the way in which we pass from the primal to the dual. But I don't think it's interesting because it's, nothing by multiplying is one of the constraints. Sorry. Yeah. What about each one of the constraints by violent with the buyer? Subtracting this to the primal, computing the derivatives, right? And remember that there is always a couple of complimentary conditions that said that either they constrain or the multiplier 0. So the product between the constrained by always said. That happens in the, in the optimal quote. That said, so here, it's given with an example. Here I have, while this example is the same. And by what they do here is to sweep the value appeal between 0.910. So let's see if it's possible to zoom this up. And I do zoom here, here, here. So this yellow line is the identity, x equal to one. This is the number of silver matters. And this is the number of such great supervisors. All right, So which shows, that shows that the newbies upper bound for enamored of our bond for an hour or so records. And what I had here was correct. Here was correct. So mu is a lower bound for the number of support vectors and an upper bound for the number of saturated. So I have a burette. And so this line here is the value of epsilon. Epsilon can be computed. It doesn't really matter, but it's easy to compute it. And we can save that as the number of super vectors in Greece or as new increase, epsilon regresses. And so if I arrived to 0 to one, then epsilon is basically zip file. And that's it. Pastels is it's sophisticated regressor, right? It's a lot more difficult to optimize this done optimizing our simple ridge regression. But it has a damages. So first, this is a very, very efficient way to confer advantage of, I guess I mentioned weights. The weights and yes, in turn, an efficient way to minimize the overfitting, which we don't have in individually rational deliberation. The way of, the way of doing it is it's indirect. And also this approach is robust against outliers. Just as the yellow like. Well, that's it. So in this lesson, I want you to remember what is the purpose of the modification that introduces new piece to fall. First, mu is way easier to optimize, not optimize, but it's easier to cross-validate than translate an epsilon. And Sagan. Neil gives me an upper bound on the number of lower one, not a super vectors and an upper bound and the number of saturated supervisors. And so we write this conversion through the constraints that we found in the Duo. Remember that we arrive to this population just by the examination of this constraint. And final aim. Wow, we have same behavior for the machine with respect to Mu, right? So if we increase the number of silver vectors increases up to numerical to binominal data. So if we increase nu, epsilon decreases. So if we increase new epsilon decreases, meaning that the Barbary traveling, as I mentioned, increases, right? Barriers it. So then using a similar MVC, using a similar reasoning, we can construct super vector machines that are lumped supervise. See, I hope I'll answer this question. So why do we want unsupervised super vector machines? Well, we want this kind of super vector machines in order to cover the, further up the observation. So for example, than we have. A set of data that say they're made up like this, right? We want to know what is the structure of this data. If we see that in two dimensions, There's no problem with it. We said, so we can say, well this looks like a Gaussian or buyer. Buy it. We have this in 20 dimensions. We don't see the struct obviously, right? And we want to discovery in this case, one can say, for example, this data can be enclose in this circle here. Why do I want this? Well, the data is enclosed in this circle or this shape here, this function. This is data that it has a high likelihood. Say if I have a set of training data like that, my next observation, it's probably going to be inside this circle or this, or this shape here. And if I have a sample which is outside, this is going to be on a layer. And this is exactly what they want to Netherlands when I want to discover the structure of the data this way. So the super vector machines and unsupervised support vector machines, they are design mainly to determine whether a sample has a high likelihood or it has a low likelihood. And this is something that is useful when we have many dimensions, right? If we don't have too many dimensions and a lot of data, there is a lot of ways to do this. But if we don't have too many samples and many dimensions, then it is going to be difficult to infer the structure of the data by using a mathematical model, for example, a probability distribution, right? This can be easily being close enough. In a Gaussian, we can compute the mean and the covariance of this data. The mean was here at mu. And the covariance is a matrix of two for two dimensions. And then I might say this data comes from a Gaussian which is proportional to, well, that is proportional to the exponential of minus 1.5 x minus mu transpose sigma minus 1 x minus mu. So this is a Gaussian, right? But here I am, I'm kidding because I can see the data's what happens if I have any dimensions. What happens if I have infinite dimensional cells that I mentioned spanned by the day basis to add and I have my FAFSA data. It might be the case that this is not that simple, right? So we have methodologies to still is called the data when we have many dimensions. And in the case in which we don't have too many data, if we don't have too many data, then mu and Sigma and they're not going to be properly computed. So this trick here is not going to be valid. In any case in which I know that my data is enclosed. It comes from about distribution. Even in this case, if I don't have too many samples than Mu and Sigma, there's no way to compute them problems, right? So here we have method to infer the structure of the data. When parametric methods don't work because I cannot compute the sufficient statistics are my distribution. So how do we do these things? Well, if you take a lot to the definition of super vector novelty detection or the original paper. It's kind of kept it, right? This is what you're going to see. The vapor. Great. So assume some dataset drawn from a later probability distribution. So a later probability distribution is a distribution that you don't know, that it exists. Laden means that you don't know and you will never know. Right? In my, in my classes in the second semester, we talk a lot about latent latent variables and latent functions. Functions that you are not allowed to see me and hear them, and they will always be here. And this is the case of a set of data. They are drawn from a distribution. When we are no toilets. So this data is drawn from a laden probability distribution. So they know the detection is to estimate our sample subset of the input space of the data, such that at this point P, Let's say this one down from P lies on side of F. Then the probability of pulling, draw from this distribution lies outside. This is subset. It was to some higher probability specify between 01. All right, Let me explain this again. So we have a sample of data. And so we have some samples here and here. So let's, let's enclose a subset of the date of this data. So the probability of finding a sample outside of the data, it has a probability mu. So in this case I have nine data here, 9.53 data here. In this case, the probability is 1 third. So Neal should be 150. Imagine for example, and they have a problem in which we know that we have some outliers and we know what is the frequency of our lives. I'm working in. And in the case of finding faults in a, in a great interactive, but with the probability of finding a fault in a grid is 0.1%, something like that. So I make observations, I collect observations from, from the environment, current and voltage or whatever they have available. It doesn't my, whatever they have available of the behavior of the network or the grid. And I use a super vector machine to enclose all the data in, in a function like that in a subset. So only 11 percent of, of, of data is outside. And that is what tells me to determine whether a sample comes from normal behavior or it comes from abnormal behavior. The way I do it, this is the ceiling. It's simplistic. By compute I observe the voltages, then I predict the occurrence, but by also happy parents. So I do the prediction of the current from the voltage, and I subtract that from the actual values of the car. So then I have a prediction error. If this prediction error is between this is less than r given limit, then the data is normal. If the original array is high, what happens is that the behavior is abnormal, so I'm not able to predict that this is the space of errors. It's going to be in three-dimensions because I have Fe parents and female bulges, right? But that will be the landscape here. I have the normal errors that will be basically around the origin. And this are abnormal ever. So. Just doing this, I determine whether I have a folder. Now. What is the beauty of this? I'm not training a machine by him yet. This is normal. This is an error. I'm not doing that. I'm not using a set of labeled data saying this is normal and this is an error or a false. I just put the data in my machine and let it figure out. And this is a training which is called unsupervised, right? Because I do not supervise the output machine. The machine does it by itself. So the machine is able to determine whether there is a normal behavior or a fault, even if unobserved fault never happened in the past, right? So it's never happened in the past. This is a novelty. Write a novel observation. And this is why these machines, they are call super vector novelty detection machines. And so this is why this is not a problem, that, that the problem itself is not actually a classic problem. There are many ways to the novel objects, and it's very important, it's very important in enforcing machines. In systems like an electrical grid, for example, an infraction it in our computer network that can be detected by abnormal behavior. Many things can be detected using novelty that actually in social networks, it's important for some people still detect abnormal behavior and social networks. I, some very rich, very famous people are in trouble now. Hence, Facebook. Why else? Facebook, Instagram and WhatsApp went down for a call. They write nice quantity of thank you played. Anyway. So this is classic on, but it was never saw before for litigation which are not too many data. Right? So if I want to establish a probability distribution, I mean a lot of data to do it. And so this is a way, it's an alternative way that doesn't meet you too many sides. Now, how do we do this? We have two ways of doing that. Name, super vector novelty detection or the 1plus super vector machine. It has these two names. Nobody has action or super vector machine. And they are super vector data description. There are a few different approaches. It's been proven that they are equivalent under given conditions. The first one is not intuitive. It's not intuitive. This is the original supernatural novel, the reduction by about childbirth and coworkers. So what they know is we use a hyperplane that separates the normal data from the non-obese the data with high likelihood from the data. That is, that has a low negative. And the assumption is that the data with a high likelihood is away from the origin. I know this is not true. It doesn't need to be true. I know. But let's assume that this is true. And that the data with a high likelihood is away from the origin and the data with about likelihood is closer to the origin. And so what we have to do is to average a separating hyperplane. And then we define losses epsilon, epsilon chains for the data which are between the origin and then hyperbola. So for these data, epsilon would be negative, so we drop it to 0. And only, we only account for the values of epsilon, sorry, epsilon chain for the data which is this side on the origin. And the space, sorry, the subspace or this classification Harvard brand, is defined using this equation, w transpose x equal to rho. And then rho is the distance to the origin. So Y1, if we want to enclose as many data, as many as we can, the sign. Then we have to minimize the sum of cheap here, which will increase this quantity. And so if we increase the distance from the origin to the plane, then we will enclose more data among the data which is considered doubled. Right? So we are doing when applying here and optimization that has two terms that go in opposite directions. Maximizing the distance to the todays to the origin. At the same time, minimizing the sulci. And we had to do it with the introduction of a parameter Mu, the same way as we did in a super vector machines for regression. And this paleo new Will, it is a bound on the probability of finding my sample or the wrong side of the margin. So what widow is exactly this. We have to minimize the norm of w. Of course. We have to minimize the sum of t n, where t n is defined for those samples that are five-day, that are closer to the origin than the plane. And she should be positive. So the samples that are, are they that are further away from the origin and from the plane? They have a value of T, which is 0. We drop it as it seems, we have to minimize this quantity. This functional. Here. We have Rho. Since we have to minimize my new row, that means that we have two maxima. Okay? So here we have the minimization of w minus rho. That is what controls the complexity. And this is what minimizes the empirical error. And bone, they go in opposite directions. Again, what we have to do is to, well, we pass these two variables to the other side of the equation. We multiply each one of these of these constraints by the buyer will not divide x alpha. And this will motivate banks deal. We compute the gradient with respect to w epsilon and then row. And then we have this Duo, which is very simple here. So we have to minimize Alpha transpose Alpha subject to discipline stance. This is the result of the Lagrange optimization. First, let's see what is alpha transpose K alpha. We have seen that many times, but what is this exactly? This is exactly equal to what we have seen that many times. I never pay attention to it. Minimizing this is one. Under the representer theorem. Any mandibular for this, we can prove it by just computing the, computing the gradient of the Lagrangian with respect to w. W is equal to the sum of alpha i x i o. Right? We have seen this three or four times four super vector machines for classification. We have our phi y i x i regression. We have our phi minus Alpha less than 1 x i. And here simply verify xy. This is very simple. If we compute the derivative of this Lagrangian that includes this one with the MBS with respect to w, we obtain this result right away. This is equal to alpha transpose RMSC. This is equal to X, where X is the set of training data and alpha is a vector containing alpha I. Right? Now, what is k? K I j is k I, j is the entry i j of matrix K, right? A entry aij of matrix k is simply the dot product between X i transpose X j. Right? So then k is equal to x transpose x. Where here we have all the training data in all matters and all the column vectors. So if we compute. Say we compute for remote students. This is something that we can, that we can extract from the garnishment, the recommendations. But there's something we have seen several times. And we can express this sum as this product or the training data times or are they failures are paid by the buyers. Pay is a matrix whose entry by J is the dot product between XI and XJ. So k can be computed that as the matrix containing all the training data transpose times itself. Now, how do we compute W squared? W squared is equal to w transpose w, and this is equal to Alpha transpose x transpose x alpha. So alpha transpose, okay? This is the norm of w. So minimizing, this is minimizing the norm of w. In the other support vector machines, we maximize minus this, right? As equivalent. For support vector machine for regression, we have alpha minus alpha star as w norm of w. And so we have to minimize this quantity subject to these constraints, alpha 0 and 1 over n mu. And the south of us is equal to one. Right? So we don't have, we don't have a value for C. Even says. And so the first constraint, well, this is L2 constraints. What do they mean? Well, the sum of alpha I is equal to one. But we only have some values here for which alpha is equal to is, is different from 0, which are the super vectors. All right? So the number of support vectors, alpha I is equal to one. Yes, Sorry. Yeah, that's it. And then we know that the number of suits that Alpha I is between 1 and 0 and 1 over n mu. So the maximum possible value of this will be one over m is equal to one. Well that will be higher than model. So we, we, we isolate new from here. We have The Neo is equal to the number of support vectors over the total number of samples. So if we go back to this, the super vectors, they are either samples that are on the margin for which alpha 1 be saturated will be between 0 and 1 over n. New supervisors that are outside of this genome is separating hyperplane. And for those, the value of alpha will be saturated. So if we establish, if we set up our prior probability for the number of outliers. Here, it will. It turns out that the number of outliers will be less than. The percentage of par value for the fraction of outliers will be less than mu. Because some of the support vectors will be nasa, where we honor. All right, so we're doing this. I will always find a number of outliers which the fraction of allies which is less than, it might be 0, but it won't be higher than you. So this is a clever way to find that lives in a case in which I don't have enough samples to, I don't have enough samples, samples to get a distribution theory. So this is agnostic to the distribution, right? We don't know and we don't care. And still you can find. What is their product here? They are liars that they don't need to be here in a minute. All right, so how do we solve this? Well, we will need to do transformation into a higher dimensional space. So this is true. And it's kind of strike for a business forward to do, but we will talk about that here in the next lesson, the next model. And so this is what we call the super vector novelty detection, and this is what I just explained. Now. There is another way to do it that kind of solves the problem of supermodels not being a novel, this not being the origin in a different way than this. This is the concept of super vector data description. It's Mobilier with more intuitive that the first launch, right? So I always wonder why, how is it possible that Rogoff and coworkers or write to this, to this solution. And they found it useful, and it is useful. All right, buddy will be evident in the next module. But this one introduced by tox, world verse is a little bit different. So high enough sample of data. And in order to detect the outliers, what they do is to enclose the data in this more or less passive of a sphere. At the same time I have to put, so there's my a is positive a sphere means that I want almost all the data inside. So I apply the same trick. I from peers. I establish the mean or its descendant, sorry, of the sphere and the radio. And I have to minimize the radius of the sphere. I think most of the data, but i, or some data to be about. And so what they do is to compute our loss. Or as lagged variable that goes between the surface of the sphere and the South. So the distance between the sample and the center minus the radius of the sphere, that is my gloves in the US is negative. I drop it to 0 because what that means is that the sample is inside the sphere. How to minimize their radios and at the same data, how to minimize the sum of losses. Again, we have a function up with two terms that go in opposite direction. If I want to minimize the number of the sum of like variables. Here, I am minimizing unempathetic. I'm a big risk. And minimizing the radius of the sphere goes in the opposite direction. It increases the loss or the empirical risk, but it minimizes the complex. So, Dave, optimization problem is this one. I have a sphere with a given radius R and I have to minimize, and at the same time, I have to minimize a function c of some of the slack variables. And it's, this is subject to the definition of R. So another definition five, sorry, definition of like 80 or so. Dave? Norm this corner. The distance between the center of the sphere. That's a vector that will be Wednesday nervous here. And the sample should be less than the radius squared plus t cap. So gn is the difference between the distance between the distance of the center of the sphere minus the radius of the sphere. And she should be positive for t negative. That means that the samples inside the sphere, so we drop its value. It is. The optimization is done by multiplying this times alpha times 0. Prefer that we subtract this sum of constrain constraints to the primal functionality is what we compute. The gradient with respect to, with respect to the primal variables. In this case, this panel variables are a, it's a vector that tells me where the ascendancy of Paris. And she writes an arc of Bush. And so we do this, we have this minimization, right? So actually, this is problem. We have to maximize this, maximize or minimize because it has side here. Maximize on tinted subject to these constraints. So the first thing is that the sum of alpha is equal to one. Then the center of the sphere. This vector is a function, a linear function of the thetas, a ligand community. And alpha goes between 0. And that is a version of this that inputs parameter mu, right? It's not that easy. Also, viewport to the original paper. It's not correct. But it was corrected later by the guy who I introduce them, LibSVM. Right? So you bought to the webpage of the LibSVM base Arabia explaining everything corrected by result. All right, and that's it. So we solve this optimization. And then we can find a radius R using this equation, right? For the samples that are on the market. Other, we know that the sample is on-device because the corresponding Lagrange multiplier is non-saturated, is less than one. So we take one sample for which alpha is less than one will put it here. But what we do it for all the samples on the margin or the radius of the sphere. And we average. And that's it. That is all the super vector machines. With this do optimizations. They are included in any package for support vector machines in, inside scikit-learn or five, bite orange or flip SBM, right? So again, here we are restricting our problem to one case in which the normal data is in one sphere. What happens when I have, for example, two spheres with nominal data? It might be the case that I have this one, this deviation. By that. I have many samples here. And I have many samples here. Exactly the same number of samples. The same number of samples. This, if this data is normal, this is normal to and maybe I have this. So I need more than one sphere, right? And so this problem cannot be solved using this strategy. All right, here, here, here, or here, but not too. What do I do? Again? I pass this data, I draw this eta into a higher dimensional space, where it is true that these data are enclosed in a single sphere. Again, this is passive. We will see next. Yes. Is it is common that when we have problems like Outward Mindset, I mentioned and they move closer. Solution. Yes. Yes. We will also see kernel PCA, that paper that I show you as a well-written paper. The kind of BCA. Bca as only optimal if the data is organized as a structure, as a Gaussian. Yet people use BCA for everything and they don't know whether sparse PCA works right? By the representation of the data using PCA only works if the data looks like a Gaussian. If not, forget, you might be destroying information. Right? What happens? We're going to visit and we will see it. We'll see very clearly is that if we pass the data into, are given here a space using a special type of kernel. Well, very common squared final kernel buffers are using a given transformation. Very common. If the result, the data looks like a Gaussian. And we will see very clearly in the examples, a bagel shop. So we can represent the data in that space using a few vectors. No matter which is very small, we will see it. So we can represent data, organize and several Gaussians, I think I put three Gaussians like that. If you do a BCA here, no way, there is no visible direction here. But then you throw this into Hilbert space. And these will look like a single Gaussian. And seeing abortion will have three. Prefer that actually here and here, right? But I will be able to represent everything which is one super vector. We started with one better eigenvector. Just one. Which means that all the data looks like a Gaussian along that dimension. And this may seem magic here. It's not, there's no magic. It's just algebra. But just a little layer. We'll see it in the future during the next crest. So again, this, so these two approaches are better restricted set, right? The first one, data is separable from the origin. Not going to in general. Or the data is telling us month sphere of radius bar. In designing habit, it might be that the shape of my distribution is not that sphere is something else. Or might be, it might be that they have several clusters of paper. But still these approaches work. If we, if we pass them, we tell them into our Hilbert space. And they way of doing that is using the concept of dual subspace. Subspace. So it turns out that I can throw the data, for example, this is an example into an infinite dimensional Hilbert space. And the transformation of the data, we represent it like that, phi of x. Okay? So we have a function when the data's of d dimensions in this function. And the output is a vector which infinite damaged. Of course I will have n data. So at the end of the day, I have at most n dimensions. Right? By my, my data lives in a space of infinite dimensions and it will span a subspace of an, exactly an image. So one is this function here, five. Well, I don't know, and I don't care. Actually, nobody knows. And we cannot see the space directions. We cannot said it's not explicit. We don't have this much. Whatever happens in a Hilbert space. It stays in the aerospace. And we're going to take a look hidden. But we know that given this transformation, it would compute the dot product between two of those backers. One possible bad product, or one of those transformations is this, this is exponential. So this exponential is our product. Why? Well, it satisfies all the properties of dot product. It has all the properties of a product. So I cannot prove it is not. All right. It is. And in particular, this is a positive definite function. That is a positive definite function. It's continuous version of what we call positive definite matrices. Right? So that again, so this is just a positive definite function. And when a function is like that, it is a dot product inner space. What space? I don't know. I don't know. But I know that in all cases. And I can use with this W a set of parameters of my estimator or whatever it is in that space with infinite dimensions is equal to the sum of alpha i times the data. And the data now is x by alpha alpha y alpha minus alpha star wherever a linear combination of the data. And so my estimator is y equal to w transpose Phi of x. So this is equal to w transpose, sorry, the sum of alpha i phi of x transpose, phi of x. And this, yes, well, possibly with a bias, and this is equal to the sum of alpha i times the dot product. So this transformation, I don't know what the inflammation it is. It's infinite-dimensional going to work with it. But the dot product is this exponential, the exponential of minus 1 over 2 Sigma square x minus x plus b. So b in that space, I have a linear machine. But from the point of view of the input data, this is non-linear. It's non-linear. I have an exponential. And that is the trick that said so we don't need to know anything else to start working with us for ships. So whatever we consider a dot product in the input space, we can change it by other product in any other space. We just need to prove that the function that they choose to provide my estimator with nonlinear Robert, this is a positive definite function, so it's about proud and that's it. Another suit. The other product is this x transpose XI, XI, XI transpose x j plus 1 to the power p. Any particular we will show this. We will, we will, we will see the gods of this in order to probe with any shadow comes out. And this is a dot product. In a higher-dimensional not infant. There are many others, many. And the problem is to choose the right one. Because some, some, some work better than others. And given problems. Again, the no free lunch theorem, this dot product will be phenomenal. For some problems. It will be terrible for others. But that's it. That is what we call the kernel trick. And this is what we're going to see from next class. That's Monday, Wednesday, Thursday, I'm here for questions. Thank you. 00 00 00 00 00 00 00 00. Hi.