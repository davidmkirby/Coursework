 Hello. Okay. Hi. All right. They say every year like six or seven types. For the first time in my life. Should I write? It's the results change the way in which you explain the comments on your results, the glycines, the way you explain how you did your experiment. This is the important thing. Monday code. This is not a computer science class. I don't know anything or computer science or anything about computers or coding. Of course I call it I started coding when I was 12. All right. Still. I'm not an expert enough. This is not the right class. Please know about that 1 second. I know my homework so is in GitHub. So you can just align. It works. I can tell when it's on GitHub because it has errors, right? I don't mind if you copy by. I want you to learn. That's it. If you just download and allows, run the code and upload, it's worth less. Alright, so just three or four because here we are here to learn I think your homework, your data classes, you get your advice. A learning machine learning. And so there was something else I wanted to say. But it's not either of paper listed six, right? Yes. Yes. Also, you can work in groups. Of course, you can buy Pali, say, this work is in vibration with this plasmid. So it's good that you're working groups, shared knowledge, Urine pastors just saved. And that will be reflected in a negative way in your, in your way. Now this set, Let's start with the next, the next module one day earlier. I welcome and more on homework, but I, I also want your feedback and I want to see homework number 4. And also there are several students that they don't have the grades in 3.2. Right? Because I didn't know how to grade. This works. I need to put comments for them to resubmit. Already said, let's go to reproducing kernel Hilbert spaces. So this is the world of machine learning where we have to pass from, not only from linear machines to non-linear ones. Right? And this is the point in which all the machine learning that we have learn become useful. So five, we just, we were restricted to linear machines. Now. We still, we will still work with linear machines, but in spaces of higher dimensionality, possibly infinite. And that is what adds non-linear properties to the machines. Let's first, before we dive into our K as hs preparation kernel spaces, Let's talk a little bit about what is the trick that we're going to use? And in particular, what is a subspace and why we should care about it. So our data x lives in one space of the dimensions. This is the set of observations, the input data to your, to your linear motion. And so they end estimator. Y is equal to w transpose x plus b, where b is our set of parameters, be w and b is what we call the primary parameters of my machine. And so by ability or the representer theorem that we will probably NMC unsolved, but not yet. The representer theorem says that under non-restrictive conditions, w is always a linear combination of the paper based training. Well, here at with alpha i by, you know, that for classification, we have our y here. For regression, we have two sets of Lagrange multipliers, alpha and alpha star. But the fact is the same. These holes through w is a linear combination of the training data. So here this, since this is true, then y is equal to the sum of alpha y X i transpose times X plus B. So my estimator is represented as a bunch of dot products only plus possibly buyers prompts. My estimator is a linear combination of dot products between the training data and the data and the past the sample under test. And here is where I see, or we should see a dual subspace. What is a dual space? For those that are not familiar with that? We can roughly speaking say that a dual space is the space of linear transformations of vectors or the input data. And we have a space de Dios ideal space as the space of linear combinations of the data and D. That's a, is vague and general by we can construct a particular dual subspace, which is a subspace of dot products between beta, right, and your subspace. Suitable Joseph space is space constructed with that products of the vape. So let's put a visual example and say in three dimensions, what we have here, 33 data, three samples. Let's put X1, X2, and X3. This wedge, their coordinates are these three vectors in a space of three dimensions. They are not collinear, so they span that space. Right? So we assume that these three vectors, they are linearly independent, so they span the space in three-dimensions. The trick here, the 0 subspace that I can buy stock is a space when the dot products between samples. So here I'm going to add that will be a duo. Or in this case is a dual space because we pass from three dimensions to three dimensions, all right? And we cannot have more than three dimensions. So in this case is just two aspects. So the notation here is the following. Here we have this notation is important to remember and to understate here. It's important for what it comes to layers. Here I call this the dot product between x1 and x2. All right, I explained later, here we have three-dimensions. These are three vectors, right? Here we have three-dimensions. They can anecdote I mentioned this, this is going to be 100010. It is right here. I express my coordinates in addition rules. So here we go. We have the dot product between x two and a dot. And here the dot product between x three. And I thought, what does this mean? Well, let's take sample X1. And we're going to get the dot product between x1 and x1. And it gives me this point. Somebody's coordinate here. Is this function evaluated our X1. So this point is X1, X1. Now, I gave you the product, the dot product between x1 and x2. So this coordinate here gives me this point. This function evaluated x1 gives me x. Let's put X2, X1. And the same with X3. This is X3. Thanks one. So this point is this function evaluated over x through basically this thing is a dot product between vector X3 and any other vector in the space. It is a function. Now, this gives me all the next paragraph. All right, So here I have a vector. And this vector is a vector that has three components. One component is the dot product with itself, with Excel and with X3. And we can do the same for other vectors. I call this vector k one or k of x. One of X1 is equal to the dot product that can be expressed in this more formal way or simply like that. Right? The dot product with k1 is a space of three-dimensions. I can do the same with the other vectors. And so I have represented vector X1, X2, and X3 space. It's straightforward to see that the properties in this space, they call here, right? So two different vectors next one, they lead to two different vectors in two. Space is separable. This is several. If this space is complete, the space is complete. And ABC space has coordinates. It needs coordinates in order to express it. Like that. This space has coordinates and row space is nothing but this. I put this example in 3-dimensional for you, is for you visually to understand what areas by, in general, is not a bad idea to do presentations, visual representations for us, or we cannot represent more than three dimensions. And second bids. Sometimes things are counter-intuitive, right? So this is just an example. But in this case you can extrapolate too many that we come up with these exact. In general, if we have, if we have vectors in a space of d dimensions, and we have, we have n factors and n is less than b. And here we have a space of Theta of n dimensions. Okay, let me do it again. Let's assume that we have d dimensions. We have a baby or band which is less than b. Hello. This is the number of dimensions on space, in this case is three. And let's assume that we have and bless that, read something too. Then when we do that your space, we obtain x and a subspace of two damages because we use two vectors, right? But if lead is higher, is less than n, then we end up with a space of the dimensions. So the maximum dimension of the dual space is the main non-dimensional is the number of elements. So when we have spaces of infinite dimensions and we have n vectors, might your subspace, it has N dimensions. Right? When my space has infinite dimensions, the dual space will have n parameters. We pass from infinite dimensions to the enzyme. And this is another important aspect because if my data is expressing an infinite dimensional Hilbert space, W has infinite dimensions. And while W and X exist, I cannot put them in our bureau. Right? So this is not practical. This is something that I can do, like I cannot implement because we're in three dimensions. But now we have n samples and data that we do this transformation, which is always correct. Then we must do something that has a structural complexity equal to n. All right? We have n dot products here. So we pass from being free parameters to n parameters. We pass from a space in which parameters have infinite dimensions to a space where parameters have dimensions. And so we saw the opponent. This is what we call the curse of dimensionality. Well, we need something else in order to understand what does this mean? What this means provide the curse of dimensionality happens when the dimension of the input space increases. And in order to, to solve it, what we do is to go into withdrawals, right? Questions about this. So let's go and see the first part, introduction to SpaceX. His green. So this kind of mysterious. But we will explain the, what we call the reproducing property. And that name will become evident. Just have to care about this title now. But is this spaces that we use? This is the way we call them and it's because of this so-called reproducing property of kernel functions that are the dot products that were beliefs. So as I said before, the algorithms that we presented in previous lessons, they have this starter. And they went, what they do is to discover, to learning a linear relationship between the input and the output and the sidebar. This is our learning, right? This is machine learning for us. What happens is that in practical situations, this normally, this linearity does also the relationship between input and output. They are non-linear. What we'd like to do is to keep using our linear machines. Keep using machines with their properties as we know, for example, super vector machines. And later we're going to use Gaussian processes or others. We want to keep this probabilities but nonlinear property. So what's present now? A model to do that. I mean, this, these are examples, for example, the relationship between the pressure of a crystal and involve back to you now that your crystals they have, they have this property of presenting both the chest open circuit voltages that we praise blood pressure by this saturate. So the pressure applied to a Christo produces about that, that saturates until the correct under the crystal structure breaks right? So this is pulling, this is non-linear, and this is the one I mentioned. There are many other, many other examples where they are, they are non-linear relationships between the physical observations than some margin, just that we want to the x power. In physics or in, in many other obligations. Team social network for example, or whatever you, uh, you name it, given application, I will tell you many non-linear relationships between observations and latent variables. This is an example that I really like because it's an example where we have a linear system. We have a linear system. But then even if everything is linear, we observe non-linear relationships. So things get awkward very easily. And machine learning. And also I'm a telecommunications engineer, so I like this kind of examples. That's the beginning of my career. I wanted to apply machine learning to communications, to signal process. Basically, people laughed a lot about me, but I finished my PhD on that. I'm still working. So let's hop about the communication channel, the easiest communication channel ever, which is something like that. Let me erase and then I will focus the camera today's whiteboard. So you see in this example. So imagine radio links and they are separated and number of miles about digitally this railings, they are separated 5030 miles or something like that. So the easiest possible communication model for this to digital communications, channel B is the following. We have a bath that is direct between being the transmitting antenna at the receiver. Then we have another path which is a reflection on the ground. And so here we have a given attenuation which will, which will be modeled by one. So we assume that there is no attenuation between this and this. This is something that we can do without loss of generality, just normalizing text. And that's all we have here. Another innovation that we call it a and a for us is a quantity which is less than one, because here we have an integration with respect to this one. And besides that innovation, we have a delay. So we assume that the signal goes from here to here instantly. And there is a delay Tao Between produced by this channel tau S. The difference, that delay difference between both. So we assume that there is no delay here without loss of generality. Again, this tau is just the bishops. And we have a set of a stream, a sequence of bits or symbols that are, that are transmitted from this transmitter. And they are pure as a symbol rate. And then in order to mix to make things the simplest possible. Let's assume as a particular example, academic example, of course, at t is equal to Tau. And that gives me a channel that we can model very easily. So that means that when I receive a symbol here and the same instant, I am receiving the previous symbol through this channel. This is never going to happen naturally. But I can adjust things. I can adjust this team. So it's equal to half. So it's possible that experimentally I do this by the nature. Nature, but in reality it doesn't happen under channels. They are, they are modeled like that, but with a higher number of parts. Typically will have 20 or 30 bytes in communications like cell phone models. This is just one. Alright? Now, if I have a signal, a symbol, and they call it y of p of n. At this point, this is a discrete version of the signal. The signal is continuous, but they sample it at every time, instant t and t. So this is the digital discrete-time signal I transmit signal y. And then we see X of N equal to the signal y of n plus a times y of n minus one. That is my channel. Right? This is equivalent to say that we have a linear system between these two with an impulse response which is equal to 2 there. H of n is the input response, which is delta at the origin plus a times delta n minus one. Alright? And then the output is the convolution between the input and the channel. Everything is linear. Convolution is a linear operation. This is a linear combination. So what do I do in order to recover y? In this particular case? Here? I can take the sample example, x of n, which is equal to y of n plus a y of n minus 1. And I can take sample x of n minus 1 multiplied times a. I change the sign. And what I have is minus a minus a y, n minus 1 minus a square y of n minus 2. And then I add them together. I add down x of n minus x. N minus one is equal to y of n minus k squared, y of n minus two. So I do on a linear operation that takes two consecutive observations, right? Remember that the observation is here, not there. I am here. I guess the observations, I do a linear equation with both. And what do I get? Y of n minus a squared, y of n minus 1. Why this is better than this for the past? Well, it doesn't matter because I didn't say it, but it's also important. I offend. They are symbols of independent and identically distributed communication. So this is true by coding. I don't know why base what is better than a. Since the since the distances are equal, like I will give you the entire distance from her from the source. Now, I want to recover an egg. Why? I want to recover like just one more example. A is smaller than one side. Alright? I can do that thing again, multiplying, taking a signal x n minus two multiplied times e squared, subtracting or adding in this case. And I will have the remote, and instead of that, I will have minus 3 to the third power. And that will decrease this interference a little bit more. If, if a is smaller than one. So this is not perfect. But it works. Now, I want to keep this communication system working. Even if I don't see the antenna. I have my phone here. I don't see any on. The phone works. Right? In this case, I might have a log here. And then if I want to normalize things to keep this, this path to half a generation one, then a will be higher than one. Right? So if I put a one here, a brick wall, whenever them here I have a high attenuation if I normalize numbers. So here we have a 1. A is going to be higher than whites as this attenuation is going to be smaller than this one. And this thing is not going to work anymore. Alright, exit heavy worse. Now, let us see these two situations from a graphical point of view. What we do as a first approach or our first element of our approaches to take two samples, right? To take two samples, x of n and the previous one, we store them, we use this to in order to extract y. And that is something that works not perfect. If we have a less than one. Of course, we have to ask noise. This in the first situation or the second situation with butter. This has noise because this antenna is a given parameter, we have thermal noise pattern. So doing this will decrease the interference, but it will increase the noise. So I have to be a little bit wiser in the choice of these biomarkers. It's not just a, it's something else. But what? All right, let's keep going. Alright, so we have these two examples, and these are two representations of 100 samples of these observations. This is okay, this is backwards. This is x of n and this is x. So when x of n in a situation where a is less than 1. So here, I lose. Forget about this. Alright? So here we have a situation where k is equal to 0.2. If we pass our linear classifiers through this. So the voltage and until then I will classify properly. Write a linear classifier is what we do here. Y. In this case, y of n is equal to W. W transpose times x of n, x of n minus one. And there's no bias here. V is equal to 0 and w is equal to the vector with parameters one minus a and no bias. So this is a line that goes through the origin. In this case, what we're doing in an intuitive way is what a learning machine would go to Boston classifier a linear one through the origin. Even if the parameter is chosen by the machine, they might not be exactly those ones. This is why people's linear. There is noise. Here we see, we see points spread because I added noise to the simulation. All right? And so depending on the quantity of noise, the lines that we aren't, they have to have different parameters so we minimize the pain. So here we can say that is the situation where y is equal to either 1 or minus 1. So here we have x, this is 15 plus 0.2. This is one plus magnesium went to business. If a is about 1.5. This is the situation. Right? You can do the I think it's it's it's done after by you then do they aren't around. And say that for y, y of n equal to 1, we have these four points and four y minus 1. We have this for other points. Right? We have, we have eight clusters. Because here we have y of n, y of n minus 1. In this, in this dilation, X contains X of n and then WIIFM and an ally of I minus 1. And this axis, which is actually x of n minus 1, condemns y of I minus one, y of n minus 2. So in total, we have three bits. And three bits. They describe a numbers, right? In general, here, we cannot see them three again, because a is very small, but here a is 1. And so, and so that, that non-linear approach is not going to work. So adding, doing this thing, if a is high and one is going to worst things, right? As you can see, this is a linear system, a channel which is linear, that is combined by a sequence of Beta. The classification problem here is not me. Yeah, so not, of course, this example can be linearly solve using the gallery. There is an optimal way to solve this problem. It's called the Viterbi algorithm. With Abby was a guy who invented this algorithm. He did an event pattern that he founded Qualcomm. He began super-rich. Thanks Larry, that we algorithm first and answer the toolbar coding later. We have phones that work. We have communications that actually work, the high-speed. Alright, so this was a practical revolution. A name in communications, Dr. Shannon, arbitrary and others. So this is just academic identity. Viterbi algorithm, by the way, is a natural non-linear. So how do we solve that? Using machine learning, we can put a line that goes like that. Rounding this blue and white, blue and red points. Right? So, but we cannot do it in this space where we only have linear approaches. So we have to apply a little, a little extra trick. And this is, well, I hope you understood the example. If not, we can go back and forth in order to understand all the details. Now, let's put a simple placebo solution that will lead us to spaces and higher dimensional. In a simple way, this is the simplest Hilbert space that we can reuse. So what do we do? Assume the problem that we have a non-linear problem to solve, but we can only apply linear approaches. We only know about linear approaches, nothing else. Solution oscillator through a nonlinear transformation into a space with higher dimensionality. So there's our classic approach, which is day, what is called a border around expansions, which has two months drop and non-linear transformation with products between components. We're not talking about dot products, nothing like that. Its products between the components, right? So let's see, we have three components, x of n and x of n minus 1. And let's construct their combinations of products between them. Whereas up to positive three, or just one. Or the one is, the components themselves are the two is x squared, the x minus 1, x of n minus one square. And this product and third order is this combinations, right? X to the third power management, social power and this tube. And this is a classic thing. And it's got a Volterra expansion. Now, how many elements do we have here? We have this constant. It's our, it's an element. Here we have 2. So that's 3, 4, 5, 6, 7, a, nine, and 10. By doing this expansion, we pass from a space that has two dimensions into a space that has ten dimensions. All right, so this is a suitable transformation into a higher dimensionality space. And we call this transformation, we vary phi of x of n. So find here is a vector that has all these components. In this particular case. Now, after we have this transformation, we construct a linear estimation like that. And here you can say that we can see that no matter what this works or not, they, Albert y, y hat is the estimator apply is a non-linear function of x. In my work, it might not work, but the nonlinearity is here. Still, we are working in a space of temporary measures and apply a linear estimator. And in this particular case, we don't apply any. If we don't apply any bias. So we can adjust the parameters. Here are W, we have 10 elements using a simple minimum mean square error approach. So now I introduce a little bit of notation here. This capital phi is a matrix that contains all the vectors expanded into the space. Right? So here we go. We have from phi one, phi of x, y to Firefox. And in cubs, businessman I might jumps. So phi is a matrix that has ten rows and n columns. So y is a vector that contains all the bits corresponding to each one of the signals. And remember that x is x and this x n is x n is a factor that contains two components, x of n, x of n minus one. And this is the simplest thing that we can do. And a widow count I mentioned here, this is that banks and banks and times have this major because that binds them and so on. We will easily see that this backward pass Taliban chimps. And so when we do this, We can adjust the wave, then we adjusted the parameters. And we can construct that. We can find now all my points for width w transpose x is equal to 0. Right? So in, in that space where w and phi live, this is a hyperplane that separates the positive samples where the negative of the negative cycle. But from the point of view of the input, when we others, That's, these are the points that one is transformed using fine and multiply it times w, transpose their answer and the output is 0. Right? So I hope you understand the example. And this. Yes, sorry, I just silicosis. So even here we have outliers. Here, we have our layers. Well in this case we don't have outliers. Well maybe this one. And this is suppose 0. Yes, we might have outliers, but it's not that we see outliers here, is that the solution is for, It's a solution that I chose because you can see the mechanics in the primal space and W. We can stop it in practice because we have 10 dimensions. So it's easy for a computer. But the solution, well, here and here, it's kind of poor. It solves the problem. And Ottawa, We can do better. I, I expected something like that, right? But this is a solution provided by is third order expansion. This is some sort of a third order function. Functions are gear. If we look at the function here, we have a convex and concave part. And then this RNA process on this puncture vine. It really is. But the boy is to understand how it works and what is the interpretation in terms of higher dimensionality spaces? This is something that is Bible. There are words of number of funds. So I answer a question and each other before and breathing treatments. So let's talk about the curse of dimensionality. What is this thing? Well, so here we pass from two dimensions to 10 dimensions. If we know about them or expansions about the re-expansion using an input dimension of two or three. Then there, there is a formula that tells me what is the output space, the number of components of the water extraction in this case, is that, right? Sorry, we have instead of r equal to 2, we have r equal to 10, I'm sorry, b mod n and p equal to five. Then this explodes. For example, here we have a space of two dimensions and on and 5, we know an expansion afforded 5, then the number of dimensions in the output space is 56. You can keep going if instead of having an input of one, I have two dimensions, you have an input of ten. I mentioned, funny. Then this becomes unbearable. All right, so this is the curse of dimensionality. If only one non-linear things in the primal space, we have to deal with very high dimensionalities and the computational burden increases. And so this is what we call precursors that channel. So what do we do? Here? We use dual spaces. Let me say it will read the outcomes later, but I want to present the next one. Which is we will talk about that in more detail. But let me start. So this is the birth of dimensionality. And we have to get rid of it. This is the language that machine learning people use. I like to define a method where we work with expressions of the input space, but still doing the same array. I want this both an expansion, I want it, but I don't want to go explicitly computing all the elements of this transformation because of this course of dimensionality. So I want something which is equivalent that works only with the input data. Right side, we have two To fortunate facts. First, in my algorithm fits the conditions are represented by dual expression can be constructed as a function of dot products between them. Right? So when we go to the dual space, and then my expression contains only dot product between peak and sovereign. Functions that are by-products in higher-dimensional space exists. So what's the trick here? What is the kernel trick? I bought the dance space and they consented. And I find the expression for the dot-product inside. And then I construct a function that instead of having dot products between W phi of x, I find dot-product between every two samples. And this dot products, they are, they can be expressed as a function of the input data only without explicitly using the dimensional date. They've elements of the debate of the Hebrew space. Let's say, let's, Let's first talk about these represent, that represent our theorem. There are two versions of the representer theorem. And actually this one is not the original by PMMA lower than what we're about. But this is the version that was presented by Verna chunk of 2000 around choke up in 2001, I believe, 2004 it, so it's, this is actually the generalized representer theorem. Very easy to understand. And that is the proof, but let's just try to understand it. For money. We often representations phi of x of n, which is a non-linear transformation of a sample x into higher dimensionality. Hebrew space age. We don't know what a Hilbert space is yet, right? But for us now, it's a, it's a vector space. That's it. And so this Hilbert space, it is endowed by a dot product that can be expelled. I need a pentaquark here equal to that. It can be. I don't know why metabolites. So this dot product can be represented as a function of the input data. We don't know how we do this yet, but let's assume that this is true. They dot product in this space is a function of the input. And so we have this space with this dot product and we have a monotonic increasing function. Bombing on a strictly monotonic increasing function. And then the third element is an arbitrary loss function. Right? Our last function is convex function of another measure or the similarity measure. For example, the square error or their bargaining loss function that we present that for support vector machines, right? Any convex function of the dissimilarity between the estimation. And beside that, we have these three elements. And now we construct an estimator. And this estimator, it has a set of parameters, and these parameters are represented by F. So they, we call it f because this set of parameters, it might be something which is not a vector in a, in a d-dimensional space, in my b vector in an infinite-dimensional Hilbert space. So we represent it as a function. For example, it can be a function between 01, it has impeded points if any damage. So f star is the function that minimizes. The cost function for the last function applied to all the training data, right? This is their laws applied to all the training data. Plus a monotonic increasing function of the NOR of this parameters. If my optimization criterion is like dance. So we have, we minimize our loss function plus an increasing function of the parameters. Then we can say that the parameters are a linear combination of dot products between training Sounds. Right? So this is a way to say that f is a linear combination of the training, the amount of the training that, that Hilbert space, a linear combination of the training data. And so if that's covered. So remember that I mentioned in my drawer space represented by this coordinate x i comma above. All right, this is equal to I because it's asymmetric. And this is what I call here p of x. So is if f, my estimator is represented by this dot product in the space that I could write as a function of x. That means that when I apply my estimators to, whenever I just make my submitted to a test sample, then f star of x equal to this. And this is equivalent to say this dot product between x and x i in that space. That's it. And another way to write this more familiar is the followings. Alpha is a parameter and this dot-product, we're going to express it as one vector transpose, transpose times the other. So this product or this function, they are nothing but space. The dot product of two samples, X i for training and x, which is a test sample dot product of these two samples in the Bureau space. But here we can have infinite dimensions. Here we have infinite dimensions. Here. We have a function. Here. We have a function that passes from the input data, the input samples to r. So we don't go from the input samples to infinite dimension stand for. We go from the input samples to a number which has this form. So these expressions is, this is a practical expression that we're going to use. My function is a bunch of dot products between data in a Hilbert space. This is always two. If the algorithm that they use are the criteria that they use to optimize my data. It has these fields too. All right, we have an arbitrary loss function that we minimize at the same time we're minimizing an increasing function of the normal pipes. We're doing this all the time. Right? Let's see an example. Before we go back to the, we go back to the further expansion. That is one of the four elements that we have, the muscular, the representer theorem by there are two that will expense. But let's go to see this example. Whenever we have a set of Betamax, why training data. And then we want to stop an estimator using minimum mean square error in a Hilbert space H. So we have to exit that leaves enough space on the dimensions. Through a nonlinear transformation. We have Phi of x lives in age. So we do this transformation. And so then we have matrix phi, which is equal to phi, phi one to five and the set of training samples in a Hilbert space. All right, and then, and then my w is equal to five. Well that's what creates regression. Phi, phi transpose plus I minus 151. Video. Remember the solution that we apply, the linear solution for a regression. It's this, but instead of, instead of five, we put x. Now we don't have that file. Hello. We know that this is the algorithm that comes from an optimization that fits this definition. Here. We have on every tiny loss function, the error square plus we have here the, the norm of the parameters square. Divide sign up, right? So my, my criterion is minimize the norm of w squared Gamma plus the sum of errors. Right? So this is what we have here, written in a different way. This is the arbitrary loss function and this is the increasing function of w squared. And so we saw this, we get this solution instead of x, we put five. But now I know two things. First, this is an infinite dimensional space, so this is an IA, W is in two dimensions this synchrony, some infinite-dimensional matrix. Annoy. So that's one thing that we know. The second thing that we know is that they, they, they generalize. Representer theorem works. So I can say that W is equal to a linear combination of the data, which I can write as phi one phi. These are made with this. All right, so these are matrices. Vector. This is a good analysis and ambition. Start me right there, the past exposure. So the president gets the same at the same. And these are metals. So we have these two true. So let's find Alpha. Alpha is a high alpha equal to five by transpose plus gamma I minus 1 phi odd y. And so this here I have phi, phi transpose phi and alpha plus gamma. They identity. They can just ignore. Find alpha equal to phi y. And then I can simplify this expression by taking out five. This five here, this one here because this is a scalar and this one here. So phi transpose phi alpha plus gamma. Let's, let's now put identity here. Now the same identity because this identity, it has infinite dimensional because it's simply bands, right? And so I just fit in my palette here. But I know I'm finding another Pi. Since alpha has n elements, this is timespan, it's a different defend. This is an identity of meeting the dimensions. This is an identity of my main branches as this is being welded wire. And then since this is n times n, This is n times n. I can take common factor phi transpose fine plus Grandma, why I times alpha equal to one? And finally, we have that Alpha is equal to phi transpose phi plus gamma i y minus 1 y. And what is the big difference here? This has a matrix which is what is this? This contents here. All the matters transposed and here all day because we love transposition. So this is a matrix whose entries aij is the dot product, better I j. So this is what we call k. And this is what we call matrix K. So we have Alpha Phi transpose, find us Gamal, I'm white. And so this is a matrix. A matrix is a matrix. So this matrix here, phi transpose phi, is what we call the matrix, the kernel matrix. Kernel matrix. Why? Because they entry, entry aij are these matrix is equal to the dot product between Phi of X and Phi of x j. And this is what we call the kernel between x i equivalent function we're going to excite. So we must problem that we're going to solve. The problem, which is all. So finally, alpha is equal to 0 a k plus grandma I invert that one. And using the expressions that aim to be as before, my estimation, y of x i. Let's call it F now. A function of x I, which is my estimation when I is equal to the sum of alpha i K of x sine x. So x is the test. That's it. To test my sample, I have to compute all the products, kernel of a product between their training samples and the sample. I multiply this vector by by bank or alpha, or I multiply each one of these bonds by, by the corresponding value of Alpha, and that's it. And our thought is this. So this is my reproducing kernel, Hilbert space or non-linear version of the ridge regression. This is called, this is an algorithm, this is out. This is a thing and it's called Kernel Ridge regression is just a favorite. Epsilon would be just constant. Alpha will be a constant. A constant, yes. Of course, yes. It's now it's a constant because we're salt. I introduce a kernel version of the autoregressive moving average. Moving average filters. With the sole purpose of Califano of calling a problem. All right, south and I managed to publish the paper. Anyway. So there are many algorithms that start with a K. And then Cornell versions of everything except the supervisor machines. We don't use linear support vector machines. We use pronounceable reductions and kernel Gaussian process. But other things, linear filters are linear approaches that have a kernel version. We put a K, for example, kernel, principal component analysis, Cornell and ridge regression, kernel of independent component analysis, DKA, corner, corner, wherever. Everything. Right when I learned this, when I learned, as I say well as above, with linear algorithms, let's construct nonlinear versions of them for free, for fun. And then my neighbors. And he was fine. Yes. And so we solve things that are non-linear using our linear elaborates. In particular. We are going to use that with super vector machines for classification and regression. And the thing is very straightforward. Now, we're going to finish here by four the next class, we have to revisit that water a problem. Because here I put an example of the application. Are there represent the theorem. So how do we easily construct a non-linear algorithm from a linear one? That's easy. We always do the same. And by what is K, right? And what is the, what is k relationship to our previous problem we have about camera expansion. What is the dot product in this space? And how this dot product is a space contains implicitly, they will vary expansion implicitly. So what I'm doing with the dot-product is exactly equivalent to the water extraction. And send the slides and we will go through it. We will go through it in detail. But as you can see, there is a lot of math test boolean algebra. So let's stop here and I'll see you next. Time.