 All right. Good morning. So let's say you said lumper what the percentage of time? Yeah. Well, with other references, sometimes they died or things like that. I get percentages of about 30 percent. So that doesn't mean that it's it's plagiarism. It means that the titles of their preferences, for example, or where it's similar or equal to others. That that by what your personal page? I haven't said Yeah. 62. 62. Well, we will in this case, when this happens, what they do is to look for the coincidences and see what's going on, what habits. Sometimes opened last year I got a 100% and then I took a lot to it. And it turned out that the students have submitted the same homework in two different semesters. Right? So this, I don't know what is going on with your work, but we will take a log and then basically I will recommend refresh things. For example, when you submit a paper, you upload it. I got rejected recent papers because I uploaded them to archive. Elsevier recommends doing this. So they recommend that you submit your pre-print Elsevier. But then I had 100% and I was two papers rejected. And when I said While age your policy, they say, well, we contrast, this is period. So we go in, they re careful when we submit a paper because editors and publishers, they were the same. They use, turn it in or use, I don't know what software they use, but this is something that we should avoid. 60% is high enough to present formula and we'll say welfare. I think it's pulled. Anyway. So I know that this has an average western arrived, but we will save. All right, so here we're spaces. So let's see if we remember that. We were, we were looking at this problem here. This problem here, right? Where we had basically a union channel that we saw. Data is normally products consisted out, consisted of transforming our data into a higher dimensional cube or a space into a high dimensional vector space at this 0.1. Way to do that is to use, are all direct fashion, right? And this does, is to compute products of the different features of your vector. We have a vector of two components, which in this case our x of n and x of n minus 1. And we compute combinations of products up to third order. And with this, we have 10 dimensions, including one where we have a cost. So then we use this, we put all these components in a new vector of components and we apply, we apply the squares amine women's wear approach. So W is, it has the same solution as we know. But instead of having x here and we have find when phi is a matrix containing all the names of the transformed vector, Phi of x is a vector containing all these combinations. So this is an easy example of a transformation into a GIS-based spatial find dimensionality. And in many cases, the transformation looks like that. And actually if you take any kernel. Any dot-product, any positive definite function. And we compute our caldera expansion while Taylor series expansion, for example, right? When we get is a polynomial. And so with a more than one dimension, it's not about, it's not called a Taylor series expansion, but whatever expansion. So this is resolution. They blind points that we see here. They are points that satisfy that equation. W transpose phi of x equal to 0 sub in that space with 10 dimensions, we have a hyperplane. And the expression of the hyperplane is this. W dot x, Phi of x is equal to 0. And they, and some of the points of this hyperplane, they have anti image, which are these black points. In other words, these points, these points here. If you transform them using this transformation, they fall. They go into a hyperplane. And designer plant separates this data once it's transforming the same subspace. Right? So five bags, sometimes we call it the image, is an image of X in this higher dimensional space. And then these points, they are the untick image right above of these hybrid plants of this. This is the image of a given hyperplane, that space. Right? Sometimes it's not intuitive, but as you can see, this is the equation of a plane that crosses the origin because there's no bias. So what happens? What is the problem here? The problem is that that we have 10 dimensions for an input of two dimensions. If we put, if we use an expansion with border 5, then we will need 56 elements and so on. This is what we call the curse of dimensionality. If we want a solution that works better than the one that you saw, which is not very efficient. You need to add more capacity, expressive capacity to your machine by using a space with more dimensions. In other words, what do you want to do if you want a solution that has more expressive capacity and you have to increase that by Michel running dimension on that space. And this space. If I may terminate, the dimension is 11 is equal to the dimension of the space plus one. And there's a space delimiter rank, as I mentioned, will be 57, right? While the input space, it has aids in one to three because as I mentioned, the following. So this is a way to increase the expressive capacity, what they call complexity of the machine. This is a problem. Dealing with 56 elements in it, translates into an increased computational further. And this for an expressive capacity which is not too hard. What do we do? The following. We use the representer theorem and we find functions that are dot products in this space. Let's start with that. While the representer theorem, we saw it. And let's first take a look. This Volterra expansion and this estimator. The minimum mean square solution is this one, right? Easy to prove. And then what happens is that using that representer theorem, we know that w, w the set of parameters of my estimator can be expressed as a linear combination of the data. This is the matrix containing all the vectors transformed as columns. And this is a vector of parameters, column vector. So if we put this equation together with the previous one, this one, then we obtain this equality and by isolating alpha, we have this alternative expression. So we call phi transpose phi we call it K, because it's a matrix that contains all the dot products between the data that we see here. So we pass from a problem that has a matrix that is, it has dimensions equal to the number of dimensions of space, ten or 56. We use a matrix that has dimensions N times N, where N is the number of data. Okay? And then with alpha, we use this expression. Then, see if it's here. Yes, subsets. W is equal to phi alpha. This estimator here can be expressed like this. We change W by this expression. And what we have here is a set, the set of parameters n parameters Alpha times this thing here, which is the dot product between all the training data and the tests up in that space. Right? And so in scanning validation, we can write it like that. And now the kernel trick comes since here we only need a bunch of dot products. Let's find out our product that we can easily compute without previously computing the transformation into the hero space. Right? So also we talked about dual spaces. By doing this dot product, we are passing fine. That has a number of dimensions into a dual space that has dimensions because here we have n samples, right? So this matrix vector product produces a new vector with n dimensions. And this vector of and I mentioned is the representation of phi in the dual space. Time. For questions. This is pretty advanced. So he has gone in this fight, was this fine? Yeah, I was 18. And then vector, this fine, it has as many dimensions as they are in the Hilbert space. In my example, it has ten dimensions. You said that the key is n by m matrix K is yes. Let's clarify this. So fine. Is a matrix that contains all the data fi of x1 to phi of x. And so since 5 is a vector of ten dimensions, right? Dimensions to General, and we have n columns. The dimensions of this are the times right? Now in they primal notation here. Sorry, Yeah. Yes, but here we have this matrix phi, phi transpose. Phi. Phi transpose is a matrix that we call I be, well, 1 over n times five sd about why the autocorrelation matrix. And it has d times n times n times d. So this has dependency line matches. But now we have Cave, which is why transpose Phi. And so this matrix is as n times D because it's transpose fits the transpose of this matrix. But folks, and here we have b times n. So here we have always, and that's okay. We can think of this matrix as a, a matrix whose entry i, j is the dot product between vector right here and vector j here. Right? By this matrix K. We can think of it in a different way. So we take each one of these matters and we compute the dot product of each one of these vectors times all these vectors. And that the product produces a column, column j with volumes. That is the projection of vector phi of x i. Into the dual space with n dimensions. Right? Remember that if we have a space where we have three vectors, in this case, that will be phi of x 1, phi of x two, and phi of things, three vectors. We can construct another space or your space when the coordinates are this. So what does this mean? Well, it means that in order to represent vector phi of x one, we compute the dot product of x1 when x1 and we write, we have this scalar, x1 times x2, that scalar and X1, X3, this one. So this will be, for example, this is a product between Phi of X1, X3, and Phi of x one. And so every enough, I'm better with this three coordinates. And that is when they call K of X1, what escape of x? What is our represent but dual representation of this. So this goes to here. So different spaces that your space, but it gives same properties. It gives the progress of the tradeoff space. And it is the column. The first column of this matrix. Yes. A question. I heard the question. Now. All right. I heard a voice, right. So that's it. Okay. Now, no other is the dimensional. Notice the dimension of, of, of phi of x. And we end up with a matrix with n dimensions, where n is the number of samples. And so here, my remainder is always right. Nation a linear combination of n dot products, where n is an amino data, training data Annex M is any sample, for example, a test, test. But we haven't finished yet because in this expression we see explicitly these two vectors. How do we solve this? Well, the next step consists of finding about product in the higher-dimensional space that can be expressed as a function of the input space. And this is the example of the three are the three data bits expansions. So we ever see now that for this Hilbert space and I constructed a dot product, at least for this is a product in the Hilbert space that I construct. Any can be expressed as a function of the input space, as a function of the vectors in the input space. So here we have five Sci-Fi anymore. And so we pass from the input space to a dot count. It, we have a function like that. We just plug it in the previous, in the previous estimator. And what we get is this. For this particular case, y of x m is equal to the sum from n equals 1, n of x and transpose x m plus 1 has her hands up. Yes. Anymore? This this is this on this is an expression for the other three. Well, they match. Yes. So it's not about four or five? No, this doesn't work for IFR, but if we put a four, then that's it. This is an easy example that we can see in order to get you some intuition about what's going on, what happens. Alright, so let's prove this. Let's prove that. Let's prove that indeed that dot-product corresponds to the decimal expansion. So let's see. We have two vectors, x and x prime. Let's, let's forget about this. One. Here. I remember this, change it. Let's use x and x prime. This x one is the one before it. So we have these two vectors, one with combining embryonic still and the other with components x1 prime, x2, y2 prime. And let's compute this dot-product. So inside, inside this parenthesis, we have the dot-product in the input space, which is equal to the sum of the company, the element-wise product of the components. So X1, y1 prime plus x to x prime, right? This is the dot product between these two in the input space for this particular case. And then when we have a one here, now we have to expand this expression. And after a lot of Boolean algebra, we get all of these components here. Third power here, right? This is third power, this is two. This is two to the second, second order, sorry, and first Bonner. I need you do it. It's kind of boring and long, but you end up with this expression, right? So now we can take all the elements of x in one side and all the elements of X prime and the other side. So the element that we have here for x one as x one to the third power. Here we have x to the third power. Here we have X1, X2 times the square root of three. And so on. Here we have three x, we have x1 times x2 squared times the square root of three, and so on. And exactly the same for the second component. So we multiply this times this. We have. So we decompose this expansion as the dot product between two vectors. And these two vectors, they have the third power, the third power, the third order combinations, secondary combinations, and then first or the combinations plus the cost. And so you do it either this dot product, then you have, you have this. So for this equation here, we prove that this equation is equivalent to the dot product between these two vectors. There is a difference between my previous expansion and this one, which is just a bunch of rescaling factors, right? This is nothing but, nothing but constants, right? The square root of two and it's part of the tree. So apart from the constants, they are this expansion and the previous one, then they are the same. Right? That's, that's it basically. So this done, this is a dot product, as you can see here. And it's the product Volterra expansion, other two vectors up to some constants. So B, we need to put a higher dimension on here that are kinda mentioned here. For example, five dimensions from Acts for and we need to I a higher exponent here, high power. Then we will have a lot more products. But in any case, in general, it can be proven that this is handled product key in a higher dimensional Hilbert space. How do we prove it? Well, through another theorem the Mercer's theorem says that we will revisit. It. Says that any positive definite function is a dot product in some Hilbert space. But we will see it again. So that's it. Alright. This is a particular example of what is known by the rather all kernel trick. Somebody wrote it in a paper. And people like that. So we call it the kernel trick, or they're rather about kernel trick because it's very old. It's order that the Queen of England actually, right? So this is from the prints. So for the beginning of 19, yeah. I thought this was harmless. By the end of the 19th century. So far we have seen tools that were published and there where they are, not same time ago, right? The day. Nothing of this as knew, nothing of this as recent bottleneck published by nature When I guess Paris, the theories in 1975. And they come from theories that were published at the beginning of the 20th century. And this is even older, All right? But these guys just put everything together. They put everything together to construct support vector machines and to expand them to non-linear cases. So this is that we have seen an example of a symbol problem that cannot be solved using a linear classifier. But we can solve it using a non-linear approach. And non-linear estimation can be constructed by a nonlinear transformation to a space of higher dimension. And we do this because what, the only thing that we know is to deal with linear approaches, linear machines. So let's for the data into a high dimensional space. And weren't there linearly? And then nonlinearity, yes, added through this nonlinear transformation. So there is a non-linear relationship between the input space and the output space, but bare end up with space, which is, which is use linear machines. So when people say support vector machines and non-linear machines, not true, super vector machines are always linear. Always linear, maybe linearly neighborhoods basically in the Hilbert space, but nothing else. Alright? And actually all the tricks, all the machines that we will see in this class, they are linear but in Hilbert spaces. So doing this transform, transforming to a space of five-day mentioned suffers forum where we got the curse of dimensionality. And we always end up with spaces that hi, I have a much higher dimensionality and as I mentioned, ID might be infinite. And in this case, the curse of dimensionality is unbearable. Sorry, we have a function that transforms into an infinite-dimensional space. We cannot work using machines. But in order to, in order to fix this first, then we use that represent the theorem in one side. And we find that products that can be expressed in the input space. So we don't see the human face. We see we see a representation in a few hours. And in the US space, we always have n dimensions, where n is the number of paid. So that boundary is dimension of my machine is endless one. What does this mean? If, if I end up in a space with n dimensions, when I is the number of data, what happens is that my machine is 192 feet. If I dump unfolding complexity fit on one fault, if I'm the Chairman, I guess I mentioned, since I have n data, my name is dimensionless m plus 1. Then my machine will be always able to classify or regress with 0 errors on the training data. And this is not, this is something that we learn that is not good because it will not work properly in new samples. Not previously seen minded frame. In other words, test samples. Okay, So here is where I'm super vector machines and they can follow that Babbage them, I guess I mentioned, makes total sense because previously we had 100 samples and dimension two. Alright, in your homework, we have 10 dimensions. I by very large number of samples. We can see how they buy Michelin. I guess that works. But it's not a big deal. Here. It's a big deal because we always, we end up in spaces. Proust's law be Salonica, as I mentioned, is higher than the number of it as exactly about the denominator plus 1. So now let's see how we, how we construct machines T2. Spaces and how we deal with they control the complexity here. So a little bit of furious spaces. This is something this is something that I think it's good to know. All right? Good to know a little bit of what is a Hilbert space, right? And why we call them reproducing kernel space. So first, let's revisit the concept of vector space. A vector space X over the reals. A vector space constructed using real numbers is what we call an inner product space. For a dot-product space, if we can find a function, that's a function that is a bilinear map from the space into bar, right? The dot product that we know is x one transpose times x2 gets bilinear, right? So we take two elements of this space. We do the operation and the result is a number. We can construct many of these functions. But only a few are. Actually only one function is at a Hilbert space and paper space has only one path. And so this bilinear map satisfies that x times x is positive. And this map is what we call a dot product. This month is strict. In equality. X times x equal to 0 is true if and only if it easier. But we don't want striped. We can protect our products. Are weights. X times x is the dot product between x and x itself is 0, where x is non-zero. But we are not interested in this kind of products wherein we are mainly interested in columns that aren't true, for which the equality doesn't exist except for the orange. So we are interested in this kind of straight dot product because if the product is strength, then we can construct a norm. Norm. We, we define it like that. And it's equal to the square root of the dot product, the forbid up with a better way with itself. That is one thing that we got was such a norm. And the second thing that we can but strand, and this is equally important, it's a metric. What we want to do is to compute distances between vectors around. So here's a product, this is straight. Then this quantity is different from 0. Unless these two vectors are equal. Right? Sub x minus x0 is the segment between vector x and z are the distance factor between x and c. If we want to construct the metric or the distance between these two vectors, we compute the norm of this distance back. So this works only if the product existed. So what is a Hilbert space? Well, for us, Hilbert space is a space that contains a strict inner product. And for weights, the space is separable and complete. Separable and complete. And that's why this apparel with this complete, well, completeness is finite that every Cauchy sequence converges to an element h eNB space. I call it f here. Sometimes ambiguous spaces, they had all A's in one or two Hilbert. Sometimes they will call F. Because in general, business spaces, our function spaces. You can imagine that as when you have a1 plus space with infinite dimensions, a vector in this space is nothing but the function, a function, a continuous function. That has infinite points. Right? So this spaces in general, a function, spaces, spaces of functions. So what does this mean? We have to, if we have a sequence of as a sequence of vectors H n and H, a sequence of vectors, HM. The difference between these two vectors. When n tends to infinity. The limit of the superior of this, when m is higher than that. And that's the infinity is 0. This is completed. How do we have windows for us? And for our purposes, how will this? Well, if this is true for any vector, we get high again, even better, anything, then we can find vectors that are as close as this vector as we want. And some of our spaces, they are not complete. For example, if, well, it's very easy. If you haven't started that space where only natural numbers space, it's not a bit. And in particular, if you can track a space where the coordinates, they can be even only 0 or one, right? You have a sequence of binary numbers, or 0 or one. These are vectors in space, but the space is not complete because there is always a distance different from 0 between two vectors. You cannot, if you have this vector, 1, 0, 0 in a specialty dimensions, you're going to find another better, different from this. With a distance as small as you get. There is always the distance between two vectors. Alright? And so this space is not good. We want to spaces that are incomplete and separable. So separability means that a countable set of elements, right? Edge loops from a time and 12 base. And a countable set of elements exist. N elements exist. So that for every epsilon, we have that the distance between one vector and a vector h is less than epsilon. So you can separate elements. Right? This is very important. Why? Well, this Hilbert river spaces, they can be assimilated to L2 spaces. And other spaces is a space of all sequences of real numbers such that the sum of elements is less than infinity. And the dot product is this. Thank you. So for us, my Hebrew spaces, they have coordinates. These coordinates may be infinite. So x is a sequence of numbers. It might be a sequence of infinite numbers, right? But they dot-product between two of these sequences is less than infinity. And Dave, and in particular, the norm of any vector in the subspace is less than infinity. Right? So you cannot find a vector in that space with a norm that tends to infinity. And this is very important because we don't want to follow a vector in space. And this vector that in the inward sped it has finite norm. And then we throw it in a space and then the normative influence. All right, so then if you take all these elements here and you solve them, then you always, always find that the sequence, if used for them from the maximum, they sequence with a maximum value of this events with the minimum value, you always find that the serpent suppose like that. And you can, instead of solving the sequence, you can set the start date the second way, right? The second is what? So this area here is always less than infinity. So for example, our vector x that contains only ones is not a vector in that space. Because if the sequence contains only once and it has infinite elements, then the neuron tends to infinity. Well, this element is not part of the hero steps. Right? This is very important because we don't want to compute the norm of a vector. And then I find out that it is in fact, just doesn't happen. And this doesn't happen by virtue of the way that we construct. So my, my dot product between two vectors can be always express lifetime. We have ordinates in that space. It might be infinite limits, but we have audience. It turns out that this dot product is bounded, is less than basic. Right? So that is my heel respects. Now let's compute dot products. This is a dot product. And I said before, well, but we can only find one that space. And it's true partnership. But let's take art and I space or reals with n dimensions or dot product this, this. So we compute Dame. We compute the element-wise product of elements between these two vectors. We multiply each one of the products times London. And this is, this might be a dot product, right? So it might be other product if and only if this is always non-negative or positive. 4, when x and z, they are the same. Okay? And that implies, that implies that all elements in London, this, of course, lambda is a diagonal matrix. I didn't say that. You understand that, right? It's a diagonal matrix. Are the elements London, they are positive. If some of the elements are 0. If one element is 0, then at least they will find a vector for which the norm, the dot product of x with itself is 0. So give one element lambda I is 0. The dot rhythm is not strict. And if one value of lambda is negative, then we don't have for the product because we will find at least one better for which then the product of x with itself is merited. Say we want this to be a strict product, then all values of alpha I must be positive. But for this space will really have one dot products, right? We are really hot product. And I said, we can only find one product in our hero space through. So whether we have your X transpose, so the dot product, the dot product of x, z and x and z is equal to X transpose Lambda x z. Sorry. These are vectors, this matrix, diagonal matrix. And this is equal to X transpose lambda one, lambda 1.5 z. So this is equal to x prime transpose, z prime transpose, where z prime, for example, is equal to lambda 1.5 z. So what we do is to apply the original product that we have, this one, this. But prior to the dot product, we transform samples. We transform the data using this transformation. And so what does, what this implies is a transformation of, since this, that matrix is, is a square, this is a transformation of space to itself. So they start this product, product prior to elaborate, it performs the transformation. But the data is between these two vectors like that. Right? We can put other matrices here, but we can always do this. Or we can always do transformation of the matrix. So it's the product of two matrices. For example, using the transformation buying Cholesky, Cholesky transformation, I'm sure transformation. We can do an eigen, an egg and analysis, and we can transform this matrix into two matrices that are equal and then we compute them, compute this product, right? So in any case, in any case what we end up is with this product. There's only works in this particular case. If lambda is a diagonal, I meant being a diagonal matrix where all the elements lambda i, they are strictly positive. If one or more or 0, the dot product is an umbrella, but it's not step. If at least one element is negative, then this is not a bug anymore because it doesn't have the properties are applied. By the way, if one element is negative, then you don't have a single solution for runaway. All right, Do you have a pair of complex numbers? What's jumps? So that is a dot product. All right? Let's assume that my Hilbert space is a space of square integrable functions. Right? So we take all possible square integral functions on a convex subset of X. We will, we will see what does this. So we can, we can construct two vectors in that space, that arm to function. And then the dot product is going to be this. Instead of having a sum of elements are in script, instead of having a sum of elements. Here we have an integral elements, right? But it is about product. So if f and g are the same, this quantity is less than infinity and it's always positive. So this is my path. And so here, here the concept is a little diff, right? Concept is we take a subset of X. For example. For example, x between 01, all the elements between 0 and 1 of x. And the vector as part of my space are all possible functions here. That our score interval that's only have one X and our possible functions. So this is our block and dysfunctional space. Function space. If we define functions in C Instead of our, right, we can, we can define functions in C, but we can define x in c. An equivalent expression exists for that. The only difference would be that one of the elements, one of the vectors is conjugated. We have to conjugate one element in order for this to be a dot product, right? It's, it's very important. We can use real, we can use complex numbers. We can use quaternions and we have to respect big, big conjugate in order to, in order to obtain a suitable product. More things in an inner product space. This is true. All right, they done a dot product between two elements. Squared is always less or equal than they know. The product of a square norms of each one of the vectors. This is the spine Cauchy Schwarz inequality. And we define the angle between two vectors. Like that. It doesn't matter if the vectors are defined in a finite dimensional space or an infinite-dimensional space. If we have two vectors, we are in a plane with today much. So without loss of generality, I can say that this is a vector and this is another vector defined in an infinite dimensional space. Right? By these two vectors. A final Blaine, two dimensions. Hence, there is always an angle theta that I can measure. By definition of paper, sine of Phi cosine Phi is equal to the dot product between vectors over the norm of nodes. Okay, we will use this, so don't forget about it. We will use this. Amazement. Where else? Now? What is a positive semi-definite matrix? This is something that we have to bear in mind too for several reasons, but in particular for our, for our machines to be able to work. So what is positive semidefinite matrices? Let's assume that we have a symmetric matrix. Symmetric matrix is a matrix that it's invariant with respect to two transpose a, transpose a. We have a, for example, and this lambda matrices, they are somatic. We transpose Lambda since it's diagonal would get Panama. And the matrix is positive semi-definite if its eigenvalues are non-negative. Of course, this is a square matrix, it is a square matrix. And then we can easily find eigenvalues. Otherwise we will have by, I don't remember what display ads. But anyways, let's not just assume that it is square. We will talk about matrices that are not square later. So a is square, and then we can find eigenvalues and eigenvectors. The matrix is positive, semi-definite if its eigenvalues are non-negative. And we can prove that in an easy way. A matrix a can be expressed as eigenvector eigenvalue decomposition, where Q is a matrix that condense elements that are orthonormal. So they are orthogonal and they, and they are, they have no one. In lambda is a diagonal matrix that contains the eigenvalues, right? So A's pair of elements q lambda, they have the property a heel I is equal to lambda iq. This is how we define the elements that we have here and the elements that we have here, assuming also that Q are orthonormal. So then let's take any, any better x and do a dot product. The B includes this matrix. This is a product, right? But this is equal to x transpose q times a, sorry, lambda times Q transpose x. So here what we have is a vector x prime y1 times x prime transpose y. So this is a dot product. If this quantity is non-negative and it is a strict dot product. If this quantity is positive and this one is positive, always for any vector x by x prime. In the same manner. If Lambda contains elements that always, that are only positive, non-zero magnetic. Alright? So this is the case with all four. So this is a dot-product. If and only if a contains eigenvalues that are strictly positive. This is a strict dot product. F Lambda contains elements that are still positive. Otherwise then that blood is not strict. We will see that in general, we have infinite dimensions. What am I not probably is not going to be a strict. But then we will constrain the dot product in your spaces. Right? And so we always have matrices that have n elements and they have. Eigenvalues that are positive. So just think of this. We have an infinite dimensional space. We can recite infinite dimensional matrices. And then you my audio, well, they factor of eigenvalues. It has to tend to 0. Most of them they have to be 0. Otherwise. We will have dot products that lead to vectors of infinite-dimensional. So that my matrices in that space, they have eigenvalues that are almost all of them. That as if it turns out that in the dual space, we have n eigenvectors and eigenvalues. Right? So we have this thing, we have to be reminders. They couldn't Fisher theorem. They got official theorem says that the minimum eigenvalues of a symmetric, symmetric matrix its ends. So the minimum eigenvalue of a is the minimum of this product where v is different from 0. Right? So of all vectors V that are not contain, that are not etiology and the minimum possible dot product Baghdad is the minimum value of an egg and bathroom of eigenvalues are. All right. So in this case, a symmetric matrix a is positive semi-definite if and only if. This is true for any vector. Right? Basically, what I'm saying here is what they said, what I'm saying and as lied, sorry. My screen. What I'm saying Like is what I said in the white board. A symmetric matrix a is positive semi-definite if and only if. This is true. For any vector v, which is not the origin, which is Nancy Wright, is true except for the trivial case that subject. And so it's easy to prove that Graham and kernel matrices are positive semi definite matrices. What is a Gram matrix is a matrix of the products. What is a kernel matrix is a matrix of the products in a higher-dimensional space. Our dot-product is what we call the kernel. Kernel matrix is nothing but a Gram matrix is a matrix of dot products. So it's easy to prove and we will get that kernel matrices are positive semi-definite, matrix is positive semi-definite in China. So they may be the case that for a vector v, The dot practice it. Right? So they define, as I said before, they define drugs that are not strict. So the final kernel as a function that has this property, a Cornell is about product and a Hilbert space where phi is a Monday from space X, space F. And so we define a kernel matrix like that. Alright? The kernel matrix is always defined like that as the entry ij of the matrix K is about product in that Hilbert space of two vectors, X, i, j. Right? And, but here we don't take into account fight with not phi. We don't care about fine. Because we know that this dot product can be expressed from the input data. And so we finally like that. And then we can prove that this product is non-negative for any value of v. So we have this product, right? And then this product is equal to if we take the, each one of the elements of this product, this is a scalar. This is equal to the double summation of V by VJ, some element VI here, an element b j here times the dot product x i j into some of this product here. And so this corner, yes, it is dot product between these two vectors. And now we put the I here inside and BJ here inside. And then we put the subnet, the sums inside of each one of the elements. And we end up with this equation here, which is use the distributive property and solve this here. Since element vi and vj, elements x, i, j, they are equal, they are the same set. We can construct this dot-product between the sum of vectors, the sum of vectors of excitons, the eye themselves. And so this is positive. This is positive. It can be 0. If this vector phi is the origin, is 0, or if BI is passive. But in other case, this quantity is positive because this is a single dot product. So this expression is non-negative for any sequence v I give these vectors. They are not the origin, a fight or not the origin. And FBI does not say something else that we improve. Our matrix a is positive semi-definite if and only if we can, we can decompose a into two equal matrices, B transpose and B is true. If a and B are both like that, then we compute B transpose AB, which is this B transpose B transpose times b, which is the norm of vector b v. Right? So this is the norm of vector v transform with matrix B. And B is any matrix, anything. Right? So that's, that's basically it. And this is something that is useful, right? And 11 Example will be a express as a function of eigenvalues and eigenvectors. So in this case, we have q, we have q lambda 1.5, lambda 1.5 u transpose. So then in this case B is this. We can also transform matrix using transformation are equal to a, equal to L transpose L. Where L is a triangular matrix. All right? There is only one triangular matrix that satisfies this. And it's called them. I said before. This is the year. I can't remember. I don't remember what's the name? I have a hard time to remember banks, right? But you can decompose a square matrix into two triangular matrices, right? Composition signs there will be U of L. Okay? So what are the outcomes of this lesson? We have seen the definition of peeler space and some important aspects of your space, completeness, separability and L2. And until properties, we have seen some examples of products. Of course, we will say more that are useful for us from our point of view of time, we have seen the property of positive semi definiteness. This last property is important for us. Why? Because our machines, our machines contain the expression Alpha transpose Alpha. Right? For example, in, in super vector machines, actually what we have is this. Let's see, alpha transpose times on. So we want to solve, we want to solve for alpha, and we want that there is only one solution. So this quantity here, it must be always positive or 0. This should be a quadratic expression. So we want this to be positive. So this is always negative and we have quadratic function and a linear function. The sum of both, it has a single maximum. So this machine, it has a single solution. And this solution always exists. From practical point of view. The fact that K is positive definite, it's important. It's fundamental. So we must construct kernels matrices with dot products. One can say, Hey, you know what? I have this function that passes from the input space, input space to scalars. I don't know if it's on the cloud, but it's non-linear and it's convenient for me. You can do that. And you might find a solution. But it might be the case that you have more than one solution. It may work. It may still work. But then you have to check out a solution and some solving this, It's not going to be that symbol if this expression doesn't have single maximum. Right? So in OpenStack matrices k, we use the dot products. It's better to use dark brown. It's better the US then positive definite functions which can swipe positive-definite matrix. We will see what is a positive definite function is happy. And that's it. Right? So next, we'll see some more properties of certain else. Let me see. We are here to do this thing. Here. Stop share. So we have more spaces. So we will see how to pronounce that our products. And then we will see colonel IN analysis. So how do I PCA? It's a particular case of the application of the properties of kernels to construct a linear algorithm in that he had been a Hilbert space. Right? It's a particular case, but it's a rather complex case which might be useful for you in that illustrates how we use the properties of hubris basis and Kronos. And then the last thing is about the reproducing property. So we will see why we call them reproducing kernel here respects. But that's a costumes. This is probably the most Theory of us. And from here, things are much easier. We're going to do. Professor Yes, sir. Maybe recently submitted on Canvas. Yes. Yes. I explain it and it's not just because I've been on when I when I look at the different children often and that sounds like that's not an option for us. Because are they let me say settings like it's number 4. Sign-on for mediates, its the only option that says unlimited attempts to figure it out.