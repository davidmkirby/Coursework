 So when writing papers, you've never built a lot of people are now quite. It was, yeah, we wrote a part that they gave us 5%. So Mike was occupies and he said, I want 60% cerebral. Basically, after all, whether they're turning, that they're going to separate this guy which have 0% and you put five. And this is illegal. So thank you. And I might have a suing me for adding somebody there. Anything you want 50%. Say it was to go against me and I said, well, you can certainly put it my present you on. And they didn't. Anyway. So let's go to 6.4, which is kind of a structure scores. I'm going first to wrap up. Yes. So the thing is that we have we have name, which is called the kernel trick. Class hour. Looking after his non-linear what i, then, the expressive power of this machine is then much bigger, right? And so let me summarize. Let me summarize. So we have machines that can be expressed as w equal to the sum of sum, sum. All right, So my machine is a linear combination of the data. And this is two. Dr. Dawna, we lost audio. Hello. Can you hear me now? Yes. Sorry about that. So while I was saying that we have we have a theorem that make sure that that follows me to represent the set of parameters as a linear combination of the day, not, which in turn allows me to express my estimator. This way, you're seeing a linear combination of the product between the test sample and the training data. And then, so the kernel trick has two steps. First, we assume that that is a hill. In this case, to which we can throw the data through our nonlinear, nonlinear transformation. This transformation passes from d-dimensional space, here, a space of higher dimensionality, possibly infinite. And so this space is endowed with a dot-product. This does, is this one. So this is nothing but the sum of them. Multiplication of the elements of, of each one on day of the backticks, right? Just a rubber dot-product in that Hilbert space. But this can be expressed as a function of the input data. So we don't need to actually construct fine. We don't need to compute fine. We just need to use the dot-product sense. My machine can be expressed as functional products are. Finally, my machine is y equal to w transpose phi of x. That's equal to the sum of alpha i times k of x I, x plus a bias. And this might be infinite, might have infinite-dimensional this too. But we don't need to see them. And some of these spaces, they are explicit so we can actually construct the transformation, but we don't need to. So this expansion that they've constructed the other day, it's not actually needed. We just need to know that that space has a dot product. We need to another product, that's it. In some cases, this transformation is unknown. We don't even know it, and it has infinite dimensions. So until really the only thing that we can do is this. And the computation of complexity of my machine. Yes. And so here we need to perform and dot-product. So we have many lay down. This is probably not the best thing that we can do by if M is a reasonable value for my computation of resources, then this is a way to do things. Right? When I saw this and I understood that. I I am right. So they don't question that these are not the things that they use in social networks. For example, to apply machine learning division of guidance. Because they ha, they collect millions, billions of samples. So we have to use different approaches. When I now, when we want to do artificial intelligence in Twitter or Facebook or Instagram, right? Because we have a lot of theta, so we don't use this for problems where the number of data is limited. Deep learning, which is basically use, it cannot be applied because they are actually, we need a bunch of data for effects that were, probably wouldn't have too many data we use that states that roughly speaking. So in any case, the computational complexity of my machine is n and whatever and our products. And that has to be multiplied times the complexity of computing this operation, which is much less that invaded conflicts. So this is what we do. Now, what kernel we use? And there's always two answers. The short answer and the long answer. Short answer is, I don't know. The longest comment in the long run. It's sound this is why they explained the other day. At a glance. Now, let's put an example of group start with an example of a dot product. So if there is another theory that we haven't seen before, but that's, and I will explain any grant that says that any positive definite function of r In our b times ROE into our, any positive definite function as two vectors into and real numbers is about product. In has been some Hilbert space with another here we are with part one, possible positive definite function. One function which is actually no negative self is the square exponential. Square exponential. Well, in classic machine learning, this is called Gaussian function or which is not Gaussian actually, or RDF. Rdf is a bunch of function touches this, right? So boomers call them RBF. But the correct name is square expansion is this. K of x i x j is equal to the exponential of minus 1 over 2 Sigma squared x minus x j. So the exponential of minus the distance between those two vectors multiplied times a constant. And this has a shape of a Gaussian. Of course, it's not that often because it's none of our eyes. They, either of these is not one, right? It has a shape of a Gaussian. And it's a radial basis function because it's a function that has a mean, that's a sender and a circle around it. So, but there are others. So and it's a basis function also sometimes is called basis function by. I don't use any of these numbers are because it's confusing. Rhymes. We call, in Cornell learning, we call basis other function. Actually, we call a basis function is phi of x, right? So this function is positive definite. They must be in it. It has all the properties of a dot product. Right? In particular, the inequality, I think we'll tackle poverty. They buy another narrow than a general properties of drugs, right? So this is dot-product in an infinite dimensional feature space. How can we prove that? Let's use the definition of sine of a function first. What is a cosine function? Cosine is equal to the cosine of two vectors is equal to about product. Let's use this nomenklatura X9 SJ. Did that problem in aerospace over the norm of Sir I exchange. Right? So this is the dot product in that space, which is this. And another thing that we can do it, That's right. These are equivalent and footers. But anyways, so this is a product, so the cosine of two vectors is this high. And that can be an explicit glue within us. The kernel dot product between XI and XJ over the current product between X i and itself times the quinolones front of the x I, j and Excel. This I still have the square root of. Now. What is the kernel dot product between x and x? Is the exponential of minus the distance between x i and x i, which is 0. So the exponential of minus 1 over 2 Sigma square x minus x i squared, which is equal to the exponential of 0, which is equal to 1. Right? This itself gives an interesting kind of weird property of this. The data that we're throwing this space will seal. So the cosine of 5 is equal to here we have one, here, we have on one end here with a kernel. So the equal sign as the coordinates of the exponential of minus 1 over 2 Sigma square x minus x j tubercles. So this is a kernel. And what happens with the exponential function is that it is bounded between 01, always higher than 0. And it is one if and only if x i. Is equal to x j. So we can say, let me erase because to put xi minus x bar is that it's hybrids. Yeah. Yeah. It's equal to one when the story so open. Yes. And so then we don't have the cosine of phi of theta is equal to exponential of minus 1 over 2 Sigma square x I minus j squared. And we know that the exponential of minus 1 over 2 Sigma square x I minus j square is less or equal to one. N is higher than 0. And so this equality is only true if x i is equal to x j. So we can save this if I is different. And this is basically the, so that means that two vectors that we throw in this space, they are never orthogonal. If they were orthogonal, then the exponential will be 0, but it cannot be 0. So phi of x, I am fine of Jane are never orthogonal. And since they dot-product is always less than one, that means that two different vectors never describe an angle which is 0. Cosine of 0 is equal to one. Right? So two vectors, XI and XJ, they are never collinear in the hero space except for the case in which they are saying better. So phi of x I and phi j never apart angle theta equal to 0. So we take two vectors, we found them in aerospace. And what we see, we can see it because it just to betters. What we see is this. We never see this. And we never see this. So till vectors, any two vectors that we throw in this space always describe our component which is co-linear, undergo one which is orthogonal. Or I always love never just one. So if we put two vectors, any two vectors in space, we have that these two vectors span a subspace of two dimensions. Right? Now, we draw another vector. This vector, always this price I component, which is orthogonal to these two vectors. So if we throw another vector, we have a sustainable directions. And so on. Because all the vectors that we throw in that space, they will always have a component which is orthogonal to every other vector. So withdrawal and vectors. In this Hilbert space, we have n dimensions. And this has no limit. It without infinite vectors in that space, we describe a space of infinite. So this exponential is the dot product in a space of introductions. I could write this in a more formal way, but it doesn't really make any sense because an intuitive way of explaining it, it's more convenient here. It's kind of mind boggling that in every dimension that you go about, there's a, an element that is orthogonal to everything else. Yes, this is like telling the wrap my head around why this is why you have here, right? You can have many database in that space for every two vectors that you're happy about this. It's easy to imagine enough for live through this only flip through there. This is why we have the mathematical polarized to draw with special interpretations because we live in a space in three-dimensions, our brain has dimensions. Now, we have seven I mentioned we will be up to sang by eBay. Say every two vectors describe a component which is orthogonal to one of one of them. That means that we have n elements that are orthogonal. So in other words, this, this means that if we throw and vectors, we have stability. That means that no set of vectors that we throw in the space are linearly dependent. More formalistic it. Right? Now two vectors are linearly, linearly dependent vectors, then we have nitrogens. That's it. So in three-dimensional space. Now, what is, what is the transformation of? Fine? Yes. So you said that. So the n vectors, yes, that one vector has its product, that's by parliament. But yes, so can be not exclude that one vector is at the end they pursue that. We're talking about one better, better as a common complex. Yes. So will it not be at minus one? The dimensions? Now if I have one vector, we have one dimension. If I had to venture with two-dimensional books, if we have vectors, we have 10 remains. One of the vector has orthogonal components on there. Yes. So the dimension will not be n minus 1, which is 9. Let's see by that holds for all the vectors, the vectors have 10 so that every, so all the vectors, they have two components. One is linear and the other is orthogonal. One is linearly man, and the other one is orthogonal. I mean, it's always n. But since this false for all them vectors, then we have two dimensions. All right? Each one of the vectors as nine volumes that are orthogonal to the rest of the vectors. So these vectors describe 9 I mentioned, and the new vector describes as an animation on. So at an easier way to think of this is, is, is simply, if this is true, that means that among the ten doctors, you're going to have two that are linearly dependent. So you have 10 vectors are linearly independent. Sovereign I mentioned. Alright, so that's it. This is an umbrella because it's a positive definite function. And it is the drug effect of any three-dimensional space. But what is this Hilbert space? What does this transformation? Again, short answer. Either not somebody else. All right, Bear expressions, but they are not explicit. So we cannot extract the components of effect. And we don't need it. Why? Because the properties of that space, they are the same properties that we find in a dual space constructed with this dot products. If we have 10 baggers and we're going to deal with all the dot products between them. We have a dual space of 10 images. So we must from infinite dimensions two on it had some space. The properties of way of saying, alright, so we don't need to, this is why we have Bois subspaces. So we don't need this, right? Well, I don't know You read that because some mathematicians in a meeting they asked me, do, does this throw any lights over? What is this transformation? And I said, Not to my knowledge. And so I always kept thinking why this guy asked me about this. It's because maybe they might infer properties are theorems from this era. So this What is the aspect of the data inside this space? Well, all the data that we throw, it goes to the first quadrant of a hyper sphere. Right? So you think of a sphere and one of the violence of the sphere, the equivalent where all the components are positive. That's the first one, right? But the components are positive. So in two dimensions, this in many dimensions wherever, but they're quadrant. Here we have for violence in our sphere, we have more six, right? So the only components are positive or the data is thrown in this quadrant, in this surface. Why is that? Well, you saw a vector. What is the norm of the vector x i, no matter what it is, what is the norm of the vector, binormal vector is equal to the vector with itself. So then armies one. Right? The norm of the vector is one. Here, for example. That is phi of x. By now, you take another vector. The normal vector will be also one. So it's going to be somewhere in this sphere. But we know that the dot product between two vectors is between 01. So the angle is between 0 and 90. So here, it cannot be here. Because then another vector that is here, we'll have an Anglo which is higher. So if one vector is in that, if one vector is in this quadrant, whereas they have to be the same, but in three-dimensions, you have a sphere for white dots here. That's really because they will be there. This is so restrictive us down. But it doesn't matter. Because the space, it has infinite dimensions. Infinite-dimensional spins that age. A barometer mind is that dimension is infinite. So no matter where did these vectors are confined in a hybrid, why don't you can classify them using our classifier inside this space. You don't classify them no matter what. The labels. Since they damage our money, because I mentioned are the space, if it is infinite, you can always place a hyperplane that classifies everything correctly. So they expressive capacity of linear. A linear classifier inside this space is infinite. Right? Spot an example where the input has one. I mentioned the input image. So that's, and here we put randomly beta. And obviously this problem, Here's non-linearly, is not live in linearly separable, right? Is not linearly separable in this space. Okay? But now, let's put an exponential problem. Each one of these samples, right? So for the x axis, we put positive exponentials and for the most negative explanations. So we have something like that. And so when a sample as a response, which is positive, we assume it's 11 is negative, it's minus one. We can do that in two dimensions. I can put positive exponentials here, a negative exponentials here. Because if I anything that you all right. So we can think of this kind of classifiers like that. This is a classifier where I first row with the data into a space, and then I use this little space, and then I use linear classifier. And this linear classifier. We will have all these thresholds here, which are the points where the function is equal to 0. So this point here, they are points of hyperplane in the here a space here is the same. There's points for which the function returns a 0. They are points of the hyperplane in the Hilbert space. Or explicitly classifier that has this product is expressed like that. So this is my training data. I, this is exactly what I do. I put for every sample that I put an exponential centered into some, right? So basically, I can classify anything. This has infinite capacity or infinite complexity. And this is not good enough. It can have infinite capacity, but it doesn't need to. Because we have methods to unfolded ammeter one, I guess complex patterns have, I guess that much, right? How do we do that? Well, here, we will have two ways of controlling the value to one, I guess I mentioned. First, well we have C, we see in our support vector machines. And by r cross validating it, we control the capacity of the machine. As we solve. By. Then we have another brand, which is this. Sometimes we buy it's one over two sigma square because their husbands from our Gaussian, sometimes simply gamma here. Right? It doesn't matter. So I like to use this expression better because this is directly related to the white for my next bunch. So how does this work? Again into it? Talking about this, because this is a very popular kernel. Everybody right through you get any neighbor that uses kernel machines, they use this. Or at least they try this and see if this is better or that angle. So these work. The squares financial is a particular case of stationary kernel. It's called stationary because they, they, kernel depends only on the distance between the two samples. But it doesn't matter where they are, right? The only thing that matters is the distance. So the exponential of minus 1 over 2 Sigma square x minus x j square is equal to the exponential of one minus two Sigma y over 2 Sigma squared times the distance between I and j squared. The distance and the distance in the input space. So no, We're, these are, the response is the same. This has nice proper this too, but we could be talking about ask for three months. I non-stationary, Cornell is the standard dot-product. Or they polynomial kernels that contained at least another product. If you change the norm of the two vectors in R. Using the standard product, even if the distance is the same between the two vectors. Bro will change, right? They don't follow the standard Euclidean dot-product. Dive depends on the norm of each one of the vectors. And this is not what happens here. So this is called stationary and it is not station. They weren't. The thing is that, as this is the expression of money back. So we will compute the exponential, we compute the Taylor series expansion of this exponential. This this is proportional. This is proportional. I don't I don't remember. I didn't record this and I don't remember the exact expression. But this is proportional to the sum of the distance to the power, the distance squared to power I to I, pulled over by the k over k factorial over Sigma squared. So this is a polynomial. All right, I'm probably adding two to find the expression in Wikipedia to, to make it right. But this is, at any rate, this is a polynomial. But this is, this is a polynomial of infinite order. In we changed again D by the distance explicitly instance XI minus a j. Here, what we have is something which is very similar to the above data expansion that we saw. If we put two vectors here. And we compute this instance explicitly funding on one of the vectors. We have all of that expansion and zombies what that expansion, we can extract it to the two component vectors. I mean, we cannot, in general, we can now, as I said, phi is not now in general. By undoing for one-dimensional, for example, a one-dimensional, we have an expression for that. So this is a polynomial. And then it can be, then this can be expanded. And then this expression will be the dot product of two vectors expanded using our Direct such. But it has infinite or. So. What, how comes that sigma controls the complexity? Well, k squared, k in February on it increases with k force. So the components, they have an amplitude that decreases with, it decreases with cave. Right? So we have infinite dimensions. But these dimensions, they go vanishing. And they have to, they have to, because if they don't, we will find that those with known which is infinite. In this case, it's not true all that because they have no others. Even if they have infinite pump words, they have no one. So the components of the vector, they vanish with the order. And so how fast they varnish. It depends on singer, Siemens. For small values of Sigma Musgrave. I think that this is two k, right? I'm sorry, but I didn't prepare. I'll take a look later. But I want to do the Taylor's expansion now, right? Because I might do it. This is not the Taylor series, but these are my plowing. It's actually centered at the origin. But anyways, so if we put a small value here, then they elements of this expansion, they will banish slowly. But if I put a very high value here, day components of the expansion, they will vanish very fast. In the limit if, if this quantity is very high. For small distances here, compare. So I mean for distances that are small compared to sigma. Then we will have only one component that matters, which is the second order. The first order doesn't exist. All right? So borders from fourth to hire, they will vanish. If sigma is base very high. Then what we have is a second-order polynomial. And in this second-order polynomial, we will have a few effective dimensions. Right? So if the input is a scalar, we only have two dimensions. So we pass from one dimension to two dimensions. What is high and high value of sigma compared to the x? That's it. This is the distance d. And this is the one. For this case. In the distance they are on this order of magnitude. For this case, I'm hi baby of sigma is something like that. Maybe a centimeter high. Basically, I can express four smallest and says less than one. I can express this as a first-order, second-order polynomial easily. By if sigma is small, then I have the exponential, which is this thing, will be something like that. I cannot express this with low order polynomial. I need a higher-order polynomial to approximate this. So this is a space for which small distances. Day 4, when the data was there, they have close to each other in the input space. They live in us. Here we're space which has a few dimensions. So the dimensions expand by a polynomial of order two. And then we use this corner. All the data between minus 11, day live in a space with a lot more dimensions. So here we have eight small. And here we have Paige. Hi. Alright, now, if I have samples that are here, here and here, and here, you're seeing this exponential function that classifies them, blah, blah, blah. But if I use this exponential, probably going up. Alright, so this exponential offers me more expressive capacity that this one, why? Because of this list is small, because it expands a space with a feel effective dimensions. And here we have high age because of this. At small distances, we already have a space with a big number of effective diverges. So they kernel itself. It helps to regularize the problem. Sigma here or here, controls thereby meta-learning is dimension. So the kernel itself, it has regularization properties as well as W. When we minimize their neighbors about the regularization path products. But they don't want to get inside. Get into that. Just lets keep this intuitive explanation. And this is what always happens. Kernels in higher-dimensional space in three-dimensions. They can always be represented like that. And in such young girls, some of them, not all of them. They may be infinite-dimensional space kernels. They have components that vanish, right? And how fast they vanished depend on the parameters that we use for that. And this is what joseph amateur like this edge. So we have to cross-validate CDMA, right? We have to cross validate Sigma. As long as and as well as C and epsilon or Neil. In regression. There was on, yes. So what happens with a polynomial kernels? Remember that we have, we also have polynomial. We see here that they conformance. The components of this exponential, the components in a Hilbert space. They vanish. A bandage on how fast they vanish, it depends on what happens with the polynomial kernels. Polynomial kernels thing is different. Actually, we can probe. Now when k is equal to this. Let's use this nomenclature. This is a channel exponential or polynomial kernel of which we saw an easy example the other day happens here is that, well, it's not difficult to prove using the Newton's binomial, using some math. That they, components of this Cornell. They have a shape like that. Well, plus a constant, right? Let's do it. Well. The polynomials, they have spread like that. So basically the first and last component, the linear component of my polynomial kernel. And the last one is they have, they have the same weight. Right? So I can, if I if I increase deed, I increase the number of components. But they cannot control how much they width, how, how important they are and actually the components that are around their name on it, halfway down, they are the ones that have more weight. Right? In this case. It's not possible. Using this, it's not possible to control the them. It's impossible to control. How many effective component, say half of them they are effective. And this is why polynomial kernels they give, they, they, they are problematic. They are Parliament. And in particular, this is high, this quantity will be very high. So I have to, I have to limit the norm of this vector. So they are between 01. Otherwise, they deem importance of the components, the bank, that dynamometer is always very high. So it's difficult to control a machine with polynomial curves. Sometimes they work. For particular cases. In general, people don't use them. That they didn't use them too much because of this. Because you cannot control the affective dimension by just tweaking a parameter. And you control that. Yes, you can, you can. So here, what we have here, if our components that remember that here we had one, that we have a square root of two, then we have three in our component in our example. So in our example we have one square root of two square root of 3, and that increases with Wendy increases. So one way to control the number of effective dimensions is taken into account. And here what we have is products between components, right? We have X1, X1, X2, X1, X2, X3 idiocy, right? So in order to control the weight that we add to each one of the dimensions, we have two other matrix here. So we can use k of x i x j equal to x times a x Jn transpose plus b. Alright? But then we have to cross-validate more parameters. We have to personally that d to c, What is the better option? But here we have a matrix with a bunch of components. If we have probably done the dimensions here, this magic has capital B is where elements and we have to consolidate all of them. So this is computationally intensive thing that actually adds complexity and it might not work in many cases. Available against is to put an ammeter matrix here, which is diagonal. So instead of having, carried all the components, components we have coming up, which is more manageable. But it's not as general as we want. Isn't the general because it only accounts for products between IPO components. I would basically right. So even if this is a suitable thing, it's not used either. It's not used because actually, that's right. So many cases we just got to the exponential. The infinite complexity. Yes, but very easily control here we don't have infinite complexity, but difficult to unfold. You can do the experiment when you, I finished the corresponding homework, right? So I asked you to do an explicit expansion, then do it implicitly with our polynomial kernel. Instead of three. But 10 or 20, you will see what happens. The machine will not converge or it will merge the song, we're solution. Alright? It's very, very difficult. One for large n in two-dimensions. When you put here a very high number. Let's tell what's going on. Should I use? Again, I don't know. It's an art more than a science. And in many cases, we have to figure out just using heuristics, right? How do I think that my data is basically what is a, what is the dynamics? How about the mean or the variance, the maximum, the minimum, how many components, how spread out they are? I don't know. We have basic kernels which are the exponential. They irrational but that they, they, I know sinc function. There are many. The polynomial, right? Place it. And the question is, well, try the ones that you think that they are reasonable. And then you can combine curves. You can combine grids. So first, let's go down to this theorem. This theorem is very fortunate, at least for me, because it allows me to skip all the a theory about Hilbert spaces in order to understand what's going on. Right? So we could go through the representation theorem, the probability of reproducing that represent poverty of kernels with everything we have it in the last set of slides. But this is a theorem which is not easy to prove. We're not proving it, but I will get us. There are references to papers that you can read online. The one minute James Mercer or the one by ICER might I be like? Which is a newer way to prove this theorem. But it's easy to understand, easy to interpret. So this is a fundamental result to this is a fundamental result to understand kernel methods and to justify. And it's the key idea between behind the current object. So this mortality or it can be stated as follows. I want you to understand. All right. So yeah, nicer money. No. 1964. This is what this is called a rather all chronology. So let's assume that we have a function that maps from two vectors in Euclidean space into a scalar. That is, why did up above, that's right. You take two vectors, u, the innovation, you get a scale. So we have one of those functions. Then. We also have the set of all positive, all square integral functions, right? We have f positive time 0. It's going to have functions which are functions that satisfy these properties. The integral for them all the ln of x of n squared is less than infinite. This will make sense later. So if my function k satisfies this property, so we pre-multiply times f of x, then the kernel between x and x prime, and then we post-multiply times f prime. Well, this is a scalar, so. All it doesn't matter, right? Then this may be this integral. The integral for the domain of x is always non-negative. This is what we call a positive definite function. A function that has this property. If we take the function and we must multiply by any positive, any square integrable function. If this is always positive, then this is a corner. If this is true, then the statement is very strong here. If this is true, then reproducing kernel space x and our mapping function phi exist such that this function is a dot product between two vectors, two vectors in that space. So this thing is true. Then this is true is not easy in general to prove whether a function is positive definite using this expression when they're there. Otherwise, right? The general theorem is this one. Why is this important? Well, first, because if we can prove this for a function, we don't need anything else. This is a dot product in one space. I don't know. Sometimes we can prove whether the function is here. It's a dot product of an infinite-dimensional space or finite geometric race-based blue. Right? Bye. All righty. Now let's, let's go fanned. Construct sampled at discrete version of this interval. So we have the integral from 0 to x of f of x times k of x, x prime, f of x prime. This is positive. So if this is true, and I sample this interrupt, I will still get a positive, a positive expression. So what is a discrete version of this? So this is a double integral, dx, dx dy. So they discretization or not. Here's define. We have f of x, k of x, xi, x j times f of x k, right? X and x prime is the same, it's the same domain. Right? So we can say this. We have x i and we have a j, but both they are the same set of double summation. We can just express our assumptions. Genuine forward. This is positive or is it now my functions f, they are square integrable functions. In particular, that means that my functions, they are bounded. My functions are unbounded. So F of X is alpha, i is a number. Any number, I guess it's another. And then I can rewrite this double summation from I equal to one and j equals one to j equals I, Alpha I, and k of x I j. And this is something that I can express. Alpha transpose K alpha, where alpha is a vector that contains all vectors alpha i. And this matrix K is a matrix that contains all these dot products. And this is higher or equal than 0. So k is positive semi-definite matrix. If I construct a matrix with a positive definite function, I get. Positive definite matrix. And this is all I need for my things to work, for my cost functions to lead to viewers that have a solution. And this solution is unique. The only thing that I need is a k is positive definite. Alright? This will be higher than 0 in this dot-product is strict. Let's assume that my products are straight and then this is true. This is online. And this is the connection between Kronos and support vector machines. When one can think of dot products, one can think of positive definite matrices that function then lead to positive definite matrices. And then I have solutions. I don't think what you want, you want whatever is the best for you. Getting no problems. And think of which is positive definite functions are related. And this is why. So between the two, I have this in the script. This. And we have few minutes. Let's start with some more properties of coordinates. And these are the properties that Ptolemy to construct Kronos with CMS that Morgan us with gardens. They are what? These probabilities, they are called a closure properties of coordinates. And in this, what allows me to say that a corner or a combination of Kronos is or it's not a kernel. All right? So they probably one, yes, they Daddy some Hilbert spaces. So if we have two cardinals to kind of functions, say for example, linear or polynomial and exponential. And then a linear combination of them. This is a kernel under a simple condition, a and B, the elements of the combination positive. If you have a and b are positive, then the sum of kernels is a kernel, which is something that you can prove. So you have a set of points. And with them, you might start to matrices the first quarter, not the second hurdle. And then you add them together. And so if you multiply this combination, k1, k1 plus k2 times Alpha transpose Alpha. And this quantity must be positive or non-negative in general, for any alpha. Well, I know that k1 and k2, they are positive definite matrices because we know that k, what they want to get your kernels. So you develop this equation. You have a times alpha k1 alpha plus b alpha keto alpha. This is positive, this is positive. So if a and b are positive, then the matrix K is positive semi-definite. That's as easy as that. So a sum of kernels where the elements are the sum, this are positive, is another curve, right? Simply because the matrix is positive-definite. But of course, if you want to think of here a space S, What the hell is going on here. What is this new space? Because this is a corner. That means that the sum of these two is about product, a dot product, or the bulk of what the space, where it's very simple. And this is why we call that direct sum or fewer spaces. So we assume that we have two kernels. So we have two transformations into space. The space by my phi one and the space that we're fight to live. And so we call them H1 and H2 viewer. So let's construct a vector file, which is a concatenation of these two vectors. And each one multiplied times the square root of a and B. And let's compute the dot product between two of those concatenate, concatenate vectors. So we do this. Then we have this times this plus this times this here. And that is the result, k1 plus k2. So adding together two kernels leads to a space which is the sum of both. So we put one space and that orthogonally, we put the other one. That's it. And so the cardinality of the space is equal to the cardinality, the sum of cardinalities above the spaces. All right, So if one state has I mentioned be, the other has dimension b1 and b2, then they dimension of the space is d1 plus d2. All right? In case of infinite-dimensional, we have, right? But if we put two different exponentials, then the number of effective dimensions d is equal to the sum of both. Is this useful? Yes, it is, Yes. Right? Why? Because when we transport the data into a Hilbert space, we are transforming the features. In this transformation of the features that make that is what extracts the information. Remember the XOR problem, the x or toddler where I had this thing here. And what we did is to add a new dimension in a convenient way so that they go up. And then we have three components instead of two weeks information. We can extract the information in different ways with different kernels. Right? One kernel might not be extracting all the information that we. And another kernel way my extract different information. We can add them together. And that has a lot of papers about combinations of kernels. How do we do it? For example, we put a bunch of exponentials with different y's, right? Or we can combine a linear and nonlinear kernel. For example, I have an electrical measurement where I know that I have linear or non-linear components. There you go. I have an expression which is linear to extract the linear properties. And then another together can be exponential. That takes care of the non-linear components on me. And I might want. This is not the only way to construct kernels. There are more, we'll see them during the next, the next class. Right? Questions. Again. This is that if I need to slow down, I'll do it. If we don't finish with the other topics. I don't care. I want you to understand that. Thanks. I'll just give it to me once a month. I'll start over. I know you have homeodomain going to keep on going. I'll start over. I'll do it, right. I want you to learn. Right? So they said, I let you go. Thank you very much.