 Alright, alright, so debonding some, I guess the units with a given probability 1 minus Beta. And this is here. This happens. So the umbilical arteries, the arteries bounded by the empirical risk plus the stickers. So with a given probability, you have as a function of age or behavior like that. First, we know that the empirical risk increases with age increases. We know that. We know that because when dates is higher than them, a number above that, when h is higher than the number of data that you have, a classifier is able to classify everything with 0 barracks during the training, right? So you can find a classifier that is able to demystify everything with 0 errors when case is higher than the number of summons. So this quantity tends to 0. Right? But then we have something which is called Statista risk. And it's a quantity increases monotonically when age increases. And then we have the actual risk, which is our lives. So what happens here is that when age increases, when the expressive capacity of the machine increases, during the training phase, you classify everything with 0 errors. And then when you change the data, the machine is not going to work properly. When you put data. Not for tests, not previously seen by the, by the trainer, then the machine is not going to work. And this difference is what we call destructive battle. And this, I believe that you pretty much understood it, right? But now, if we go to the experimental part, how do I ask you to, to service? Well, here we have the actual risk. We cannot measure the actual risk. The risk is the probability of error during a test. And this probability we cannot, we can measure it. We can compute this because the probability needs, they have knowledge of the distribution of the data. We want puppets. But what we can do is to measure our sample using a sample. We compute a sample mean of the error during the test. So for us this is the death penalty. And same with, same with the training error. And training error is something that we cannot compute. Given a machine with a set of parameters are really trained. What is the probability of error in training while we don't know. But we can measure it. So what we do is to take a given value of age and with this given value of x between a machine with us there. So we train our machine, we just did with the training data and so on. And we do eat many types. And in average. In average, they measure error. Me try WorldCat to the probability of error in training. So this is a training error. So training error, we measure it by doing the same experiment many times. Of course, every time we do the experiment, we have to change the training data. Right? We have to change that. Otherwise would be the same data. We get the same solution. They will pay. The machine has single solution. This is something that we cannot do in reality because we have one training dataset. There's no way we can have an accurate measurement of the probability of early training. But if we have a limited number of training and test data, this have we done, of course, this would not be realistic. But for this experimental, compute this training by doing the experiment many times when defending, right? So one of the, one of the things that I find in homework is that these graphs, they are noisy because you do not average enough. Sampling, enough runs. This training error measure obtaining error tends to the actual training, our improbability by mutual or the, or the law of large numbers. Right? Now there are refinements to the Islam that measure what is the variance of my measurement? And the variance of my measurement decreases when the number of times I do the experiment increases, right? And so this is when you see it, you don't have too many rooms that you have a noisy while not smooth graph. But if you leave it for a couple hours or one hour, I don't know if you saw an experiment and it took like 10 minutes, right? And this is an F4 that you have to do when you try to publish an actual paper. Because they, their readers, they audience, They want to see the effect. They don't want to see noisy graphs. We want the CDF, if I am speaking of which, what is the interesting part of it? Well, when did, when the test error is minimum, we cannot put aids, we cannot measure age or fixed rates and then train a machine directly. But there is something in the support vector machine that increases monotonically with age, which is C. If you remember. The criteria to train a support vector machine for classification, the criterion is to minimize one hour to the norm of w plus c times the sum of, it's like games. And I want to know whether it's a child. Yes. And I think that is this byte of them. Okay. All right. So so if you compute, if you take a look today at all, are they practical criterion? We have to traverse? This term is the one that is used to control the magnitude running because they match. If we minimize w, we minimize the magnitude of, I guess average. And the limit is 1, w is 0. And the environment around this dimension is one. Basically we're going to specify anything. It will be more wind ensemble machine, well, have a problem. So this is what minimizes the big animals. Then if c is a small quantity, then this will be negligible. And when you optimize this, there's functional. What you will do is basically to minimize w. So if c is small, you will get a low value of h. And under contrary, if we put here, I'm very large ALU, see that this quantity will be negligible. And they optimizer will try to minimize the empirical error. And it will, we will be in an, in an area around here. If C is high, we will get a value of h, which is high. And the dimension of the space or the example the stand. So the magnesium ion, as I mentioned, the maximum value for a manager running, as I mentioned, is 11. But we will pay back to business. So instead of states, here we have C and here is the problem. C is not linear with h. Alright? So at the beginning, when c is small, then they want me to rank, as I mentioned, changes of tension a lot. And then it starts changing slowly. So if you first hear these three arrows with respect to C in linear units, then what you get is something that looks like that. So something like that, something like that. And then they test error goes like that. And since the gloves are noisy, they always are because we don't, we won't be enough time to do infinite branch. Then what happens is that here, you don't see an example. You don't see what you want to see. And this happens around, I don't know. This is one of the many, up to three or five. And so what is the solution for a while? What we have methods to do that? And, and the most useful, useful method is to put here on logarithmic scale for this. But it's not just about the scale, it's about this pacing. If, if the interesting part is between 01 and between one and 10, almost nothing happens, then what we want is to have a lot of values here are not too many values here. So the first step is to choose values for C, right? Values that are that have a low spacing here and here they have a higher space. And this is always a reasonable thing to do. Because if you play the C equal to 0.1 and then you put a 0.2 from 0, 1, 1 and 0, whether you are doubling, they devalue. But right? But from ten to 10 to, you're not doing anything. From a video of C, which is that whenever you see that standpoint to they will produce the same result. But maybe between 0110 when two things, one work the same way. From one to two. From one to d are doubling. Thanks. Alright, so what do we do? This? First, we chose good values for C. So C of k. We have k as the index and they used for, for my veins, right? C sub k. One example that I put this too to take a small value of 0.1 and they got doubling and doubling it. So Seagate will be equal to two to the power k, 0.10.1 times two to the k. That will be reasonable. 012014018116 editions. But we can change this face. We can put some other base, right? Instead of two. We can put the smaller number, right? So instead of doubling, we just, we can go in increments of 1.1. So I multiply each value. Like, Wow, that was, was about, I bet you awake having a warning. So we can put 1, 1, 1. And then so for every value, we multiply 1 times 1, we might get an a which is 10 percent higher, right? So you can do, in general, you get your face value and then you multiply times a to catch. Or you might save so much trouble. I want 100 values between 01 and 10. And then that is, that is a function that does it in MATLAB already in Python, which is called log-space. So you say c equal to log space, 0.111100. Then you have this. And you don't know the basis that you don't care, right? Is 0.1 from 0 to one segment 100 values, and they are normally, are exponentially spaced. Write that in a few. Typically lot of negative two is at that point. You have to put the logarithm in base 10, I believe. Yes, you're right. Absolutely. So it's minus 10. 100, that'll be my example. Write this minus one. It's actually ten to 0.1 and here is 1000. You're a penny. Right? So this is in MATLAB, in Python, you have to go to Mumbai, I believe. It's, you have to do a known by not Backspace or MPI. I love space, re-import, non-biased, and be easy. But then you have to represent it properly because if you just represent see in the y, in the x axis, when you half as many labels here. And then I have already seen. And then this is totally useless because you will have a smoother thing here. I'm just going to be small, right? So then you have to represent things like that. What you have to do is to represent the logarithm of C. So here you put minus one and then you have ten to 0. And the machine has, There's a space from here to here. You will have 100 values uniformly spaced in a logarithmic representation. All right, so this instead of C is log in base ten of C. Simply, when you, when you plot the error, you have to do plt.plot. See the arrow. That's at. C is an array containing 100 values and that will be n. By thought. You're going with my love to. And this law, you have to put mp log, right? You get the vibe of it, I hope. Right. And that you will have a proper representation of the phenomena that you want to, that you want to, to, to see. Say if you haven't done it yet, please do it and resubmit your homework. Buy stamps or that? Yes. The Zoom Room 5. Number five. Yeah. Very similar with the whitespace, but you have to measure the MSE. I was constantly getting close degeneracy. Is that as those challenges from afternoon, everybody else, those are all the same thing. Okay, I still need to technolog and so save a generalize. And I have posted the same day in the fall of my well, the answer in the forum, but I want to see it, It's minimize the error with respect to what are my mean square error with respect of, I don't remember. What was gamma? Gamma? Yeah, guess. So in my experiments, I get something like that. We don't have a lot of Dynamic CRM in that experiment. It's, it's kind of regression. So well, I need to take all of that. And so did others do not force them? Do not force them. The limits and the y-axis, right? Because the effect is rather small, right? By it. And Gamma, you have to represent it in logarithmic units, right? We will see in Gaussian processes that gamma the day, the maximum likelihood value of gamma. The gamma that produces the maximum likelihood for the training data for finding regressors? Yes, equal to the variance of the error. The variance or the by saying error. And this is approximately equal to the radiance of them. We have now the same rules here. It's not the same, the error and then I start the same. But they are same magnitude, same order of magnitude. So that gives you a hint on what to wear to work to explore gamma. Sorry, gamma is very high, then you are here. Okay. What else? My more questions related to that. How would fall? I'm glad to see that the theory is well understood. At least you're able to reproduce. And incidence. This is the step of learning, being able to reproduce the theory. But it's, it's about the paper itself was stuck on some of the interventions. They have a few words only. You cannot you cannot do an introduction. A few, a few words on it. Right? Some people, they spend three pages in the introduction probably is not good either. Trying to bite. I want to see an introduction with a reasonable startup. I propose ones. I propose a structure in which you first start with your motivation for this that are beyond and then you, with your innovation, your contribution, compare with the state of the art. And why are we going to do this? Because it's very automatic. You don't have to think for two months. And you have an effective introduction. Some, some something that people will, will understand. And we're like. But I agree that there are other structures that are also pseudo all, and people will like to, I want to say something, something which is reasonable. So if you don't have inspiration to write your introduction with your arms, Dr. follow that one. Motivation state of the art, and any comparison with your contribution. And then you might say, well, state-of-the-art, manage support vector machines. Why else do we have? Right before support vector machines, what did we have? Almost nothing. And I would say, yeah, 50 years of machine learning, right? And among all the techniques, many, they were linear. All right, So this is what we have to discuss why support vector machines. They supposed a breakthrough in, in machine learning, even from a linear standpoint. First year of the perceptron, that was Roma out. And that's 1950, something like that. That's essentially a linear structure. The same linear equations. So y is equal to w transpose x plus b. And the difference is, they are the differences in the house. I hesitate to explain now because I will probably get it from Alex Alex planet. It's about a particular case of four. I want to, I don't want to use the word degeneration because it looked like it looked like a battery, but it's not. It is a degeneration of the least squares. Instead of the error, we have the sign of the error. So basically, you use, given hyperplane, you'd asked assemble. Assemble is been properly classify. You leave it like that. If somebody has been wrongly classified, then you multiply it times the error, which is minus 1 and you added to w. And so the perceptron with some machine that when jumping is hyperplane until it found a place where everything was fine. So you have two samples and your machine was there. You'll find this, which is misclassified. Then you add the sample to w. And we'll have a machine that pacify. It only works if you can classify everything with no errors and with a linear classifier. So this perceptron machines did not converge unless the machine, unless the problem was linearly separable. And then you again, these are tale about what is, what are the drawbacks of this room and how he constructed a machine with no sun, a lot, millions of wires and mothers, electric mothers to, to change the lives of w. And a camera with 400 pixels, big one only with 400 pixels. And he put images, forums, shapes in that color, right, to classify them. Anyways, it was a great thing, right? But back in 50, right? After this, what do we have? Well, that was the personal role. Then. Some people came with a minimum mean square error, right? The minimum mean square error. And they, minimum, Jennifer was used for the logistic regression. Right? So what they do is to dave, a linear machine, w transpose x plus b. And then this machine, they wanted it to output a one or a minus one. Why did you post? If you build a machine that out with the sign, then this machine is not mathematically transferable. So what they did is to put this linear outward passed through a sigmoid. The sigmoid outputs gives an output that goes between 01. And then they apply and mean square error there. That's a logistic regression. So then we have LMS, right? Ridge regression. Ooh, ooh, ooh, ooh. Pretty regression is used for classification or regression. Logistic regression is only for classification enemies misleading. This is for classification. And then we have a support vector machines on a map and see all this in papers. And then you will have a wide spectrum of methods for comparing yourself with. And this has to be documented with with references. My glasses, my, my slides, they are not original. I copied everything from other people. Right. There's only a few things, a few side things that you can live without that where my contribution to this field here in this class. The rest, I got everything. You need, the original references. Or if they are not original, they have to be very good. For example, they tutorial and support vector machines by works that you have there. Then you have the word threshold goes by burning turbo rank is, and it's about who you are and nobody scientists of our young and she was the first one together with trauma to Paris about this. Well, people cycler but not enough. And others, right? You don't have to cite original references. I have a paper with a very well known British man who is called Shape Taylor. And I ended his flock. He was very famous about kernels for his targets. And so I only talked to him once. I met him at a conference. And we've always worked remotely, I wasn't standard, was it? The UK? So when we met and he is so broad the paper and he said that I cite that we cited because we're offers. We say that his papers he said this. He said don't cite me because when they haven't, this book is not the original. So go to the original sources. And this was sent by one of the most well-known people in the world in so do not cite me. Don't cite, Don't do not ever said my slides. You're going to use them. Of course. You can share them, you can do whatever, but do not cite them because they are under E site. The original paper, some of them they are there in Europe. Right? That's it. In general, they weren't very good. It's very good. But these things are missing. And I want you to run because this is, that will be a very easy state-of-the-art. In some cases, the state has not that easy. All right. One of my students, he is working in estimating what's going to happen with the clouds in 15 seconds to four minutes or eight minutes. And it's totally novel. There's no papers about that. And that's very difficult to, to a state of the art. All right. We will do it with an infrared camera. And so we guess what it's going to happen with the clouds, where we're going to be at how? And this is how practical is. It has a practical application in solar power. Of course, we use machine learning, we use pandemics, machine learning and other things, right? So it's a vehicle for us to put something. Nobody did that before. Alright, bye then when we send the papers to reveal, the reverse of the state of the art is missing. And we have to we have to make hard something. We have to otherwise reviewers, they say no. You don't know what it is. I don't either. But do the FO and we hesitate to say nobody did that before. No, Wait, you don't know. Right? So you have to do research. And data will be an easy way to where in general is going to be more complicated. Of course, when you, you, you have been working for a year in our target, you will probably know everything remotely because one of their research is to read papers, browse and annually or Elsevier, Wiley or wherever. And publishers the police science right? By you. And that'll be it. I have another question here. On Earth, so hey, are you okay? All right, so questions, no questions. Let's start over with kernels, right? Let's say the elements that you need to know about Kronos, not just to put a machine to worry that will be very easy to understand how they work. To interpret what habits, when things don't work, right? Or two, to guess what could be done for the machines to work better. And this is exactly what you want to come up with. Different techniques, novel techniques around cards. So we start with the same old idea of a linear machine. Where W is a set of parameters, is a vector. X is my observation. And why is my decide minus sign up with? So between the design up with and where they get a half an hour. And I need to do something with the other to come up with a great deal. But here, we're not talking about the training criteria. We are talking about how can we do this? So the machine has nonlinear problems. W is a vector, x is a vector and both live in the same space. W lives in the space of the data. And this is nothing but the dot-product. How can we endow a machine with non-linear properties? And at the same time use the same techniques that we have for training. So I don't want to change their idea. They wanted to change anything. I just wanted to, to give this non-linear probabilities and leave the rest and change and change. And that's where does the trick, it's called the kernel trick. And it has several steps, several things that we have to understand in order to fully understand what's going on here. So if, if instead of x, we, we put here a non-linear function of x, then we have a nonlinear machine. A machine which is merely never respect to x. But this doesn't solve the problem. It solves the problem. If this transformation that we want to apply is a vector itself, It's a vector and this vector, it has a higher dimension. And what is the reasoning of that? What is the rationale of that? If I, first, first, I want to put here a nonlinear function. So this thing will be non-linear. But say from above nature monarchies theory point of view. What they want is to have a higher bar meter one I guess, dimension. So the machine will have more expressive capabilities. So what they do is to use a transformation. That increases the number of dimensions that they have here. And this transformation is what they call five. So I Bass Pro Max, which lives in a space of r, of d dimensions, of space, H. With a higher number of dimensions, a vector space with a higher number of batch. And this is what my transformation that we put an example. An example would be two, play with the components, the elements of this vector by computing products between, between components, right? So if x it has two dimensions, then I can construct a vector phi of x, phi of x, which says, for example, X1, X2, X1, X2. And align. Well, the way I put it as a product, that's S1, X1, X2, and X1 times X2. Then a bus from a space that has that nice just to space it has three dimensions plus one I mentioned that has a constant. So for 80 here, we will have survived dimension of 5, 4 plus 1, right? And then phi of x is a non-linear representation of x. So when we compute w transpose phi of x, that we have this W 0 plus W1, X1 plus W2 to W3 x1, x2. So this is the easiest polynomial expansion of this vector in two dimensions. And this is the dot product. And saying that this is the dot product between w five. This is non-linear. Here we have a nonlinear transformation. So we put here a, B once. It's not needed here, this particular case by then, here you have a non-linear estimator. If it's hardly non-linear because it will have this. But we can expand this with products are for the three or four or five. And then we have something which is more than a linear. And what happens is that the number of dimensions increases and increases in a way which is not affordable by. But it is, this is a vector space and this space, it has a dot product, right? It's easy to see that the dot product between two vectors is find x one vector x one times x, x prime. X prime transpose is the product between two vectors. So we have pass from the input space into a Hilbert, into a vector space, into a vector space endowed with a dot product, which is the same as we had in the aim was back to the same. Right? So in that one, the x prime is a different set of data. Yes, x prime is a different matter. Right? So here we have X1, X2, X prime 1 x to the two components. Right? So in this setup, it will be not difficult to do the list to work with this and to, and to apply support vector machine or any other algorithm to this data. And we have a machine which is nonlinear, but we train the machine in that space. So we read linearly, we find w, w 0, 1, 2, 3, and then we'd be, right. So that would be a way to do thanks. I'm in machine learning. The old days people used both data expansions because they were a nice way to apply linear algorithms. But at the same time, you have non-linear properties. But then some people started saying wait, Volterra expansions - they are very cumbersome. The computational burden is very high. We can do something else. And this something else comes with a representer theorem. Is everything understood here? Right? And I put this example, but more, a little bit more complex example in, in the slides, we arrived to three dimensions. Order three, I mean, so we got, I believe that we got 10 dimensions. We put higher orders and we have higher, bit higher, maybe somebody, then things become very cumbersome, very difficult to make to automatize and with a high computational burden. But then we have the representer theorem. We will prove even when they arrive will probably, it's not difficult to prove. Just logic. It doesn't need any, is some reasoning. What this theorem says, that this machine has an alternative representation function on dot products of the data on this machine. y here represented as a sum from I equals one to capital N of alpha i x i transpose x plus b. Says, and this is always possible. So I don't really need w, right? I can represent again find alpha and then represent my machine has a bunch of dot products. But this doesn't solve the problem of the curse of dimensionality. Because this is a linear machine. But if my machine is non-linear, that representer theorem says that my machine can be represented like that. So I still have this annoying. But this is an important point because I get rid of w. And I have an alternative representation in form of dot product. That is the generalized representer theorem by sharp. If I think, if he published on. Of course people were doing this. Thanks. Before the theorem. The theorem was needed. So the first, the first element for non-linear machines is to understand that in order to half are nonlinear machine that can be trained using linear algorithms, is to pass from this vector to a vector of a higher dimensionality using a non-linear transformation. And then the machine is nonlinear, but it can be trained just as before, because in that alternative space the algorithm will be linear. So we use exactly the same. We have W, we have w, but here instead of x, we have 5. That's the first thing to understand. The second thing to understand is that we don't necessarily need this occupation. If the space has a dot product, then I can represent my machines like that. And the third element is a shortcut or the theory which is the Mercer's theorem. Why is this theorem useful? Theorem is useful for us because it allows us to compute these dot-product without needing to compute this transformation. Right? So what we want, It's a nonlinear transformation. Then what we need is a ton of money. So let's find functions that aren't dot products in higher dimensional spaces. But that, that computed using only the input data. So we skipped this step. We don't see it. Right? And this theorem says that IF function k of x, x prime exists, that gives a number, a real number, right? And this function is positive definite, then it is a dot product space, right? So if k is positive definite, then k of x, x prime is above ground. We don't need to compute this or this. And the only thing that we need to get the function put the input data and sudden insight, that's it. And then we have a dot product. All right? And then we're going to have spaces with a high dimensionality, very high, even infinite. We don't even notice because we don't Bot Insight. Alright? Questions. Is everything clear yet? Nothing but alright. So that is basically all the theory. Now. Examples. What are these functions? Well, any function which is positive definite, the definition of positive definite functions, it's interfering with stuff. I don't care about that. No. I just say there is a theorem that says that we have function like that than other brands in Hilbert spaces. But we don't go to the aerospace. We just use the function. The function is a function of x and x prime. Examples are a kernel, a suitable Kernel. Yes, Bx squared exponential kernel that has this shape. And variants of this. You can you've done transform x first and then using a linear transformation if you want, and then you still have that problem. This is about product. And we proved, we will do it again. We proved that this dot product in US case of being 29 atoms. So it has infinite expressive capabilities than others. For example, a and B both are the same of gamma x minus x. Alright? The sink is saying that u is equal to the sine of u over u semisolid function, right? If I mentioned this as, it's because it's shaped like that, right? If you know what about signal processing? You unfamiliar with the same function. In general. Functions at our correlation functions, they are Kirchhoff's. A function that has properties or what a correlation is, a positive definite function that it is a kernel. So another is X transpose X plus 1 to the power p. This is another kernel. It's a positive definite function. It doesn't work too well. It is rational quadratic function. Well, for this, you rather look at this slides because I don't remember exactly, but I think this is one over x minus x prime square. One item 2 plus Gamma. Right? This is a kernel. Any positive definite function. These are the three. They are stationary. Because the kernel, the product depends only on the business side. I don't care about the way we lose the information about, about Dame. Where does the information about data in all of the vectors? X minus x prime? Of course. This is non-stationary. The dot product will be different if two samples with the same business are in different positions of the space, being in space. For science sake answer. So first, first stage and second stage. First, first, second maybe. It didn't add onto this book is stationary. Stationary. Sorry. I didn't understand what is first of all, sorry. Sorry. Sorry about that. Okay. So this three, this, this and this, they are station, right? Because they depend on the distance only. And we play a lot with girls, but we'll do that later. And, and that's it. Right there. In the next class. I will talk about dual subspaces that come with plastic. At this point. I want you to reveal and be able to reproduce the concept of nonlinear. Start the linear algorithm. So we want to start there something which is non-linear with respect to the input. But it still works with that linear algorithm. For the optimization. Other we construct this got right, then the meaning named negations of that represent them. And then the meaning, an implication of themselves. Fearing. Alright, so you do the exercise. You ask yourself, Did I understand this? Will I be able to reproduce these three concepts? And then say there's something left that you don't really get stuck you during the next Wednesday. So Wednesday. And after that, we will talk about What is this concept of your subspace. We can live without understanding that class, right? But I think it's worth to understand this is a very easy concept from our form, our plight of view. All right, so we want to go to, to abstract algebra, then things get a little cooler. But for us, this concept of the all subspaces is pretty straightforward. It will be pretty straightforward. Okay. Any questions? All right, so let's stop here and we'll talk in a couple days. I'm sorry. Yes, it does. That's first-order. Second-order. Or Chairman. Yes. What kind of statutes? In the first few times, t plus vessel changes the video or audio? Does a second-order? Yes, I can I think it was the auto covered. Yes. Yes. Exactly. So the expectation of y, y, y squared equals y and y prime, you want to spinners and intentional. Hello, hello, spite of xt1. Let's, let's skip, be here, times w transpose. All right, thanks. All right, bye. And then this is equal to 5 of X times the expectation of W, W transpose times 5 of x prime transpose. And suddenly this is a matrix, positive definite matrix b, the expectation of some random variable U. And so we have 5 times x times sigma find X. So this is the covariance of two outputs, right? So here the covariance is equal to the kernel between x and x prime. This is about product because this part, this is a positive definite matrix. And say this is the expectation. So this is the exponential of minus 1 over 2 Sigma square x minus x prime square. Here we have something that has covariance matrix that doesn't change, whereas the norm, they change only with a business. Okay? So this is second-order, okay, Garbage. All right, Thank you. And our regressions just mentioned at the beginning. So sides of all to release about two basic concept is authentic for this class. I wanted to and forestry your side of which parties or is not a form of nowhere. Again, I'm sorry. I'm not getting a question. I just wanted to just do a little bit. Can you talk about which parts potent? So it doesn't model trees. It's more work basically. All right, So yes, laborious, three or four. So which part is not arbitrary? Just talked about. Remember clear. I think yeah. I think that the way they sat Let me let me just stop.