 So we're going to do my best for our next hump. Right. But before that, let's see what I explained in the last day on the level of the example in y. So we talked about today, thanks, right? About actually several fundamental aspects of the the kernel trick, the techniques that we use to do that we use to provide estimator with non-linear properties. And so our estimators, they have this structure where phi is a transformation to a higher dimensional space. So we have first this, the fact that in order to have a nonlinear estimator, we want, the only thing that we need to do is to pass the input data through a nonlinear transformation. This nonlinear transformation has the form of a vector in a higher dimensional space. And then we have (1) The Representer Theorem that states that under nonrestrictive conditions, w can be represented as a linear combination of the training data. And then as a consequence, y can be expressed as a linear combination of dot products between training and test data. And then (2), we have the Mercer's theorem that says, if a function k(x), x prime is positive definite, then k(x), x prime is a dot-product between vectors transformed into a Hilbert space with this form. So, if k is positive definite a Hilbert space exists to which we can throw the data through a non-linear combination transformation. And this k is a dot product in that space. And with this, we have all the ingredients that we need to have a nonlinear estimator, which is this. And one example is they're very well known and used square exponential, exponential kernel function. And we're going to see an example. What about alpha? Alpha is related to what we call the row subspaces. But let's not go there yet. Let's go ahead and see the example of a very, very easy example. I remember the I don't know if this is being the quarter or what. Because there is now let me say in the first screen, just a CSV records or it's been recorded but parallel when. Thank you very much for letting me know. But I don't know if when you see when you visualize this, you will see the screens, the whiteboard, and let's hope so. At any rate, we have a summary of what we saw last day. Now, example. So let's assume that we have a very simplistic classification problem, which is the x or the x or y. So we have the plane and for possible sample, sample, for possible data. And so this is one minus one, minus one, right? And the samples here, I labeled it with one. And this eye level, this with minus one. Minus one. I want to classify these four samples or samples that are close to them. So whatever is in the first or third quadrant must be, classify it as one. And wherever it's in a minute, in a second or fourth quadrant, I wanted classify them as, one. Might say, for example, this is here. Here will be plus one's a classification boundary going to be the same, this axis. These two lines, they are classifying. Diversification matters. Because we define the labels as this. If the sample is in the first or third quadrant, we classify them as plus one. So we need something that synthesizes this wheel that we have a sample, which is here. You can see that this sample is closer to this point here, then to the other points. So we want the machine that the tags just like say if I am in the first quadrant, I am closer to this point than to any other. I want a machine that the dexterous. So I want to choose a machine that computes the similarity. The similarity between my subject, my sample, and the others in a nonlinear, well, in a convenient way, how do we do that? Well, we can choose a nonlinear kernel function to the back. So when they do is to construct a machine that has this operation, the exponential of x. This is my resident, this, right? Because I want to use x for my samples. That's x, that's my sample x, right? X minus x one. X one is a vector that lives this battle that leaps in the first quadrant. This is at one, this bullying X3 and X4, right? So this is 1 over 2 Sigma square x minus x1 squared. This is a function I didn't finish to two, right? Right. They didn't finish the right date, the expression of this. But with this function f x and as x one, they are very close. This distance is going to be 0. This exponential is going to be almost one. Right? So this exponential function will increase its value as I get closer to two X1. And if I go away for next one, it will decrease, it will tend to 0. So if a sample is close to X one, I want this function to say I am positive and hide. Typodont was here. And if I sampled here, then, then, well this distance is high. So the distance, the movie so high that this exponential will be negligible. Theta constant. I need to use another Gaussian or another exponential that tells that this sample x is close to three. I always forget the minus. It's painful. So if my sample is here. Then this exponential will, will say, Hey, I have high value. This will say I'm almost 0, ignore me. And then in my sample is here or here, then this distance will be smaller than n. So this is the Gaussian or the square exponential that I want to use. This exponential wants to say I am negative minus the exponential of the same. And do that for the four quadrants. And said. So if a sample is close to X1 and X3, this function will be positive, right? And if my sample is close to X four, or X to, this function will be negative. And this is how this particular classifier works. And now I can write this in a more compact way as the sum of alpha i times k of x, x i. And I need a bias, but in this case the bias is 0 for I equal to 1 and 12 forth. And alpha I, alpha is equal to. Let's construct a vector which is one plus one plus one minus one, minus one. This is my vector. And k is this exponential evaluated at x minus x i, where x i is from one to four. Understood? This is what a curl machine that's in classification or regression too. But this problem is just classification. The easiest question, sir? Yes. So I guess it doesn't matter what order the positive and negatives are on the author. In x1 is positive, X2 is negative, X3, X4 negative. If it is this important that it is an agreement with the labels that they chose. And since X1 and X3, they are samples that are positive, they are plus one. Then the corresponding kernel functions, they have to be positive in this particular case. All right? And the same for the negative once I put that minus sign here, because I want die. Or someone which is close to x still produces an output which is negative. Or if I'm close to X4, I produce an output which is negative. Okay. How does looking at the alpha matrix that you drew here? Yeah. Yes, of course. So this relate to the x one, corresponding to x one, alpha, one with a darker as they can the second, third 1. Switch the signs. This oh, yes, you are right. Okay. So you're absolutely right. So this is minus this is plus. Okay. Thank you. Thank you very much. Yes. So this is 113, they're positive. 12. Etiquette. Second. So you are not doing X1, X3, X1, X2, X3, X4, right? Yes. Yes, from I from one to four in this order. So x y is positive, X2 is negative, X-rays positive or negative. So the closer it is to one. For example, it will go to 0, 3. The large number of cryptosystems. I'm trying to think of what you said before about the distance. So if it's closer to one, then it will float is 0. The distance will go to 0. And then it goes to the three would be large, so large that it has to be a positive part. That's Sarah, if my sample is here, this distance will win. So small and so close to 0. And then this exponential will be the exponential of 0. So one, right, because of this minus sign, for example, of the same sample, will produce a different name here, this distance is large, that's high distance. So here we will have a higher number. Since this is negative, this exponential will be closer to 0, right? So the exponential, either 0 or large or verbal equivalent. Sequencer, either 0, the next pencil or a large number, we'll equivalent to 0. And external setup. So if you block 0, you can see where the exponential of 0. And if you make it go to the extreme values, the entity and London will be exponential. Because the, if this then this distance is a high number exponential 0 when the distance is 0 and exponential density one. Right? So this is a function that will fire, you want, it will say something. If it has some sample cluster, it I know sigmoid function. It's not sigmoid. There's also the classification by one day, but this is fun for that matter. Well, that is another topic we could be talking for three months. By this is, this is a square exponential. Sigmoid comes from the word Sigma, which means S. Yes, right? So sigmoid is an S shaped function, is like that, right? Well, it's hardly unless I know, but this, So, and this is related to a probabilistic interpretation of our classification, right? But here we're not trying to do this. We're not trying to that this is a topic that I will explain and mechanized by just, let's think of kernel function, the sigmoid, it is an activation functions. And it's not exactly related to this, which is a kernel function, right? A sigmoid is not, in particular, it's not a product. It's not positive definite. So this is okay. And so this is basically what we want. If we have a sample close to two ascender too. One of this, then they, they, exponential will be high. Right? Fire File and say, Hey, I have some of us, me. And so if the label is one, we want this to produce a positive response otherwise and the other part. And this is what we do by weighting each one of them. Or the kernel's functions with this rebels. Right? And that is the way in which simple classifier symbol kernel Bayes classifier works. We might, we might need bias for in order to introduce, in order to get the desired response by running this particular thing. So, but what did we do here? Well, when we did this to pass this data into an infinite-dimensional space and then follow them that day. The flame inviting infinite-dimensional space. And then we compute the dot product one-sample x with this four points in that universe space. And this is the equation of the hyperplane in that space. Let's see the example. This example. Again, by using my computer, I am able to, to join the session and arithmetic. Okay, if we, if we have artificial intelligence and in our lives, real one and I will set computer connect to whatever you need to do it, right? We have an exact form, right? This is not the variance. It's ironic though. It's one to begin. A conversation opener does not even know. I, I, I don't suppose the listeners. Do we mean? All right, so now let's see if I can share screen. I'm going to share the desktop. So let's say again, just so you know too much about this. But we have here in bags. We have here a set of hello. So this is my before. This is my four points. 111 minus one, minus 11, minus one, minus one. And then what they call labels here is actually alpha 1 minus 1, minus 1, 1. So I'm sorry, but I have that, I have everything a different order, right? I gotta fix that. So that will be minus 1, minus 1. And then this is going to be minus point line. Ideally. Of course nothing's going to work now. And so this is one. Alright, so I have now the same set unbiased in the, in the whiteboard. And I choose my Cornell as they, Well, it says RBF radial basis function. This is why they call this query exponential. And so the rest is just computing this function. This function for the whole plane between minus 22. So what I'm going to do is to compute the value of Y for all the points in this plane. And let's see if this works. If I, since I change something, of course, some of our voice. So here we have two different graphs here. Let me see. We have no artificial intelligence. And then the people that decide the things, the rules and everything, they do not teach or use. This rooms. And so it's totally inefficient anyway. So we, now, I have this function here. They want to move a little bit for you to see it. All right? What is this? This is the value, the kernel about the estimation y between minus 22 in the plane. So if we are close to 11 here, weren't close to the point 1, 1, the function is positive. Or a workload. They use it of course and say Here's where it was two minus one, minus one, the function becomes negative. Positive, sorry. And these other two points, the function becomes negative. Why? Because in this point, Let's stance a little bit. But since a parameter here, let's put 0.6. I will explain this later. Let me see if I do it again. Nothing happens. So because in this two points around plus 1 was 1 or minus 1, minus 1. Here, around this point, only one x squared exponential is relevant. The others, they are negligible. They are close to 0. Right? Here, only one. Score exponential. The one that is close to 11 is, is it has a high value for y. Again, I can represent this in a different way. So if I only represent this exponential, if I only represent this exponential, then you will see that it is only non-negligible around 11. Let's see if I can yeah, same n out of it. So it doesn't count. Doesn't count. So if I present, but they have again, this is I and this is the contour plot. This is the same value. So these are going to blood of the failures are there, are, they, are these four square exponentials together. And here we have positive values. Here to, here we have negative base. This is how it works. But this is something some particular case that we can make. But it's not difficult to construct a classifier that works for any distribution that you have. My hand. So you give me a set of points, you put labels. And you want a classifier that's able to classify that data, for example, but you can do route. So if instead of having this, what I have is what I have is a set of points placed arbitrarily. Like that. I can always apply the same trick. And then put Gaussians or square exponentials around us. And be able to classify the horse. I have to choose the y of the exponential. I have to choose the white of the exponential has to be small enough. In this case, it doesn't matter. Another example repair company, the same. Here. So now here I chose an exponential, exponential that, that is very wide. So this is barely by non-linear. Remember that when I, when I represent that they bought their expansion, well in this case they, they Taylor series expansion of the Gaussian, another Gaussian discrete exponential. The first term is two. Right? Here we have a linear combination of functions that are all bonding order to. This is why we have something that looks like a saddle point. Dry. So this is parabolic or approximately parabolic in this direction and then in this direction. So here we have a Gaussian which is very wide. So it's, it's not difficult to approximate discussion by a second-order function. That's what we see here. But we have two different secondary functions that aren't in opposite directions. We increase their wide. Tell me that thing that work. Yes, it does. You can see that the function takes an aspect that is clearly closer to a Gaussian. It still works, right? It still works. They, this points here. They are the points where the function is 0. And this, by the way, they are points that belong to the classification boundary. So they are, they the image of these points in the aerospace? They are in a hyperplane. We can keep going. And it still works. Isn't that cute? I still worry. Alright. But of course, that works for this particular case. It can go in. And there is a moment where you live. Yes. Yeah. It looks like a nail, right? Right. But keep working. But if you increase than the precision of the machine won't be enough. And there's lines, they disappear. They disappear because the machine is our day was the popular. They've been there, they are still there. This always works until you have delta functions here. Well, this one, B delta functions because they, they, they are integral is not one. But you understand what I mean, right? So this gets thinner, but this binder is still here. So we could do that thing with any arbitrary distribution of data with any number of Beta. And this is why the capacity of this particular machine or this particular destructor is infinite. No matter how many they die have and how many. And where the labels, I can always classify them probably. Right? And we know that since the beginning of machine learning's, It's just now we have an alternative interpretation of what is happening, which is a little bit more abstract. It's in terms of, of Hilbert spaces. So this, what we have been doing here. This, yes. My name is Alpha k of x, x plus b. In general, this is represented as a linear combination of the products. Where this comes from. The fact that w is equal to alpha i phi of x I will fight is a non-linear the inflammation of the data into a Hilbert space. And then this W, W, which is alphai phi of x i, is a hyperplane. It's high in a space of infinite languages. And that is a tool that we use to say, Hey, this is definitely positive definite. My algorithm is linear and my solution exists and is unique. So I give you seeing the same linear algorithms that I was using before. And why instead of using this, we're using this one. These two are equivalent. And this is not useful because here we have infinite dimensions. Here we have n dimensions, because n is the number of Beta. And here is where DO spaces come into play. But we need to know a lot about dual spaces. Basically, ideals of Satan's or subsidies representation of f of x is the following. So we have the sample packs. And this is the input space, which is on the, we have a sample of something that we can observe. And then we basically pass it to five of x. This is feature, it's called a feature space. And this is my Hilbert space. This is implicit. We implicitly pass from the input space to the feature space. But this we don't explicitly do. We don't know how it is. We don't care. Alright? And then that's first as what we have. This is implicit. We don't do that because we don't write too. And if today Duo some space, what is the dual subspace? Well, we find f phi of x a dual representation of this, yes, k of x, which is equal to the dot product of the sample with another set of samples which are in the training samples. So phi1 of x, 1, phi of x down to Phi of X and Phi of x. So we pass from a vector that has infinite dimensions to a vector that has n dimensions. So this is simply a vector in n dimensions. Right? Well, let me, before I like this, of course, we don't do this directly because we don't know, we don't know the transformation. We just do this. K of x one x, two, k of x. And if I have my kernel function exponential for example, I can do that. In the example that they lie, they erased. I have four samples that I use. I can tell they are training samples because it's a made up classifier. But a bus from a space of two dimensions into a space with only four dimensions, which are the four components of that sum. As k of x one with x two k of x for which x, for exponentials in that case. In general, we have S. Then in this space and construct a hyperplane that classify. And this hyperplane has parameters Alpha. So my classifier, that is originally y equal to w transpose x, phi of x, or f prime of x plus b is now when I pass to the your space, y is equal to alpha transpose times k of x. Is, is. These are beggars plus b. Bye. Bye. Usually represent it. Like that. It's the same. In order to construct a non-linear classifier, I pass from a space of the dimensions which has, I see Mumbai observation into a space of n dimensions. Where n is the number of training data. And this is what we call ideal space or at your subspace. Because they're private. Space is a Hilbert space. It's also our feature space. Is that the dual space preserves the properties of the primal space. So whatever I can do here, I can do it here. And this is by virtue of the representer theorem. Questions. How are you using the SVM lib? Because this function is bijective. And find in the lab or are you using ESPN live for for for RPF? Know. I'm using I'm using this function that we wrote my colleagues and I in 2008. So I wrote the first version of this and my friend wrote is probably the best. Now here's the best one non guy and remote sensing. So we wrote this is stupid function that computes a linear sigmoidal polynomial and RBF kernels. That's it. My do you have my Spanish monarchy, myset function, optimize functions to do this. Alright? So what we do here is we take all the data, we compute the distances between vectors, and we will then we have rights square exponential two. That's it. And this function, you have it in. It's available for you to use w memory. Have I, if I have a, if you have available to them the corresponding Python function, they can code it. So I'm not using the SVM live for that. And actually we did this because it is possible to use the SVM package without using the kernels that it has internally. And we do this because we wanted to construct our kernels and so on. And so inputting the data when you though is to compute the kernel matrix. And this is what your business, your inputs, right? So you can Use the SVM live with any kernels as long as you recompute them. But I don't recommend to you to use SVM. I mean, I don't encourage you to use MATLAB. You can use Python. I think it's much better. You will learn about all the deep-learning optional or exotic places as free. This distribution, right? It's available. Bang for my buck as pretty expansive. Because they programmed a communications for the, for Boeing in MATLAB, Right? So they pay big dogs. So I want you to understand and to be able to reproduce, to be able to play with this kind of things. And I don't think there is any mystery here. Just to stay your time to think. And we're going to do the same exercise for brush. Here we are not talking about any different, any algorithm or any criterion. It just stutter how this works. And seven is this is a linear, a linear factor. So we can apply the same techniques and forth when you dolphins. We want to use a support vector machine. We want ridge regression. We can't use cell. And I'm going to finish with this. You want to, you want to use regression for that. We have Y equal to alpha transpose x plus b. And so let's see, this is an estimation. We have an error either. And we want to minimize the error. When you can say, well, let's, let's minimize this error. Regularization term. Compute alpha from this criteria. Right? Basically, what they want to do is to minimize the squared error plus gamma w squared. I have to minimize this, right? W is not here. I have alpha. This is the criterion. I cannot change the criterion. Otherwise it's not going to be ridge regression. So ridge regression is mostly used for regression, not for classification. But here in this case it will work out. After all, I want to minimize the squared error. So where do I go here? But now, what do I name a theorem? A theorem at random. One that you can remember. The name of the theorem does not represent. The theorem. Representer theorem says that w is equal to Phi Alpha. So w is a linear combination of the data file contains from phi of x, y into phi of x. Okay? So this alpha i phi of x. All right, so we have this representer theorem. We're going to use it in two ways. First, I say okay w is this, no problem. Then w squared, which is what I have here, is equal to alpha transpose phi transpose w transposed time w. And so this is this. And so w transpose a is alpha transpose. Fine. I suppose we find out, and that's it. I have found out by this. Or do I get rid of this? I don't want this because it's in three-dimensions. What do I do? Your thing? And sometimes it's not permanent. So you can name another theorem that says process. Yeah, yeah, yeah, that one. So this is all the vectors in, in rows. And here we have all the vectors in columns. This matrix is the matrix of dot products between samples in the Hilbert space. Right? And we know that in this universe space, that is dot-product that we can express as the inputs. We have k. So this is matrix k dot products between data. Oops, right? K I j or the entry i j, K is equal to this function that they all, okay? And so then my, my ridge regression is the minimization on the MR squared. The expectation of the error, sorry, the expectation of the error squared plus Alpha transpose Alpha 0. And since the expectation of the error square can be approximated by the sum of the squared errors, which is this quantity here, alpha transpose k of x I minus y, which is the actual value squared plus Alpha transpose K alpha. Now, I can compute the derivative of this expression with respect to Alpha antenna and obtained from Ryan, I compute the gradient of this expression with respect to alpha equal to 0. And I can. Hi, sorry. You can do that. We did that in with our deal. We can do that with Alpha because it's going to be the same. Like I said, this is the first way to compute the ridge regression algorithm in with alpha. That is another way of using the representer theorem. Yes, it is an extra alpha. I'm sorry, this is alpha. Right? So This is the first way to use that representer theorem. There is another way. Wait, we are really compute they algorithm with alpha and we know what our face. So let's use the representer theorem in a different way. I see hobbyists this in which they refused to the representations in the primal space. So they saw everything using only that your specs. Right? Because we have everything Soviet time and space solids just transform. So the second way, the second way to do this prints the following. Second way of using the representer theorem is saying, We know that w is fine alpha, right? We know that w is equal to find phi transpose gamma I minus 1 phi alpha phi one. That is a solution, right? Of the ridge regression, except that we use at here, right now we have to use Firefox. So matrix find. Do you all get the point? That's it. And so now we can say Alpha Phi Alpha is equal to 5. Y transpose was gamma i. 1 is 15. Why we manipulate to isolate alpha, that's it. So we pass this to the side and we have 55 transpose phi alpha plus gamma i and phi alpha equal to five prime. And then I can eliminate this. This is my alpha plus alpha equal to y. And then if I multiply this times another identity, I have this. And then finally, because we read that this is the one that's faster than computing, computing again. And you get the same result. And these are the reasons for which we, we just go on, we go the intuitive explanation of how one of these machines work. Because if we use that represent the CRN numbers SUM and, and they representation in primal-dual spaces. Then we can do things like this. Things are actually easier and clear. By the way, this tray theorems, while we're used to here and had a big deal. So let's think about this. Let me rub up with an outline of things to take care of. That one. Yes. Again, one. Let's try. My machine is always this one, 5 x plus b, y, etc. And we have the representer theorem that says that w is equal to, well, here I said fine. Alpha. Or if you want the sum of alpha i, phi of x I. So my machine is y equal to the sum of alpha i phi transpose X i. Or if you want. Let's use this notation 3 and 4 for the primal and dual representation. Which is basically this, the PyMOL representation as this primal. And this is a duo and m vector spaces. This is the primal space. This is a vector in the primal space. And this is a vector in the dual space, which is the dot product between x one and x two x and x. And with this representation, we do this for the master theorem allows me to construct this. So Alpha transpose k of x must be two. And that's it. These are the four things that you need to, to assimilate to let them go in your minds. This thing, fourth thing, you have to connect it to the example. I would like for you to download it. You're going to use Matlab for free because your URL. So do this exercise. Being able to reproduce all of this. And let me know. And we will keep working this up. All right. So.