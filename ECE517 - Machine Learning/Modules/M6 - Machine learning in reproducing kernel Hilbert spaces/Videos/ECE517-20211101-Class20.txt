So before we keep going with Hilbert spaces, I would like to talk about this homework where you have to put together a regression. Celebration methods. Using ridge regression support vector machine epsilon super. Useful for a question. What is the problem with a homework and do something? In that case? The run is that while there is a design behind all this homework for you to learn several things that are important for it's one of the molecules. I don't explain it. So for example, here, what is obvious is learn how to implement regression methods, linear regression methods. But also this exercise is designed for you to learn how to practice, how to do cross-validation of parameters. Which is something that we only implicitly did in the previous exercise. In a way which is not practical, right? When you sweep C in order to test whether how much you get mostly permit shebang this theorem. We do sort of validation. What is not here, where they want you to do is to take a given set of data and the number of samples is limited. And do a training and do a cross-validation in order to find adequate parameter. So for ridge regression, the parameter is simply gamma data visualization parameter. Exercise correctly, you say that there is a minimum by S, minimum is not probably two to four. It's not dramatic, right? But you see that for gamma equal to 0, you have an error, then you increase gamma. When it gets close to 0, 11, I believe. I don't priority. I don't remember which then AMF name the number. So you get close to the optimum number, then you see that the error, the validation error decreases and that it increases, it goes away. By, I don't ask you to do the same with super vector machines because it's important the machine we have different parameters. For example, in epsilon super-resolution, you have two of them known as Obama. So if you do the same exercise, I just think of any Obama for fixed values of C and epsilon. Then you, where do you get is something which is pretty much constant, right? Super vector machine has a different way of beef and behavior. So what I mean there is for you to just see an epsilon. And how do we do this? It will have number of parameters. We have to do is to sweep them at the same time. Or in the case of two parameters. We have. Let me see. Services vault. So epsilon. So far, we have two parameters we not seeing. And it's not valid to choose c and then three epsilon then choose the optimum value of Epsilon. We see that doesn't work like that. When we have to do is to create a partition of the space for C and for D epsilon. For example, extra volume from 0 to a given value of C one, for example. And then see gamble between 0.1 for example. Let's put 10 to minus 3, that one. So tests. And we have to go to swipe our spacing, which is logarithmic. And same with. So that can be, for example, pen for minus three. And here it was ten to 0. That is, from a very small layer, 2, 1. And then we choose a given pair of values. Do a training and do a cross-validate, and then do a validation. All right, For c by epsilon k of these grids. We first do at frame. And then to do a test with validation, set and store the error e I, j. And we have to do that for all the value that we have here. So we put 10 here and 10 values here. We need to know 100 tests. If you do a representation on the error as a function of epsilon and seeing that, you will see that there is somewhere, somewhere there is a minimum. And these are the two layers that you have to choose for C and for Peter. And what I saw is exercises where you cross gamma. Now, there's no grandma here, but there is a value of gamma that is irrelevant. First, exercises where you choose a value of c. You cross-validate epsilon. You take invalidate our value of c. You are currently on our Epsilon. This is not correct because this till they are two arbitrary. So this, for example, you choose a value of c and corresponding epsilon. You get this. And then maybe the minimum is here, right? And then you evaluate gamma, sorry, UHC, and then you get this. It might be the case that your vagus that he goes to the author. But if you chose this, then you will probably end up with a solution which is not. Right. And this is the difficulty of parameter cross-validation. If you have too many parameters, then you have a space of many dimensions. If you have three parameters, then you have to give in and then you just tiny intervals reached. You have to do 100 thousand runs of this, right? So in general, for the evaluation, there are other methods that are faster. For example, a very well-known simulated annealing. And simulated annealing. When you have half, is public speaking or write papers as you choose values of C. And then epsilon in this particular case, at random using a given distribution, for example. So you put a Gaussian centered somewhere, then you will get several values. Instead of having to sleep. 100 value as teachers to 10. And then you choose then you, you take Dave, best results. For example, here, here, here, and here. And using these four points as a mean of a new distribution than you choose a random. Several other males around here, here, here, and here. So for example, 20 more. And then among all these values, 1, well, if you're lucky while we close to the minimum. And then you can do that as many times as you want. In this case in two-dimensions. Well, you won't be too far away from doing it 100 times or even more. By the accuracy that you will get will be mites. Much, much better because if you have ten parameters for it's one of the 10, ten barriers for each one of the parameters and you just all of them. You will end up here, this will be the closest. But the teachers values at random using this simulated annealing, you're, well, we'd probably close to the actual mean. So you then choose what values in a provision, but you do it. That is why we call simulated annealing, right? Because it somehow similar to what we do when we have to anneal a piece of iron, piece of steel, right? We go smashing them. And they've atoms of the metal they reorganized. So first we will add with the metal in a very hot mantle, right? And, and you, you go smashing the same but lower than that. I don't exactly know how it works, but it's similar to this. So there are others you can use. These are heuristic methods. You can use genetic algorithms right there. For example, you can use a method which is called particles ones, because ones or many others. I'm not going to list all the methods to do these things by just sweeping. Integrate is the most basic way. You'll have many biodiverse than you wanted to take a lottery. To more sophisticated methodologies to adjust that. We will do things in a different way in Gaza protests. In any case. I just think parameters, consolidating parameters is it's an hazard is very heavy computationally speaking. And this is one I want, I want to explain. Right? So if I put a note on your homework saying, well this is not right, please try to do it again. So you can finally see this. Of course, this is something that you see when you have two parameters. For. Maybe the task is not too heavy, but dependency. But you will have Ira, I'm a four-day mentions and here instead of through four dimensions. And if you do this for, for a set of four parameters, then once you have this matrix or this may feel, then you will be able to find the indexes that contain the lowest square error or say you have to look at rough, just using our right. Some people call them answers. Right? When you have matrices of order two dimensions I taught and our tensors. I'll call them our rights. So that's it. I don't know if you have any questions. People in the room or remote audience. Yes. So for the numerous be or would that be three variables or these epsilons new? Yes. Neil replaces epsilon and new values between 01. And you can easily do a linear spacing and assimilate and millenium will be much easier to write. So it's kind of difficult to those. Similarly, there are million with given to them, right? But you see modelling with the parameters space, nobody read it, it's in it. You need to do it carefully. Alright. So that's it. That's all I wanted to explain. Please redo the homework. It's it's not a lot. It's a lot for the machine button for you. Please give me an explanation of what you do and an interpretation of the results. And again, I still found Coates writes in your homework list, I don't want code, I don't read it. That choosing a right triangle, I don't have time to go through the poet. And it's actually not a good way to explain to your instructor or else, alright, so let's go to the IRI if you have any questions about hybrid spaces, about what I explained the other day. It was aimed towards summary of the things that are important that you have to retain, that you have to be able to reproduce at several points, several sentences, summarizing everything. And I just wonder you to review this for three or four bullet points for you to check whether you are able to reproduce what I sat there. And if not, let me know. And then we can just go back to the economic construction thing. We've talked about the Master's theorem. Let's go back to the master theorem. Try very hard to learn all this stuff. This theorem is sort of our shore guns on the theory that involves human spaces. And it's fundamental for what matters to us, which is kernel methods in machine learning. And it's the key idea between this, what we call the kernel trick or the brother all kind of fade in some papers on the Missouri State as follows. So we have a function like this one that maps a couple vectors that live in a space of B dimensions, maps it into bar. Roadmap, maps them into art. So we get two vectors and we get a number. This is a bivariate function. And sorry, this function fulfills this completion. Which is that the integral for the domain of x of k, which is multiplied times pre-modern times f of x and post-multiplied as f of x prime. If this integral is positive. When f is any function which is square integrable, then this function is a dot product in a space that is which you can reach by a mapping function. And then this mapping function with this mapping function of a kernel and especially the government had become stuck in the form of a, right? So if the function is positive, if the function k is positive definite, then here as base excess. And this here will be reached by using this nonlinear mapping, the kernel, the kernel function is the dot product between two vectors times branch conforming to this space. So basically, we do not need to know five. And in many cases, this phi is not explicit, right? And in some, some books or some neighbors, I learn, I read the expression key then Hilbert spaces. When this expression phi is not known, we call this Hilbert spaces here. And because we cannot see inside, right, we cannot actually do this operation. There is another theorem which is also of importance to us, which is that representer theorem. That says, as a court of law, it says that any machine learning or Sandy or can be expressed using just using dot products. We don't need. Then this transformation, we don't need to see the space. And this is related as well to the dual sustains representation is or are the things that I explained earlier. So this is why this theorem is of paramount importance eyesight that, that, that theory, I mean 30 papers for the papers online, maybe more, NO. Probably all the neighbors that use Kronos III scientists there. So there is a proof theorem. There are a couple of papers that are cited. And one is this, the Iceman. Iceman, which has 60 1964 paper. But the original one is from 19 no, to, I believe it's from, from this mathematician British population. So the interpretation that we keep the theorem, it's also important. So if this is true. If pay is positive, then we're going to sample version of this, of this integral. But it's going to be also positive. So we will do is to we, we sample, we sum of x and x prime, right? We, we take a sample of x and then this integral becomes a summation. And so since f is square integrable function, that in particular means that the function is, is bounded. So they, they function f applied to two this samples, X i becomes a sequence of numbers. Any numbers positive, negative, they can be a, we already can be complex. But no matter what this function is, then this sum is positive. And so this expression, we can rewrite it like this, where alpha i, they are the five functions f of x evaluated, f evaluated at the point x bar. So we have this, finally can be written in matrix form like this. So here we have a vector containing all the functions f, x, i, k is the matrix containing the dot products are these evaluations I'll pay over x, I, and j. And then this is positive difference, have this positive. So that means that this function, this matrix gate is positive. All right? So this is our matrix that has eigenvalues are non-negative, they are real and they are non-negative. And why is this important for us? Because we use this expression in our, in our optimizations, right? So the optimization methods that we have seen, all of them in support vector machines and using mean mu, mean square error or using Bridge regression. They, they have, this was a linear function of alpha. So this is a quadratic function of alpha. And then we have Beside some linear function on, off. So when we optimize, when we maximize or minimize this function ofs that container quadratic, bi-linear. But we know that the optimization minimization or maximization, I don't care because we're solution and the solution exists. Right? So our super vector machines or ridge regression or wherever, they are, algorithms that have a sperm that have a resolution that exists, right? A single solution, the solution always exists. Okay? And this is something that you 1.5 if k is not a dot product. So you can say, Hey, you know what? I'm going to use this nonlinear function of x and x prime because I1, this particular behavior for my machine. Wherever you can still do it, you can still use the same methodology with a function k which is not positive definite. The problem, the only problem is going to be that your solution is not going to be using. So you will have several solutions or infinitely many solutions. They might still work, right? But the optimization is going to be more difficult than the criterion is not very clear because you cannot express it as a linear, a linear algorithm inside a given space. If you want a linear algorithm, it's angular space that has non-linear properties. This matrix has to be positive definite, so this function has to be positive. And so of course, if you use other methods like neural networks, this doesn't happen anymore, right? In your optimization, you and others. What else? Okay? So they said, once we have defined what a corner is, a process currently is that product, then there are other properties that Robert is not list that are interesting for us. So the first one is what we call that exon of spaces. It, we have two functions, k1 and k2, that we know that they are positive definite. Both are dot products in a Hilbert space. If we consider a linear combination of them. And we make sure that this linear combination. Positive definite, then this is itself a dot product. And the only way that I can make sure that this is positive definite is an a and b are positive scalars, right? Real positive. So this is easy to prove because we want a kernel matrix K constructed this function k to be positive definite. So this quantity should be positive for any values here in alpha. And so we construct this matrix as this linear combination. We have is two, since K1 is cornell, this one is going to be positive, this is going to be positive. And then if we want this to be positive for any value of alpha, that a and B should be positive. That's it. And that is a geometric interpretation of this, which comes in the next, love, this one. So adding together two kernels are constructed. A linear combination of kernels is equivalent to concatenate two spaces, right? So you have one huge space here and then you have phrenology was basically you put them in, you put them are orthogonal. Right? And which is equivalent to concatenate the vectors on the first space with the vectors of the second space. We multiply each one of the vectors times the square root of a and B. This one didn't. Here is exists and is unique. If and only if a is positive. If a is negative, then we have two variables in this world with a 0, of course this pathway. So if we add these two vectors and we compute the dot product of another two vectors, construct it the same way. Y holding x and z here, that was formerly x prime assembler, right? So what we do is this times this plus, this times this that we have here. And this is one of its products. This is the origin, right? And this is why this is called direct sum of hero spaces. This is one way to construct constructed pronouns. So we can, for example, think of one process that has non-linear properties at two different levels. So we, what we do is to represent this non-linear properties using to these carbons. Or we might say, my process is almost linear. It has some nonlinearities. Well, let some linear kernel here to the linear problem is, and then funnel lineages led the machine figured out through a non-linear curve. Right? We can do that. Or you might say, I know that square exponential Cornell is birth, but I don't know which one. I don't exactly know. Gamma, sorry. Sigma has to be one or 0.1. I can put the three about where three different parameters and then cross-pollinate this time. And the machine will tell me, well, kinda number two is good, carbon number 3 is also good Quran and one is not. So with using just cross-validation. But I can also multiply kernels. If I have two positive definite functions, the product of both is also a corner. And proving that this a lot more cameras on, but let's try, right? So I have two matrices, K1 and K2, that are positive definite matrices. They have the same the same dimensions. And then I want you to be stance or dot product between both. So here what I have is in particular this scanner product. What they do is to multiply each one of the elements of pay one by the entire matrix K2. Bank. And 7, if this has n elements and this has n elements, this disparity and my exam, this is n, and this product is going to be n squared times n squared. Right? So I will have a huge matrix. This matrix is not used for, for us yet. But so given the properties of this product, and so that's the answer. We know that the eigenvalues of this product, they are all the products of eigenvalues of matrices. This is not difficult to write publishable. So if this paradox, if this matrix has positive positive eigenvalues and this has positive eigenvalues. These metrics will have positive eigenvalues, and this eigenvalues will be, They all, all possible product between two vectors of each one of them. Right? So from one to n square eigenbasis, and then this matrix K, then this matrix a will be positive effect. And now the trick here is that they, they, they, this products, K1 times K2, or x and z, and x and c, right? This products, they are inside this matrix, right? Because we multiplied all elements with all payments or anybody wanna we map is inside. So if this is positive, Let's choose a particular vector, Alpha, where that has zeros. Place it, place. So when we have our product, which is not between a couple of vectors and the same vectors, then the result is 0. Right? And then by doing this with ways to eliminate all the products except this last, this matrix will still be positive. I mean, this product will still be positive. Because this matrix being positive definite, that means that this is positive for any factor alpha. So for our vector that has a 0 around, it's going to be positive. So this is what we call alpha. This is our medical and alpha n. If we take all the zeros out, we have alpha prime and h will be the product between the two matrices, not the tensor product, but the element wise. And that's it. That is the proof. So if you multiply two positive definite matrices, that result is a positive definite matrix if you multiply element-wise, element-wise, right? So you have a matrix K1, which is K11 to K12, Fe to one and k to two. And then we have K2. Let's put can't give pump A11, A12, right? They product between k and k prime with women, with his product S. Let's call it o dot. This is an element-wise product. This is a matrix that contains elements K11, K12, K22 to 181 prime indices. So element-wise. So this matrix is positive and is positive that this matrix will be positive. This is a sub-matrix on the tensor product matrix, which is positive. That is a proof, right? So for example, I can multiply two. I can multiply two dot product, one being linear and the other one being squaring function. If I multiply two exponentials, well, I simply get another spike threshold. For this particular case, it's straightforward to prove that the product is right to be positive definite, but arrogance is not that it directly. But don't worry, if your products, starbucks, a positive definite function, the product of them is a positive effect. It's not that easy to see what happens in visual space. But if we do the dot product of two matrices, then we get a different space, which is a space that whose dot product is a bunch of all the possible combinations between groups or between the data, right? We're not interested in any of them, except of a pair of data with itself. This is why we call this tensor product. It's not a tensor, is not a tensor product with the way the tensor product of some more properties. We know that this function, it is a carnival side. You get a function f of x, and then you multiply it by the same function applied to z. This is another path instantly. And not difficult to prove. Because while here we have some knowledge, but let's still let's not do that. Yeah. So as I said straightforwardly, this is also, if we construct a kernel inside the space, this is not productive. So here, instead of having k of x and z, we have k of phi of x and Phi of Z. So we get into Hilbert space. And instead of computing a dot product there, we apply a kernel that is equivalent to pass into going to give her space and from there to a different universities. Right? So for example, again, construct expansion of that expansion of, of our sample data and enter by a square exponential orders. This is also a column, right? So we only, we do not have kernels in the input space, but we might as well have canals inside the Hilbert space that would be equivalent to go into human space and then to that capital I never needed. So another problem is it proved to be positive semi-definite matrix. Then this is about product. This is a kernel. And as an exercise, determine in what cases. This product that has a product that is defined in two spaces, 12 spaces. One space. And as faced with the two. In what cases? It's not worried about this. Well, in what cases this product, basically what this is, this is our path. So I'm going to prove it. Yeah. Right? So let's see, X belongs to D 1 and z belongs to the truth. So the product X times V times Z is, well, we can think of this as our product into steps. This is one space and this is a different space. We wanted the product and above is defined between two vectors in the same space. So what is this matrix? What is this matrix? By the second, sorry, the dimensions of this, this has dimensions. This is transpose this, as I mentioned, one times d1. And this has dimensions one, D two times one, right? So this matrix has dimensions d 1 times a. B is a matrix that we can start with two transformations into a common space. So Let's have the matrix. Let's see C1. That is b1 times B, and that have C2. That belongs to the two big types, the two orbits. Let's do it like that, be B2 times. So if we multiply, let me do it like that. This is the one on the right. So if we multiply C1 times d times x, C1s, C1 times X is a transformation from a space of B1 dimensions. This is the one, that one, and this is the one book. This is a transformation into a vector, into a space of the dimensions. And then the same happens with z. With z to that passes through space. Behind one. Here we have the 2 and 1. Here we have B times v. So we transform these two vectors into a common space. And then the dot product is going to be X transpose Z transpose Z to Z. That is, might not write this vector transpose times a. So B is nothing stood. Right? Is that interesting for us? Well, it might be interesting. Why do I want the transformation into a common space? For example, let's assume that I have two signals, one with a low sampling rate and another one with a high sampling rates. For example, I can, I be sampling the temperature? And the solar radiation changes very slowly. It, right? By the solar radiation, it my chains much faster depending of whether a half hours or one hour of the day. So in order to capture the information that they thank series of solar radiation can give me. I had to sample it fast. So I might have a sampling rate of 15 seconds on one minute. So I have 60 samples for our brother. I just need 1.5 hours. The temperature doesn't change too fast. So if I if I, if I observe the radiation for an hour and temperature for an hour, then I will end up with a better of 60 dimensions and a vector of two dimensions. If I put them together to estimate something, for example, they aim solar power production that depends on the radiation, but also the past. It also depends on temperature. What happens is that the 60 dimensions of solar radiation will absorb. The two-dimensions be the temperature. The Labrador will be in a subspace with this model. Suddenly here might have a hard time to use that information. And might have, I might have more dramatic examples. I take a picture of an infrared or they are the sun and the grounds around. That is hundreds of dimensions. One or two. I mentioned for the time under the machine will have hard time with all these dimensions. It will have a hard time to capture the information of the camera. So when they do is to do a transformation that captures the features of each one of these factors. And I put them in a common space. Maybe the 6060 samples of my radiation, I will use them a little bit and I end up with 10. And the divertor, I increase to 10. I'll have tackling. So I cannot I again compare spaces with the same damage and things can my work back? All right, that will be a simple simplistic, a simplistic example of information fusion. Asap. If I have, if I have in general observations coming from different sensors. Cameras were a writer, whatever, brand radar, maybe just one-dimensional. And I want to combine it with an image, right? I might have multiple, multiple sensors. Many of them are right and military, they use that all the time. That became notorious when a government tough how this really is the file about UFO, right? That's that became serious. When they say Yeah, that is your thought, we don't know what they are. And wake up to them using all this array of sensors with different features. And still we don't know what Eric. And in this report, they said that they were going to use artificial intelligence to figure out, right. And then is when it became interesting. So we need data fusion. And one way to do Data Fusion is simply this. Now, you will say, yeah, it's a linear transformation. You can do that in a given space, right? In an implicit way is an implicit way. But these Native, these matrices have, and I mentioned the one I mentioned B2, which is interest. But we have the representer theorem. This matrix. It has a dual representation as a function of the input data. With that, we sort of thing. So we are able, we, we can do this in infinite-dimensional feature spaces using the kernel trick by passing everything into your space. Textbook. So this matrix, this matrix is, it has I mentioned d1 times d2. And this is about product. If this is actually a transformation into a different space, and this is two. But again, when it happens, is that the singular values of this matrix, they are positive or 0, right? You cannot do a PCA directly in, in, maybe we just add square. What we can do our singular value decomposition, which is equivalent to all the eigenvector eigenvalues of this matrix. They have to be positive. That's it. Right? We will see this again. So a particular case where this matrix, it has a similar values that are positive, then this is our topic. And this means, this means exactly that. We first do a transformation into a common space of d dimensions, arbitrarily d dimensions. And then we do that are more, thanks. Some more properties. If k is k one, k one is cornell dot product. Let me use a laser. All right. So if K1 is our product, then your screen, sir. Yes. You see, you now. See this. So if this is our product, a kernel, then M p is a polynomial function than a polynomial function of k1 is how? Well, this should be a polynomial with positive coefficients in this zone, Polynomial with positive coefficients, and this will be an up. Now, if k1 is a kernel, the exponential of this corner is also occur. Plans. So if x is, this norm, is defined as the norming in a given space. Not the norm in Euclidean space, but the norm if a given space, then this is still a solid profile. These things. Let's assume that probably 20 is, is correct. So this quantity, this is a corner and B as just a scalar number, natural number. Then, since this is a polynomial with positive coefficients, then this is a Chernoff. Write, AB is nothing but the product of a with itself many times. Right? So this is a kernel, as we proved in the probability of cancer product kernels. So this is program already. Then. Since this is a kernel by property, one of the first set of properties, a sum of kernels with positive coefficients is also a kernel. So a polynomial with positive coefficients constructed with a corner point. And then we have this a 0 here, which is also a positive. This is a positive number. This is a coefficient 0 for B equal to 00. Constructs a matrix, which is a concept of constants. A matrix of constants. It has one eigenvalue, which is positive and the rest are 0, so it's positive semi-definite. So a matrix of constants is accords. Now, this is a corner, right? Let me do it again. By the property number two. This is a corner because it's the product of several curves. So it is a column. And second, a combination of kernels with positive coefficients is, since this is a kernel and this is a combination of positive coefficient, this is the curve, right? So this is a polynomial with positive coefficients, right? And this is at the same time the Taylor series expansions, but the Taylor series expansion of an exponential. So if we put here a distance or Euclidean distance, if v is the Euclidean distance between two vectors, then be exponentially. This exponential becomes a kernel. And with this, we prove that this exponential is a continent, so it is positive. Right? Why do I say that? A wing, the inside. If we put inside here a distance, this becomes a corner because the distance itself be equal to d squared, equal to x minus z square. This is equal to x squared plus z squared minus two x transpose z. Right? And you can prove that this is a positive definite function. So it is like 10. And our current governor is up. Isn't a square box or this one? Yes. It's not difficult to prove that this is a positive definite function. It also, actually it's positive. It's always positive, so it must be positive. So knowing this, if we put this distance here inside, this exponential, becomes an abrupt. Since there's are some, it has infinite terms. We can represent that. Once we put here the distance as a function of two vectors, this becomes our data series expansion with positive coefficients. So what we have is a dot product where the backers, they have infinite components. Because it'll tell us that expansion has infinite products. This way we prove first they x-bar squared exponential is a pronoun. Second display exponential is acronym in an infinite dimensional Hilbert space. So as you can see in general, we don't have a POS expression for this. So this space remains hidden. We don't have a gross expression for the vectors that we're going to start from here. And this is what they say, right? So x minus x0, it has this, alright? In general, we have here a kernel, another kernel minus twice per month. But even if we have sine is positive and select square exponential of this, nor would it here. We're going to next, we're going to express it by that. So we have this function here, which is positive definite because it's out, We're back, It's a product of exponentials. There are many kernels. This is the linear kernel to which we can add a constant. Because if we add a constant and we construct a matrix using this, we have a matrix or bedbugs between data plus a matrix which is a constant. This matrix will be positive definite and this will be semi-definite, positive semi-definite. So you don't have a single, a single eigenvalue, which would be positive. We see. And the rest, they're going to be NCB, sorry, and the rest 0. So this matrix is positive definite. We have the polynomial kernel. Since the polynomial kernel is a polynomial function of other products with positive coefficients, then while the positive coefficients equal to one, right? Then this polynomial, it is occurred because it's a polynomial function of positive coefficients. Coefficient being just one of I dot product. So this is, it, it is proven that this is a dot-product, alpha and C. They must be positive. Gaussian going on, we province positive this exponential. We can also prove that this is I cannot, It's also positive definite. This is exactly the same. We just changed the name depending on what we have in day. How do we find them by the parameter? That's what we bought him a brush. And because it has the shape of a Laplacian distribution, right? Usually the one that people use is this, because easy to use, It works very well. Good question. No question on that last slide. Power. What does the sigma at the bottom term represent again? Here? Yes, here. What if we, sometimes we call it Gaussian, right? Just by financial because this has the shape of a Gaussian distribution, whereas this sigma is the standard deviation. Right? So if we use it, we call it Laplacian is because the sigma is the standard deviation of the Laplacian. Basically. It's not like we're periods which we did. Aliens. Yeah, onsite visits though similar to just the Gaussian distribution is the mean isn't that the experience revealed on the bottom? Let's see, the voucher has this shape, right? And you have here, this is plus minus sigma Laplacian. It has this shape here. Then that will be the mean for both. But they, they, um, let me see. One case of these works. I, actually, I don't remember. Here are two Sigma squared. Then they, for a Gaussian distribution, the standard deviation will be Sigma. And we express the Laplacian like that. The standard deviation will be Sigma. Otherwise it's going to be two Sigma squared. So it doesn't make a lot of sex to families numbers yet, but that's it. I mean, it doesn't really matter, right? It's, it's a parameter. And usually when we use an exponential, we use this, which is easier to represent that set. And in this case, we don't want to call this passion because it's not a Gaussian distribution. And some software, many software packages, instead of having one over two Sigma squared when you harvest them. So it's a multi-period. There are parameters that that's something that you provide. I don't know if this answer they still know the person that was asking me about remotely. Yeah. Yoga. Thank you. Thank you. There's more they hyperbolic tangent or sigmoid. This is also positive definite, but not for all values of alpha and C. So we have to be very good for breast title or a dungeon or sigmoid. Sometimes they call a neural network. Porno. Not too useful. I never used it, right? Not too useful. But it might be the rational quadratic that has this, this form here. If this is easy to prove that it is the product, this is a positive definite function, so it is a kernel. Will do. The inverse will be quite a bear. Many, many of them. You have more here. Our lovely negotiate xi square is the wrong point intersection, right? So this is absolutely non-linear. Two. It's a positive definite function, so it works as a kind of, you know, them or they are not too widely used? Yes. So what is the basic criteria for it to be a print of this positive definite? Period? Posited that we want to say something else, maybe something. And it has honestly contain the two like x and z. Of course, yes. You can have a function that contains x and you multiply it by the same function, but then in z. And that is why they are, these are real dot products, right? You can also define complex dot product. The dot product in the Euclidean space is easing. In nature, it's complex. So we have two complex diapers defining the complex plane where we have is a complex problem right? By the weekend. This data, which is complex in nature, and we pass them to a Hilbert space whose dot product is this exponential, then the thing becomes real, right? Because the exponential is a product that uses the distance. Distance, this quantity squared is always positive. So that prove that even if the data is complex, the data in that Hilbert space becomes real. All right, So that space is infinite dimensions, but real. In general, you can define complex dot products. And there are papers. I started working with complex data right after I finished my PhD because I apply that to communications. Right after that. Some people publish works that use complex of products. In general, right? Nice profit. It's not too often. Not often used because we have spaces, diabetic or we have the problems of a person. And then an area like this square exponential by. And you can define a nice rubber this, and you have to take a look to abuse the bill. Let me write in algebra, built-in jurists algebra. So and there is another which is Cusco. I don't remember who he was. He might be additional material. Remember his name. Setting, setting, Joe's, me, introduce complex Collapse. All right, so I showed him the first papering in harnessing using support vector machines and the cop explained by T01 oversight and iPad for this. Anyways, but he level of general theory about complex corners. Just engage your mind or you are interested. And that's it. Alright, so here we have seen some, some properties that are useful to you can start to construct kindness, some of Cornell's process of kernels. Kernels as product of functions, kernels, kernels, polynomials, panels, and some kernels that are in closed form. So that you can construct it with a, with the data that is also recursive kernels. And Kronos for text, for example, kernels for Beta, which are not numbers, right? So if you have text, they have two different documents. You want to compare them to construct a support vector machine, for example, what do you do? Well, one basic thing to do is to use a bag of words. So I get the document and I count the frequency of appearance of each one of the words. And then I construct a vector. That is a vector that contains all the words of a dictionary. So you might be 50000 mentions. And then this doctor well-being, all zeros except in the positions corresponding to the words of my next. Hey, I can just eigenvalues, of course, the dimension of the vector by using only important works and discarding the rest. Okay? So I can have a vector with just thousands of images. And this vector contains the frequency of appearance of each one of the words in my text. And this is the way to compare texts. I have tags that have similar frequencies in similar positions. They will probably belong to the same topic. And these are things that, these are things that have been done with different, that makes for, in order to cluster this. And you can see that using this technique we are able to distinguish between politics and sports, for example. And this is pretty convenient to languages, right? When we talk about soccer. The words they weren't messy, ugly, messy appears in English and Chinese, right? So we can compare. And if this is invariant, it's pretty meaningful. And also you can do things that are not that embedding of the languages, but instead of doing back-and-forth with single words, you can do backs of words where sequences of words, for example, three words together. Hi, we can do things like that and then use normal kernels for this, for this. But the triggers, we first need to take the text and convert it into a vector of numbers. In a fixed speed is where all the texts go to the same space. There are many, many things that can be down here, right? So we just scratched the surface. But this is all we need for now. And so the last part of this is a technique to do PCA. One analysis in human spaces, you will see how powerful this is. What we'll talk about that next.