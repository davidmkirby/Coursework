 Oh, wow. So again, Sorry for being late. Today we start with principal component analysis in four spaces. As we have seen. How to do linear, linear outlets with media properties. As heartburn example of methodology which a run of two. It's powerful to extend the properties of President, the properties of a PCA methodology. What is the PCA? Well, it's it's it's a method to find the principal directions in a set of data. I have to say first that this method, the PCA, please welcome our analysis or a similar way, the composition, which is a more general method based on that. Well, actually they're both equivalent, as we will see. They are only valid if the data is Gaussian, right? So in, in VCA, we are assuming that they are the sample that we have. Ps. It is distributed Gaussian using a Gaussian distribution, all right? Otherwise the method might not work. So when you see people using ECA without checking whether the data is Gaussian or not. They are not doing it properly. All right, so roughly speaking, when we have a distribution barriers around the amine, and it is distributed that for example, black dot. Then here we see that we have today and actions in which the data is spread on that principle direction is this one and the normal one has less spread. So the idea of PCA in general in a space of dimension says to find the directions in which the variance is higher. And then if we get a point, say for example, deaths are reasonable representation of this point in a space bob reduce dimension base to project it in this axis. So here we have an error. If this directions doesn't have a high variance, they, they, approximation is good because the error will be small. That is the idea. But this thing works if the data is distributed like that. So we have, for example, data distributed like this in three clusters, for example, Gaussian. Then what is the principle that action? And it might be the case that if we apply PCA here, we find few principal directions like now. Well, we might be destroying information in the data is from our graph. And so we have to use a very high, As I said, the PCA principal component analysis, as it might be a good mental knowledge or reasonable methodology in some cases to reduce the dimensionality of the data. For example, here, we will be reducing from two dimensions to one image. It might be the case that we have many dimensions, but only some are only a few of them. They carry information. And then we can apply PCA and then we have the same data in from foreign intraspace. So here's the image. And this, this transformation is linear so we can undo it. And then thus providing the algorithm that we're using with interpretability. So every path from two dimensions to one I mentioned here, is sample. It can always be transformed back into the original space in order to to do an analysis of what features are important for the task that we are doing. So I said, if the data is not Gaussian, the method of work. And indeed, it can be proven that the PCA is a method that uses data model, which is about It's only right. And let me explain this from an intuitive of view. Whether we know whether we're doing PCA. It's very simple. So we have this distribution, right? That this is deviation. Yes. Distribution that is proportional to the exponential of minus 1 over 2 times x minus Mu. Mu is this point here. Transpose sigma minus 1 times x minus Mu. For us, we need a normalization. So the integral of this is one and it has properties of probability distribution. So the probability or the likelihood, the likelihood of x is proportional to this. And we have x axis, any sample space. We have meal, which is the mean around which that data is distributed. And sigma is what we call the covariance matrix of the data. This covariance matrix tells me what is the relationship between dimensions, right? So in this case, you can see that there is a high relationship, actually a high correlation between the dimensions. So if a sample has this value in this axis, it will probably have, with high probability will have the same value in the other asks. And if, and for this value here, the probability of that the other axis has this paleo, is that it will load. The sample will have low likelihood. The sample will have a highlight. And then these two, they are related a one-dimension as a high-value, the other dimension will probably pop a Haida image. Right? So in this case, sigma can be expressed as sigma square 11, sigma squared 32, sigma 1, and sigma 2 1 squared. In this case, we say that sigma one film or sigma 2 1, which is the same as our menu, which is a high bay, right? High-value because both dimensions are related. So in, in this case, this case here, both dimensions are uncorrelated. All right? A situation like that, knowing the value of one I mentioned doesn't give me any information about the other damage. So in this case, sigma one, t or z mature, they will be 0. In a similar distribution. The matrix, matrix Sigma is one. It's an ion that contains the values here. And an N into and out of them will have six. Right? So. What we want to do is to pass from one distribution and I got one distribution like that. So we want to transform the data. So we passed from any any covariance matrix to a covariance matrix which is out. And the components in this transformation, they will be uncorrelated. And the distribution a Gaussian, that means that they damage and they will be independent. All right? This is a property of Gaussians. If two dimensions in the correlated Gaussian distribution, they are independent. In correlation, any dependency are the same in our ocean is division if we don't have a guardianship and this is true. And this is one important point here, right? Because what we're doing is to, to do a transformation where the components are independent. All right, so how do we do this? Well, it's very simple. It's very simple. We know that the covariance is a square matrix, which is the magic or a paramedic if we are in the complex plane. But let's just stick to the real disbanded. It'll be symmetric sigma and sigma minus 12. And it's positive definite. So all the eigenvalues of this matrix are real and positive or 0. So this is in general is positive semi-definite. Sigma then can be decomposed into eigenvalues, eigenvectors. This is base of orthonormal vectors. And this is a diagonal matrix containing all the Eigen base. Then by using the properties of matrices, sigma minus 1 is equal to the inverse of this Putin's bulbs minus 1, the inverse of this one, and then the inverse of this one. And then using the properties of orthonormal matrices, we know that the inverse of Q is Q transpose. So here we have Q transpose, and here we have q minus 1. This is equal to Q times Q transpose. So in Berlin, this matrix is equivalent, remember lambda, but lambda is a diagonal matrix. And again, using properties of matrices, we know that may emerge of lambda is constructed by emerging each one of the elements of aids they are, That's it. All this, these three properties that I use, they are safe or approve proof. So then we can say that sigma minus 1 is equal to q raised to the power minus 1.5. Again, minus 1.5. Thanksgiving. We can do this because we saw, we know that the matrix is positive definite. So this debaters in, in locking down that they are positive and positive. So this can be done and it has only a solution, right? We compute the square root of this quantity thought this matrix, we get 1, one thing away. So this is composition is unity when the matrix a star covariance matrix. Why do I do this? Well, this exponential then can be written as well. Let's first construct an equivalent of x that has, then they mean remote. Right? So let's assume that we have x. Feel that which is equal to x minus mu in order to reduce the notation. And then this argument can be written as x tilde n cubed lambda minus 1.5 times the identity matrix. I put it here because. The product of these three elements is equal to this product. I can put an identity here and nothing, nothing changes. And then Q transposed, and then I need a transpose here, and then x tilde. Then finally, what I have, finally is that icon now x0, let's call it not z, but let's call it V. V is equal to lambda minus 1.5 transpose x minus mu. So here what we have done is to transform the vector x tilde x minus Mu using this linear transformation. Why this is a vector, column vector. And these are two square matrices. So this, this product is a vector in a space of the dimensions, right? This is what we did here. And hence the probability of x, the probability of B is equal to b is proportional to an exponential of minus 1 over 2 v transpose times the identity times V sub y. Then they start formation. We get vector v, whose distribution is a symmetric Gaussian centered around the origin with with our covariance matrix, which is equal to the identity. Write this as d minus 1, but x equals beta. And this is the transformation that we want. When we do this, we pass from this distribution. This distribution, where the mean is 0 and the data is distributed, similarly symmetric. So the components of b, they are in the manner from a probabilistic point of view. They are uncorrelated. Hence they are either have pain abundancy can be programmed because this exponential can be factorized into the exponentials of each one of the dimensions. And this is the meaning, the basis or the ECF. Now, what dimensions do I use this transformation? Well, here I see distribution where every component is multiplied times the inverse of the square root of the Eigen values. If an eigenvalue is based model, then that means that I have to multiply it times a high value of numbers in order to get our target distribution. Right. In a way, what we do is to take this point here and put it here. And this two axis, they are axes that if I transform them back there, it was. So this component here is transformed into this axis and we multiply it by a high bay. And this axis here, because here or other where I am here, right? But let's assume that this axis here. And then we multiply it by a small value. So this component, which is a principal component because it has a high, a high barriers around this axis goes here. Right? In order to explain this better, I could have done another transformation that doesn't include Sigma minus 1.5 pound on something like that. A or B. Yeah, B prime is equal to q transpose x minus mu. And then the transformation goes into a distribution that is like that. The three prime transpose. Sigma lambda minus 1 times p. So in this case, if I don't include sigma minus 1 here, I have a different distribution. And this distribution has finite matrix covariance matrix, which is an AI. In the elements of the diagonal, we have the inverse of the variance of each one of the components. So my distribution, v prime will be now something my mouth. Where one of the components, it has a high variance. The variance of this component is the corresponding eigenvector of this principle, of this axis here. Right? So in this distribution, they eigenvalues are the variances of the distribution. So here I have a representation where I see the importance in terms of variants of each one of them. How do we do this? In other words, how do we find Q and plan now? While we have to use the PCA theorem. So let's do this again. But now we have to use a given criteria in order to find the parameters of my transformation, which are q and lambda. Now we are finding a set of parameters and another vector like the value in our misdemeanor, but they are two matrices, q and sigma and Fiona. So first, assume without loss of generality that my data is around a center around the origin. So I take mu away, right? And I use this, I use this distribution x tilde. I think me and again, I know it so I can, I can always do this. So the other relation of function, many other correlation matrix can be estimated like that. 1 over n times the sum of this outer product between data. Remember, this is a column vectors. So this is a row vector and this product, it has d times d dimensions. Right? We add them together, we divide hands, and this is what we call our sample mean. The other relation, right? It's an estimation of the other variation. Since the data is it has 0 mean. These are the correlation is also a convenience, right? And as I said before, this matrix has a representation in terms of eigenvectors and eigenvalues later. The elements that we have here, we call them as lambda, lambda i, u farming. They need any explanation. I do, I need to go up. So the questions in the chat, All right, out of here. How do I construct the eigenvectors and eigenvalues? Well, I use the principle of minimum mean square error. Minimum mean square error as its principal or our criterion that we know what we want. The representation of the data. Representation of data will have an error. We want to minimize this error. This one is be Caribbean. And the idea I, my brother Renee or I draw here. So we have some Beta. Let's put this data around the origin, right? This data passes this deviation. By that, Let's assume it's, it's thin distribution. And then we have points. We know that we have some principal components. And then we have a sample. We want to represent this sample in a space of reduce dimensionality. And in this case we have two dimension. We want to reuse it for one. So what we do is to find this direction and we project the sample on it. And that will be the error. The error is, is a vector that is normal to this direction. Right? So we do that for a given action, arbitrary and we don't get the errors of other samples. And then we compute the mean of these backups. So my my projections can be written like that and we have a dot product here. How does this work? This element here, this direction here is represented by a unitary factor. All right? So we compute the dot product between my, we're going to be the dot product between my sample and this vector. And then we multiply it by the vector. We have another vector. This vector has the direction of this unit vector. It has, it has an error, which is this, which is this component here. Right? The eye. It's, again, it's a straight forward to prove that if this vector is unit I and I do the dot product between this vector and my sample. And then I multiply this scalar times the vector. Again, I obtain a new vector whose distance to the initial one is the minimum possible. And I gave it to you. I mean, it's, it's not difficult to prove that. So what is the projection error? While the projection error is x minus the dimension of x. X minus the x dimension of X. So X minus this operation here. Now, remember they have a set of data. So I want to compute now the mean square error of this projection, which is this. First, let's go and see a theorem That's going to see a field and a name. Let's go back to our representation theorem says that a set of n vectors of dimension D that are to be modeled using l orthogonal basis vectors, q, n, m scores, Z. This reconstruction algorithm. What did I do here? I have my sample and then I compute a projection of my vector over a set of vectors. And this is what I call the scores Z. And then I multiply them by the vectors themselves. So here in this expression, what I am doing is to compute an representation of this point that is non-Bayesian one better, but is based in a basis of vectors. So my original vector is this one here, where they have, is a set of basis. And I will divide each element of the basis by a given score, a given number. And this is memorization. Okay? Understood. I have a set of vectors, a basis of vectors that are orthogonal. And then I will divide each one of the vectors by a set of scores or coordinates if you want. And this is my representation. Let's minimize the error of this representation. So the theorem says that the minimum reconstruction is achieved. The basis cube contains the largest eigenvectors of the empirical covariance matrix already. So the minimal reconstruction error is done. If I use the factors of the autocorrelation matrix. So that's the proof. Let's start with one vector, 11 eigenvector, one element of a given basis. Here is vector which is normal, then R is one, and z is a scalar, a given number. Let's compute Dave reconstruction error between the original sample and this, which has one-dimension. So let's do that. Right? We get a vector which is normal nanometer one, multiply it by a number. Let's compute the error square. Let's do it for a set of N samples. We develop this and what we have is this square plus this square minus twice, this times this sample squared. There are observations were here and the product doubled. So first here, we have the norm of this, this thing squared. Since the norm of this vector is 1, this is 1. And this is equivalent to this right here. And now we can compute the derivative of this function with respect to z. Bounded areas of z, z1 is because we just chose vector q one. And the only thing that we know is that Q is it has not equal to one and z is anything. So we compute the derivative with respect to z. And remember that we have to keep Q we know equal to 1 w. The derivative of this, the derivative of this with respect to z is nothing. Here. The derivative is what is this? W. What did I do here? Oh, I'm sorry about that. This is Q taken out of this. There's SQL never notice this. Bascially Q2, Q1, Q0. So it'll be the derivative of this one. We have to Monica and David of x1 squared, x2 xn, right? And then in order to maximize or minimize this expression, we did x j, we want it to 0, and this is what we get. Z score is, as I said before, the dot product between Q and a sample. And that proves that the minimum, their obsession with a minimum error. We do it. If z is this product, then multiply again timescale and we get back. What is the reconstruction error when we use the Z? Well, we go back to our square error. We put, we change Z by Hugh 1 xn, and this is what we get, right? But I only change it in this expression here. I love this. So this has dimension one. Right? So let me see. Since I have x squared here and here I have x i transpose exam, I'm still this is equal to minus m, so I need a minus sign here. And here I have plus z squared, right? So what I did forget to change the sign of African. And so they should have minus sign here. So I have two big mistakes and it's like now. So what is an exit I do. I have to minimize this now with respect to z, knowing that q is a vector which is normal. And this thing, it doesn't do any minimization, so I can just take it out. I know. Now I want to minimize j subject to. The fact that Q is, is our normal vector has norm equal to one. So I did two things. First, I take this out because it bothers me and it's useless. And I add a constraint, multiply it times our Lagrange multiplier. So what I have is this z here, z with the minus sign, the original minus sign. I change it by Q transpose x. So this will be z squared. And I add the constraint. The constraint is Q transpose, Q transpose Q minus 1 equal to 0. I add a constraint multiplied times by the label banner 1. Derivatives of that. And this is what I get. Argue equal to lambda cubed. Would argue equal to lambda cubed. What does this mean? Why aren't you want to run it? What does it mean? It means that if I multiply a times this vector Q, I obtained the same vector multiplied times escape. This is the pure definition of eigenvalues and eigenvectors Q. Right? So if here is a component of, if Q is a component of my decomposition here, it can be easily proven that this is true. All right, you just need to, chains are by Q, q transpose, by soft skill to be posted on a few. So since these vectors are orthogonal, here, I'll hit that products are 0 except the one corresponding to one, the one the only dot product which is non-zero is the first one in this case. And then there's what we multiply a times 1, 0, 1. And that will get, you want to get here, right? Either the operations. So if this vector is an eigenvector and this value is an eigenvalue, then this is true. Otherwise it is not true. Which proves that they representation with a minimum mean square error is done using the eigen vectors are the autocorrelation matrix. I know the proof is longing and it takes a lot of other rows. Right? So From here, take a look the proof again. And I'll, I'll fix this. Okay, there's two mistakes here. This is Q1 and this has to be multiplied by minus one. I just put a minus sign here. And the theorem proven, the theorem is actually very easy. Now I have a representation like that with you is a battle that I don't know anything about it. I know it's normal. It's a vector of MIMD type of norm equal to one. That's the only thing that I have a force here. And Z is a scalar waveform. So I have amygdala in a given space and our presentation with just one non-monetary arbitrary in that space. So then I compute the reconstruction error. We didn't know a generic representation. And I obtain this expression here. And I know that this is unitary, so I can just take it up. Now I compute the derivative with respect to z. I want to know what is C, assuming that Q is normal. So what they do is to compute the derivative with respect to z of that expression or the white arrow. And I got it to 0. And I find that z is equal to the dot product between my sample I, my vector. So my representation of minimum mean square error is the vector multiplied times this. Now, I want to know how to find a better Q normal, but with a minimum reconstruction error, right? So now we have minimize here with respect to this chord. But then we need to minimize with respect to the vector because this vector is arbitrary. So what I do is to, to eliminate, so to compute, to develop this equation. And this is one went with the side chains. This, I dig it out. I have to add the constraint that q must be norm one. So Q transpose times Q minus one is equal to 0. Right? So we have now we have to compute the gradient of this with respect to q. And this is the final solution. And this solution doesn't mean that Q must be an Eigen vector. And lambda must be an Eigen Bay, the same that I used before to do my transformation. So this is something that it only works properly. We will have, we will have a demo next, next class, right, of that. So if I use the TCA with data which is not Gaussian, this, the final result might not be accurate. Right? In the next chapter, we will prove that when we minimize this forever, with that even normalization, similar to the ridge regression. This is optimum. When the data is distributed. According to a Akash. Friday MIMO minutes, whatever. Eyes, sorry. Regularized is optimal if and only if my data is Gaussian. So if my data is non-Gaussian, I can do, I can give them that. I didn't assume anything here. I don't have any, any assumptions or wish to theta ss, I will achieve a minimum. And the minimum will be your name. Because this observation is unit. As long as r is actually an aspirational matrix covariance matrix. Right? But it might be the case that the data is not voucher. Other representations they have. As modern matter. This is what I mean. Right? So the smallest possible reconstruction error is achieved only if the data is tough. It's not. Gaussian, will have a minimum here, but maybe there are other presentations that buffer me better. That's what I mean. So pattern we saw this one in a way this can be solved using kernels, kernel, the kernel trick. All right. So let's see it. Alright? So now we have some data, and this data is non-Gaussian. What I do is transfer this data into a space with higher dimensionality. With the hope that from some point of view from Sunday actions that they did, they see it looks like a bow shot, right? This is counter-intuitive. It's not easy to understand this, right? It's not something that we can easily prove. And it's not something that we can easily understand. Alright? And it might not work. But if we get many dimensions, infinite dimensions. It might be that from some directions of space. And we have infinitely many directions of space. From some of them the data looks like repulsion. And we can find brings about actions. In some cases. Just won the election will be enough to represent them verbatim. But how do I explain this from from my point of view file? So I get my data into an infinite dimensional space. And I invited our regression. With it. The idea is that in infinite dimensions, I can always feed on a hyperplane that pretty much contains all the data. All right, we have infinite dimensional so we can always write, yes, I think I can point to like way to explain this would be like, for example, those pieces of art that are designed in a way that any direction you look at it, it looks like a mess, but a look at it for whispered perspective. And suddenly it's in this one picture. So it's kinda like the same dimensions but the sort of direction, and you get a weak acid, you know, because here we have three-dimensions. And the, anyhow, they different or I think maybe that will be an analogy, right? So there's this business partnering with zip, with objects and 80 random. And then you log from a given direction and you see a face, right? Yeah, but it's, it's an analogy, but it's not what happens. Because we must from three-dimension, for example, to infinite time. And what happens here is that H is infinite. We have an infinite expressive capability and machine. For a classifier, I can classify all the samples with no background, right? With arbitrary labels. In progression, I can fit all the data in a plane with no error, right? In infinite dimensions. But if I restrict, if I restrict them are by nature my name is I mentioned, then my error is not going to be 0. Is not going to be 0, but it's going to be a small MAN. And in a hyperplane with a few dimensions, I will contain all my data. Right? So I can find, I can find a subspace with a few dimensions that are able to represent the data to approximate the data. Right? So in my data is Gaussian. In the input space of 10 of 2 that mash-ups. Here, I have a plane that represents the data without ever been in my data. Is simply this. There's no way I can represent the data within an hour. But if I pass this into a higher dimensional space, then I can fit a hyperplane whose untidy much is a surface or a regression function like that, right? So this in my universe space is going to be in hybrid pie. So with BCA, what we want is to find the directions of this. And here are the directions. What I have is error smaller. Alright? So if it is Troy, data can be represented in a hyperplane. Let's find the direction of that will be the principle that actions of the other operational maintenance it something in that hydrogens, right? So the first thing to do is to use an, a linear transformation into reproducing kernel space. We still don't know why this name, but it's a hero space provided with a kernel function, a dot product k of x. Let me see if I have another icon. The second thing is to construct, Let's go onto. The second thing is to construct the matrix or the transformed data. So now we don't have heavy dot x, now we have probably not phi, which is a matrix that contains all this data phi, phi of x in columns. And so this matrix phi in my half infinite dimensions, if I have n samples, these metrics might be infinite times n, right? Infinite of rows and columns and I. But at any rate, another correlation matrix exists. It must be a covariance matrix. So at the end of the day, this data here, it has to have 0 mean, but we will take care of that later. Let's assume that the data has the domain. Now. And so this is the autocorrelation matrix and these other correlation matrix. These other correlation matrix. Yes. It has a representational terms of eigenvalues and eigenvectors. The eigenvectors, I call them v, and they eigenvalues, I call them. First about what is the structure of B. Here. We have infinite dimensions. So this vector, it has infinite baggers of infinite dimensions. Right? But this is not a problem for us. It's not going to be a problem for us for two reasons. First, we have a second. What happens with lambda? One? Lambda comes from this matrix. This matrix contains n vectors. So no matter what is the dimension of Z, it's going to be infinite. Infinite, right? We have infinite bass here. Here we have n factors. So the rank of a matrix is n, which means that this matrix is always positive semi-definite. It has only n eigenvalues here. Only pass only n positive eigenvalues. The rest are 0. So we have n eigenvalues that are positive and infinite zeros that we can throw away. Okay? So sometimes I will represent law now as a, as a matrix that has a sub-matrix of n eigenvalues and then infinitely many zeros. And sometimes I will just represent day. There's some matrix because the disease then this is absolutely necessary for our reconstruction. But it's also true because here we have omit vectors. Remember that this n vectors, they have finite norm, right? The space is complete. Any vector that I input in my space, it has finite norm. So here we have n eigenvalues bounded between 0 and infinity, but bonded, right? So all of them, they are less anything. And now we know that right here, we know this property of eigenvalues and eigenvectors are q is equal to lambda q. So if we put here, if we put here 0, the eigen vectors, we can express this as are probably not. Right? This is equal to lambda transpose Q. I'm doing this because I see another stupid error. And again, this, do it again. For the hero space. I call them V Lambda V Bible. And so by repeating the thing with B, this is equal to V Lambda V transpose V is equal to one. We know that B transpose is equal to the inverse of b. So this is an identity. So we can write this as v lambda. Then look at this brain because again, another error, I put lambda v, v lambda, right? There's habits in my input space. It also happens in the Hilbert space. We have. And I mentioned three-dimensions in my space, infinite dimensions here, but this product is true. Now, what is the, how do we use? Hear that phrase that we use previously in support vector machines for example. Well, we have two things. First, they cannot read, will abide later. But now there is another important. If my cost function, it contains convex function of the error and a linear, an increasing function of the parameters, which is the case here. Then this parameters can be expressed as a linear combination of the day. The parameters here are my diversity. So I can say that p is a linear combination and call them a linear combination of the vapor. And this is the core of everything here. My eigenvectors, they are, of course they are a linear combination of the data. Right? In the space of Did I mentioned, it's, it's evident the eigenvectors, they can be expressed as a linear combination of the data because the data spans the space. But in infinite dimensions this is also true. For injury damage, we need a theorem will happen. So v is something that has infinite rows on infinite columns. But of this infinite vectors, we only care about those vectors for which the eigenvalue is non-zero. And we have n of those services internet Einstein arrest. I don't care because they have eigenvalue equal to 0. This is my data. This is infinite times n. So this must be a matrix of time-stamp. My set of eigenvectors is a linear combination of the data that spans the space. But I only have n data, so I only care about the first n vectors and I can represent them like that. So then Let me see, how do I do that. So my matrix R, again, sorry about that, but they call it c. C. C times b is equal to V landmarks on the screen. Just for a minute. Remember, see this correlation matrix times B is equal to b. But now instead of B, I can use these representations. So see Amara. Actually, sorry, C to B. So there's Uh-huh. And what is c? C is the auto-correlation matrix, which is 1 over n phi, phi transpose. Then we have another fine. So you have lambda and this is about the Feist. Alright? And so finding a, sorry. I mean, by this is c times v, which is, this is equal to V again. And then this C is equal to 1 over n phi, phi transpose phi again. And then we have a, this is equal to a lambda. And the first five, I can take it out. And I have one NaN, I can pass in here. So I can say that Phi transpose phi is equal to n. Fine. What is this? I don't know. This orange. This is not q, this is not the eigenvectors. The eigenvectors are the this is the data. So we had another another answer possible. No, no, no. This is the data this is the data file, not V. V are eigenvectors. This is the data that they didn't ask it. What is that? K, k, K, k. And so I take this time off a bad day today. Now, k is equal to k. K, say, oh, now. Ka is equal to m, a language. And this expression might not say anything. But what did I say about eigenvalues and eigenvectors before? We know that C, D is equal to lambda T. Or if you are one with all the values c, v is equal to V lambda sub k as name, kernel matrix. The eigenvalues, eigenvectors of a matrix and lambda are the eigenvalues of a matrix. All right? So I do not have access to appeal or to v in this case because it's invalid dimensions in three dimensions. I don't have access to late either. So this, It's integrated a whiteboard, but I cannot implement it. But then assuming that V is a linear combination of the data, the input data, trying to data like that. Then applying this, we find that a yes set of eigenvectors of K and lambda v is the set of eigenvalues. Okay? So if we compute the Eigen values of k, they add eigenvalues, okay? We divide times n. Then we find the eigenvalues of the data of the other operation of matrix in the Hilbert space. So we have access to Docs. And how about the eigenvectors? The eigenvectors. We don't have them because they have infinite dimensions. But we have a linear transformation of them into a finite dimensional space. Space. Just a, it's a matrix of n Eigen, Eigenvectors of dimensional. And there's AS in turn, they prove that they undergo correlation matrix in this infinite-dimensional space that we cannot access. And the kernel matrix, they have the same information. This is the proof. I'm passing into your space. Using dot products doesn't destroy the information. So we can work in the primal space or in the dual space, because we preserve the eigenvalues, nonzero eigenvalues. They are zeros over here. And we preserve the eigenvectors through and non-linear through a linear transformation. And this is why I want you to understand is, I know it's very difficult. I X by myself, probably in a happy way, so I will need to I'm sorry, I had by day and I have bad slides. But we will keep working on that. Right. The concussion of this as at least I made it to the last like the comparison test at the end of the proof. The completion is, if I want to do the BCA in a Hilbert space with infinite dimensions, I cannot see, I cannot access the other relation matrix because it has infinite thanks increased dimensions. But equivalently, I can compute the kernel matrix which is ten times and I got access to it. And the eigenvalues are the same. But the Bonanza, an eigenvector is a, they are and only a linear transformation of the original eigen vectors. So I can compute a representation of my data in a Hilbert space or reduce dimensionality spaced. Now here I have any methods. I can chose the ones that have the highest value of Lambda and discard the rest. We will do next. In the next class. I will fix this a little bit. And it says, The last step is, okay, I have this information. This is true. That works. Now. Given a vector in a Hilbert space of infinite-dimensional, how do I construct a representation of this, of this betters in a reduced dimensionality? As I mentioned, I did that I can actually see an outbreak with them. It's going to be easy. We will need to apply again the kernel trick. And then I will show you a demo, which is quite impressive because for that particular case in which we have three Gaussians distributed in the input space, we throw them into the Hilbert space. And with just one eigenvector, we can do a very accurate representation of data, right? They, paper is by the corresponding published by Bernard Shaw, perfect for workers. And it's available for you to take a look. It's much longer of course. But I recommend it because they have examples of application of this, right? We'll see some didn't annex like I'm sorry.