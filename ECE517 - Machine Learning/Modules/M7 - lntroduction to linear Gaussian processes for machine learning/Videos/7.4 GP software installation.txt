 Hi, in this video we are going to see how to install the software in order to run the Gaussian processes for regression, also for classification. The first thing that we have to do is to go to this webpage, it's www.gaussianprocess.org. And here you will find the original book by Rasmussen and Williams. This is a book that you can buy it, but it's also free access. And if you click here in Contents, you will find all the chapters of the book. In particular now we are using this chapter, Chapter 2 Regression. And I recommend you to use the book to study this module. You can go to Software here. And here you have everything that you need in order to use the Matlab code for Gaussian process for machine learning. The first thing that we have to do is to download the software. Here you have three different links to download it. And then you simply decompress it in a directory, and you will find all these folders. The only thing that you have to do is to drag and drop this script here, Startup, and the software will be installed. So then I recommend you to read the instructions, and there is a lot here, right? But you have everything here to make it work. There's two fundamental parts of Gaussian processes which these scripts minimized. That this is a function that, in particular, what it does is to optimize the parameters of your Gaussian process. We will a talk a little bit more about the parameters of the Gaussian process, the hyperparameters, covariance functions, likelihood, everything in next module. And here we're going to show you the bare minimum to start with it. The other function is gp. This function either trains or tests, or both, your Gaussian process. In order to train your Gaussian process, you have to include x and y as training input. xs is a test input, so in this case with your training and then a test. The output, mu, is the mean of your prediction, and s2 is the variance of the prediction for every point. In this example, this example is the first one of the previous video in which we train a Gaussian process for a linear model of order one. The first thing that we have to do is to define the covariance function that we are going to use. I recommend you to read the instructions about this for this case. The covariance functions is the sum of a linear covariance function. This is simply a dot product between the training data plus a constant. I insist you don't need to understand all this now. We will do it during the next chapter, the next module. But this is the first thing we have to do. We define the covariance function, and the next thing that we have to do is to define the likelihood function that we want to use. In theoretical slides and in the examples, the likelihood function for the observations is Gaussian. All right, we assume that y, it has a Gaussian distribution. We have some hyperparameters. For example, the likelihood parameter is nothing but sigma m squared, and here it should be represented as the logarithm of it. Right, so when we put -1, this is the logarithm of the covariance. This covariance is an initial value that is further validated in this function minimized. So if we define a set of parameters in this structure, these parameters, they are further optimized in this function here. Once the parameters, they are optimized, then we input them in the Gaussian process together with the definition of the Gaussian likelihood. I mean function which is void. A covariance function which is linear, that here, this covariance function, we are saying that our process is linear. And the training data and the test data. We will run this experiment. We get the example of the video in which we have the training data in blue, the test data in black. The test data is a set of, well, in this case, it's 11 data equally spaced. Then we have an output that contains the mean variance in mu. And the variance of each one of these values that we represent in this black line. If we multiply the, well, this is not the variance, but the standard deviation, its square root. And here we have the interval of two sigmas, twice the standard deviation. So go ahead and try to reproduce this experiment using only this script here that is provided together with this lesson. And then try and reproduce the next example in which we want to try the prediction of another moving average sequence.