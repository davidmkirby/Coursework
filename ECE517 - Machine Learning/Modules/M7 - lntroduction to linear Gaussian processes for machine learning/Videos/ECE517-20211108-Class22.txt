 With this climatic change thing, we have a lot of polyps. So anyways, so let's see if we clarify. Everything above. I got again, analysis or things about the universe lace made. I think that the main confusion there was apart from the, from the high price was about they notation, although all the elements that we have, right, so let's summarize the elements that we have in, in this technique and he respects first the data. So we assume that we have a set of data, which is a set of samples from X1 to Xn and in a space of d dimensions. So this five vectors that we observe in the space of dimension d, and then we throw them inside a Hilbert space. So we have, we have a transformation, fine. And then we construct a matrix, capital phi. That's a matrix that consists of the transformation of all the laptops from X1 to animate. These are column vectors. So this matrix, it has infinite rows and n columns that's copied on that. This is a matrix of infinite times n dimensions containing all the data. So we pass from the dimensions the infinite limit that. And with this, we construct a covariance matrix. Well, we have to assume, we have to assume that they mean of phi of x I is equal to 0. Let's assume this now, and we will solve that later. And then assuming this, then we construct a covariance matrix C, which is the sum, not the sum, it's the average one over n. Sum from I equals one to capital N of five of x I times phi of x transpose. This is a vector of infinite dimensions of whatever is the cardinality of the space. In this case, we assume with, without loss of generality that the space has dimension. So since this is a column and this is a wrong, then this is a matrix of infinite times infinite dimensions. I see is that a matrix of infinite times infinite dimensions, so we cannot access. And in matrix notation, this can be written as phi times phi transpose. Phi is this matrix that contains all the data. So these are our, these are our elements here. We assume that C is a positive semi-definite matrix because it's constructed like that. And since in the construction of this matrix we have n vectors. The rank of a matrix is M, which means that c can be decomposed, decomposed into eigenvalues, eigenvectors, and eigenvalues. So wie is a matrix that contains the eigenvectors are basic composition. And lambda is a diagonal matrix that contains all the Eigen fav. So V is a matrix that has this aspect here. One from Matthew be, from V1 to be infinite. We can say, right, say infinite here. And then lambda is a diagonal matrix that condensed from lambda1. To learn that will print. But since we know that this matrix has rank n, here, we only have n eigenvalues that are non-zero positive. Alright, so this matrix can be reduced to a matrix of m, my image that contains from 1 to Lambda n. Here we have. And then since almost all the eigen vectors here and here I have multiplied times 0, then we just ignore them and we only, right, the first n eigenvalues, eigenvectors. Right? So this matrix, this is still an infinite times infinite matrix because here we have n vectors of infinite dimensions. Right? So this is going to be infinite banks and banks, and, banks and banks. Right? So this is infinite times and end times. Alright? So this has dimensions, but we started with they only with em, get vectors and eigenvalues because the rest of eigenvalues are 0, solving. All right, so this is the first part of the eye. The eye will need to erase and summarize. Right? Now. See, it's constructed. Can we decompose this way? So we know that c times v. So the covariance matrix times the matrix of Eigen vectors is equal to V Lambda V transpose times B, right? And we know that this eigen vectors, they are orthogonal and normal. They have non equal to one and they're orthogonal. So in this dot-product, so these are rows and columns. In this set of dot products. In the diagonal, we have the dot product of a vector with itself. And outside of the diagonal we have they, they don't rub of one vector with any other. In the diagonal we will have once and under the dialogue we will have zeros, right? This V transpose V, its component i j is equal to Vi transpose b j, and this is equal to 0. And in the diagonal element, the entry i by this matrix is vi transpose vi. And this is one. That means that this matrix is an identity. And we can just ignore it. And the result is this cv is equal to v1. And this is a fundamental property of eigen vectors. When I multiply any eigenvector times the covariance matrix, we get the same eigenvector, v multiplied times its corresponding eigenvalue. Right? So let me erase this. I may say the fall and we'll be okay, don't see it. So I guess this and this, these are all the elements that we have regarding to a covariance matrix in the Euro space. So let me summarize this thing here. That's a matrix that despite transpose matrix two. And this can be decomposed as V Lambda V transpose. And we know that C v is equal to b. And now we got them started another matrix with this, with this data. Remember this is the date. So this matrix is k matrix of dot products between data, which is phi transpose phi. Here. We know that this matrix is infinite times M. So this matrix is infinite. This is n times infinity. So this matrix is n times that is the kernel matrix is the matrix of the product between data in the Hilbert space. So we know that the entry for k I j, the entry ij matrix K is a positive definite function over XI. And XJ is simply this. And this is a positive definite matrix. It is it that using a matrix and the matrix transpose, and we prove that, well, we construct a matrix like that. Then the result is positive definite. So we'll prove it in several ways. We can just use them. We can use the recessive, for example, on the progress of positive definite matrices. So k can be expressed as a function of its eigenvectors and eigenvalues. And I call them a lambda prime transpose. And this line is nothing but n times lambda. And we can write a theorem here. But let's first read this. Okay? Can we decompose this matrix? This matrix is a matrix of n eigenvectors of K. And lambda is the matrix of the n eigenvalues of K. So the first thing is this Eigen, this Eigen values, they are n times the eigenvalues of C. And the second result is V is equal to five times a. So that will be the theorem. The theorem grids. My covariance matrix has this decomposition V Lambda V transpose kernel matrix has this composition, a lambda prime, a transpose. So the result is first this eigenvalues, they are equal to m times the eigenvalues of this matrix. And V is a linear transformation or linear combination of the data file. And the elements or the combination they are i, which are exactly the eigen vectors of K. And this is the relationship between the primal space. There's one or a functional space or Hilbert space, whatever you wanna call it. And the dual space. So since the relationship between these eigenvalues and this one's is linear. And the relationship between this eigenvectors and these ones, it's linear. Then the properties of the properties of matrix C, they remain here. Rivalry assuming here that Augusts data, this as we have n different samples, is that the result clear? Is the result for us. And the proof of that. Once we understand the result, the proof is pretty simple. First, we use the representer theorem that says that the parameters of our estimator linear combination. In order to find these parameters. What we do is to minimize this point error, the mean square error, under-representation, right? And we restrict the, we restrict the parameters of a combination of B to B to be normal to have norm equal to one. So under these conditions, the representer theorem plots. So we can say, we can say that the parameters of our representation. They had a linear combination of the data. This is infinite times n. We know that this is infinite time sent to. So this should be n timestamp. This thing is a matrix of n timestamps. So we have a matrix that is the combination that from the data obtained, the Eigen vectors. Is that true? And in general, well, v and phi, they live in the same space. We have n Eigen, five vectors. And here we have an data that span the space. Roughly, roughly speaking, we cannot, we can think of this like that. This till they live in the same space. So one is a linear combination of the other one. Right? Now. Let me mute this. So with this simple thing here, assuming that this is true, we just need to change. We're using this, using this expression here, which ends V by phi k. So using this expression here and this one here, let's call this equation one to say goodbye. And point. And then what happens is that c times 5 a is equal to v times v is fine. Pain. And here we have Lambda, right? C, v is equal to b and b. And now we know that C is C. Yes, this thing. Again, Taiwan, this is 1 over n plus 1 over n, right? I forgot to see the expression that uses vectors. I put a 1 over n where I pass it into a matrix form. I don't have a one over and it has to be there. So c is equal to 1 over n phi, phi transpose. Then we have Phi again. Hey, this is equal to define a lambda. And this lambda contains only an eigenvalue of remember, not the infinite, non-infinite values. So this is n times n. The first we ignore it. And so we can multiply this, which is m times, right? And now I simplify this. I get the expression phi transpose phi is equal to n Lambda. And this matrix find us the kernel matrix. So k a is equal to m a is one. And as a result, because if this is the property found, the eigenvectors and eigenvalues, the primal space. We can say here that we have the same property. But for a. So they are the eigenvalues or eigenvectors of K and N. London are the Eigen fights. All right. Questions. Aggression. We determine a. Can you repeat please the question, how do you determine the matrix a? Is that just from that equation on the bottom left there. So I just need to decompose k into eigenvalues and eigenvectors. So a is the vector that if a matrix of eigenvectors and n lambda is my matrix of eigenvalues. All right? Is that clear enough? Yeah, I think so. All right. So let me call me 1 second. I need to find I forgot I forgot to do it is working. So I did find that. Well, we're going to go this. I hope. So. I'm going to show up. Share the screen. So you have to set your camera on the screen. Secondary yolk sac. You'll get on this. Yes. Thank thank you very much. So yes. Alright, so what do we have here? Well, we have sample data which is centered around the origin. And then we want to do a pca decomposition of it. So let me run the experiment and get. So what do I do here? Here? I compute. I compute. So what you see here is the transformation of the original space, this one, this original space in a space whose, whose coordinates are the Eigen vectors. And then the data that is around here possess to this space. And you can see what would they represent here has a density, of course not the data density of the data. And you can see that the two components, they are independent. And then I divide these two dimensions times they corresponding eigenvalues. And then what you see here is distribution, which is suitable only symmetric. And the valence around each one, the axis is 1. So what they did is to transform the data using the Eigen to interspace of eigen values are then I divide times the corresponding Eigen, again values. And then this is what I have. This equipotential lines are the density. I can put them back into the original space. And this is what they have. So a half our representation of the density, original density of the paper using my eigenvalues ligament. So think works. Basically, if I use the first Eigen vector, my data will be represented in a line, which is this one. And you can see that if I ignore this component, that representation is, is still good. I have a low error in the representation of the data if I project out of it. And this is what the PCA finds. Now, let's use our representation. Let's see if this works. Representation. That doesn't contain a Gaussian distribution. This is actually three Gaussians. Same space, right? So I generate data that has a probability of being drawn from each one of these Gaussians. That probability is 1 third, so we have the same data in this three clusters. There are three Gaussians by direction that the distribution is not, my gosh, it's a linear combination of production. So it's not that it doesn't have the properties of gosh, if I tried to do the same thing as this. This is my representational. Now, it says that I have a principal direction in this. This is the principal direction. So basically it doesn't work. Right? If I represent data which is around here, around here, it might work, but how about this data? Right? It doesn't work because my representation is not. I don't know if I should know if you see it properly. But anyways, they they method, thanks that when they have is a Gaussian. But I've done a half Akash, Right? It's kind of useless. Now, let's put that, that thing into a Hilbert space with infinite dimensions. So in this data can be approximated by a hyperplane. Basically. Basically all my data will be in a hybrid plan. Here, I cannot fit all my data in a line, which is what? This is what the PCA tries to do. I have two dimensions are only two principal directions. That they are. The one with the highest eigenvalue is this one. And my representation with just one eigenvalue will be just poor. But let's put everything into our infinite-dimensional space. Is three-dimensional. So I can fit my data in a hyperplane, in a single hyperplane. This hyperplane, it will have several directions, right? This hyperplane, it's gotta be a subspace of the space spanned by the data I have here. I think I have, I don't know, 100 data, something like that. There are hundreds of data. So the subspace span by this data in a Hilbert space, it will have hundreds of dimensions. I can construct a hyperplane with a few dimensions inside this space that fits all the data using a PC it. So what I'm gonna do is exactly this. This is you have this in your in your in your number. So this is what we got. But I should have more. Let me do it here. Okay. Now, let me explain things first. You'll see it here. I represent the data, which is much better. Baselines. They represent, they estimated density. When the idea is to do the representation in the Hilbert space. And assuming that the data is Gaussian, then I can, in that space, I can compute it. Equipotential lines, put them back in the original space, and this is what I obtain. This is a representation of the data that is pretty accurate. It's telling me that around the three clusters, around the three means here, here and here, I have a high density of data. And out of down and out of this three center as the density of the datatype is low. So this is a figure here, is the actual density of the data, the three Gaussians in the input space. I put three Gaussians and this is my representation. It's actually not that good. But it works. This data now is represented using some eigen vectors in the, in the space. How many? Well, one. Again, one eigenvector. So this data here is projected in space into a line. Exactly. What did I do? I don't get the kernel matrix. I compute K equal to, well, a matrix whose elements I, j is equal to the kernel dot product between XI and XJ. And this is nothing but the exponential of minus 1 over 2 Sigma square x minus x j. And sigma. This is sigma. Sigma is equal to.6. Sigma is 0.6. I compute this kernel matrix, and then I compute the eigenvectors and eigenvalues of K side half. I use the MATLAB function. You can use the MATLAB function or a Python function to compute the, all the eigenvectors and eigenvalues. So on them, a lambda, sorry, lambda, a transpose. Using the notation that we had before. I put an n here, assuming that lambda is, lambda is the set of eigenvalues. The day dining halls bits. Now I take, I take the vector that has the highest eigenvalue, I take the eigenvector with the highest eigenvalue. So I have A1. That is an eigenvector with eigenvalue lambda, which is 10, 11, I don't use it for anything. I just use it to choose A1. Right? Now. I want to compute the error between the actual vectors in the Hilbert space and the projection of those vectors into this, into the corresponding eigenvalue. But this is not beg. For eigenvector. This is not a vector in the Hilbert space. They can better behavior space is V1 equal to a linear transformation of A1. So finds A1. You might say, well, wait a minute, A1, I have it. Fine. I don't, because we have different dimensions. And D1 then I don't have it either. It has infinite dimensions, but they can still compute the error. Right? Dave? The projection bearer eve of sample n is equal to five x of m minus phi of x times v1 dot product times V1. Then all of this, right? Do we agree with this? This is my original sample, any sample in the Hilbert space. And what does the rejection while I do the dot product between the sample and the vector, the eigenvector v1, sorry, v1, eigenvector V1. I multiply that times the vector n. And if I compute the distance between this and this, I have the error. So let's now compute this. E n squared. N squared is a scalar. I must be able then to find a solution for this. So this is equal to phi n phi of x transpose times 5 plus This product squared. Let's see, find events on and on V1 squared times v1, v1 squared, which is 1 minus this times this, which is fine. X1, V1 times this times it's for us. Now here, what we have is the dot product of a vector with itself. The dot product of a vector with itself is the exponential of 0, which is 1. So this is one. Now, what is phi of x n times V1 is equal to the dot product of this transpose here, pass V1 by b1 is this. So this is what I call K of x of n vector. That is the dot product of the sample with all the samples. So this is a matter of n components because we have that here. We have n dot product. So this is equal to k of x sub n transpose times 81. And now, well, k of x sub n is the vector that contains K of x of n times x one to K of x of n and k as that function. Then I want a Corolla we chose. Now we have two vectors of n components that we can compute. This I have it is the first eigenvector decomposition. And this is the data in the dual space of valence. So this is the dot product between my sample Phi of Zang in this space, my vector k1. This is a vector, sorry, I tried. So. If I summarize this, what we have is k of length of n transpose times p1 hat is this cell that this has an easy expression. The error of sample is equal to one plus this quantity here, which is this dot product k transpose of x of n and A1. And then here we have, again the same thing. We have minus twice this quantity that we know that this is k transpose of x and n times a1. And here we have exactly the same again. And so we have this. So this is equal to 1 minus k transpose of a will stay one. This is my, this is mining. Yes. K turns out to be equal to the Eigenvalue Eigenvector, sorry, then the representation as error equal to 0. Right? Here we have A1, this presentation us. Now an exercise for you. Here we use just one. Again. In fact, we assume that x is a member of the training data. So can I erase this part of the screen? So one proof, if phi of n, x of n belongs to, It's in fine, right? So if X event is part of the training data we use for composition, then he tends to 0. If there's no representation, is done with all this eigenvectors, right? To do that, you will need to use the fact that phi times phi transpose and five escape, right? We use the hint is 5.5, okay? To use this. And of course, this is equal to n lambda, lambda, lambda transpose. Rights ideas these five, daniel or write to the progress or not, the error tests to 0. If I compute the representation of each one of these vectors, any one of these vectors with all of the eigen vectors. And the two. If phi of n, pi of x doesn't belong to find, if it doesn't belong to find, then the error and still constant. So if I use, if I try to do condition of any other method that doesn't belong to the, to this said. Then I have a limit on the error which is constant, it's not 0. I cannot represent a new vector. Or whenever it, what does it? And so if you think with this n elements, we construct a space of n dimensions. Any new element will add an additional dimension. And this is the error. Right? Then you better, we'll, we'll add another dimension. So I can have represented with a previous, whichever. Alright. So what they represent if there is not exactly what they call the density, is not exactly a density, right? But it's similar to it. What they do is to compute the exponential of this. Write the exponential of minus the error. And then represented there. Is it justify as well? They, they density is proportional to the exponential of minus one over two. The error between the sample mean, which is 0, in this case, x minus mu 45 phi n minus mu transpose sigma phi n minus Mu. Right? So basically they are the density is squared distance. Right? And I do an approximation of a rough approximation. When they do is I take any point, any point in space, and I compute the representation error like this. And so I compute the exponential of that. Never estimation error tends to 0 here, or here or there. And so the Spanish restaurant. And outside of this reservation error is high. And then exponential passes. That's of you see here. Right? So in that script, where do you see is first compute the eigenvector, compute, compute the first eigenvector of k, etc. Third, compute this. Now. That's a three steps. And of course represented the exponential, we're right. And so this is one eigenvector. Again. I can put two, for example, or 10 and run it again and see what happens. Same. Now, we compute if we put more vectors, then let's put how many data I have. One habit. So if you many vectors in your representation, basically, they representation tends to start ignoring the distribution. And what it does is to try and fit all the data. This as a Gaussian anymore, this doesn't look as good as what I am. What I am doing is overfitting here. I am adding many dimensions in my representation. I am using a plane that has a button. Each harmonic is, I mentioned exactly, exactly equal to 101. Because I use a space 100-dimensional, a planar 100 images represent. So another thing that we can say is, well, that will be the original representation. But I don't know what's frontier say. Okay. I don't know. It's provided again. I don't know. I don't really like disapprobation as much as I do. All right, so we have this representation now. Let's use, let's use kernel pain with, with a white which is way smaller. So they Gaussians, so they predict the score exponentials, they will, they will be tight, right? Basically it will be able to represent anything with that. So let's put 0.1 and see what happens. This is what happens if I have a Gaussian. So if I have a square exponential, which is very, very tight, remember, what I'm doing is to transform the data into a space through a nonlinear transformation, which is a polynomial of infinite order. By in this infinite order polynomial, many, many degrees will have a significant where we're seeing Michigan. All right? Remember that representation of my exponential. The exponential of a distance is equal to one over k times the, times the 2 k over Sigma squared. So if I think it's like that, so if sigma is very small, one over sigma will be high. And then this polynomial will have significant elements for a high degree k. So basically, I am transporting the data into a space that has many affective dimensions. And then the result is that I can represent everything, whatever. So the representation of this data here and here and here, it has a lower. Whatever data that I want to represent here, for example, it will have a lower. So I am over fitting again. Clearly the geography. If lambda, sorry, if the parameter is equal to, let's put a two. Then what happens is that I am passing the data into a space with a low number of effective dimensions. So here I am under fitting. I am clearly under fitting because it's not true that we have data here. Right? What is a way to, what is a way to compute the value of sigma? Well, we have to do this representation using an approximation of the density like. Antibiotics. And we have to compute the likelihood of the data through this Udacity. But we will talk a lot more about what does this mean. So here we're assuming that my data is Gaussian in a Hilbert space. So in that space I construct a Gaussian distribution. How is, how well represented as the data following that Gaussian distribution? Well, here we see that we're assuming that the data around here, it has a high density. So it will have a high likelihoods and the data here or here, they will have a lower, a lower likelihood. All right? So this will give me a likelihood which is worse than if I put a parameter able to 0.6, for example. Like where it's giving high likelihood to points in the space where what I had data. So this will work, but we will talk about this a lot more. All right, Finally, let's see the eigenvectors and eigenvalues here. These are my eigenvalues. Of course I can activate your back, right? These are my eigenvalues. I see that one eigenvalue has a high value, is very high and the low, right? So just one is good enough. Representation of the Big Bang, the Cournot BCA am using kernels in general. Is that powerful? Okay? With this, can generate more data samples. With this, I can generate more that I can. I can construct a distribution that has this shape. And then no more data that will be similar, right? With this representation of a probability distribution. And if I draw data from this probability distribution, this data will be similar to this one. They will be concentrated around this three clusters. All right, I can muster a generative distribution where stamps, you have this in your UNM learn and you can experiment. You can change things until something, something crashes, right? Which is what we do. Now, I am assuming that the data is centered around the origin here by a representation into a Hilbert space that uses square exponential or any radial basis function. This is never true, right? Remember that they deem, remember that the data is always hyperfine it in the same hybrid wire. So all the data is around here. And the norm of other guys want the data clearly is not centered around the arch. Say if we do not centered the data up around the origin. Before we do this operation, we can see, right? In other words, the eigen values and eigenvectors of K, they are not the eigenvalues and eigenvectors of, of. They are not a representation of eigenvalues and eigenvectors, the eigenvectors in the he respects. So I have the center of the big, how do I said of the paper? The data item by item say. But again, I can do manipulations and I end up with a representation into a space and alternative matrix, K. This is in your slides, right in the slides here. And let me see here current instruction Eigenanalysis here. So prior to doing all this, but just let me just do it proper format. So I need to center the data around the biology and that's exactly what they need to do. And so in order to do that, I have to thank each one of my samples and the hero space and subtract the mean. That's it, right? So the mean is equal to 1 over n, the sum of vectors and the sum of vectors is equal to 0, the factors multiplied times a vector of n ones. And so the central matrix is equal to your original matrix times this matrix here, which is all my vectors multiplied times matrix, are ones. That computes the mean. And now I just need to compute the curl of a product here. As a product again, which is equal to the original going to minus this. Right? So this is all I need to do. And I say beauty, the algebra here, but it's, it's very straightforward. So I'm sorry, but I'm I'm I'm I'm showing we call it, but it's not it's not going to like this. All right. And I have a booster and everything. It just a allergies. They buy it, they came fiber, whatever. It is Y-shaped. So so this produces a kernel matrix that comes from data, of which they mean husbands obstructed. And so when we did this, and we compute the eigenvectors and eigenvalues of this matrix K. They correspond to the eigenvalues and eigenvectors of data centered around. Good. If you don't do this in general, the other it doesn't work. You can try. And if it's my banana, you good results. Sometimes it does. Why? Because even if they did, it's not centered around the origin, is very close to it, right? It's very close. The distance between the data and the sender is mined in a space of infant manages. All right. So in this particular case, for for many data, we have 300000 a year and I square exponential. It will work if you them used to metadata it somehow. All right, so in general, you have to. That's it. All right. That will give you a representation of the original space. In the original space, we do not subtract the paper. Right? We subset the data in the universe fixed, but you don't have to do it in the original space. And that's that's basically it for today. So I'm going to upload all the grades, extend. One, the one on the test a week from today. The next homework. Alright, because we finished state, that is have I will, I will upload this thing again because it contains some errors. And finally, I lead to you as an additional, additional study, 6.6, which is the reproducing property here. We explained why we call them reproducing product spaces. It because of what we call the reproducing property. Strictly speaking, we don't need this for our machine learning tasks, right? But it's something nice to know because it's in maybe part of future machine learning developments that your mind bypass. So take a look to it if you're interested, but you don't understand Dame developments, then let me know and be aware that we use this notation here, that they introduced. This notation. All right, k of x I dot my interviews that innovation with the heel. Remember I will expect, and that's okay. So what's the US or Western Netherlands? All right. So let's stop here and I see you. Oh, yeah.