 For now, I will fall. So we started today with, I'm sorry, I knew module which is on this. Do it. But it's all right. Because what we have now is module where we will see the same structures pass before linear kernel structures. But what changes here is the criterion that we use. Let me put this on. Paranoid. People are naive of me. In this. Essentially we're going to see regression. The structures, as I said, they are the same, first linear, and then we will use kernels. We will again use the representer theorem in order to and our estimators with non-linear problem. By difference here is the criterion that we're going to use to optimize our estimator. And they are based on the age of, of course, here it says Gaussian processes because we are assuming a model for our Theta, which is Josh. And that simplifies things. But this can be generalized to other distributions. But before I start with this, Let's see the idea from an intuitive point of view. So we just need to understand the base. Alright, so here you have the slides again. Then I'm going to explain this. And a whiteboard, right? So the navel, I assume that you know, right? Right. But I want you to open this 10 info. It's actually very easy, right? I, very often I see people not really Good day, the other available, which is very simplistic, right? So let's first see what is c, What is a conditional probability distribution? And let's do it with a classic diagram, depends diagram that people use to represent that thing. So we have a space that is often called the University of probability formula. And so this represents in its coordinates all possible outcomes of an observation. Let's say for example, we have three coins and we toss that. And then we have some, some tails and heads. So here we have all the possible observations of this event. All right, so it's 3, 8 passive obedience. And then let's define a given event. For example, the probability of obtaining a combination of type tells her best price. For example, 1, 0, 0, 1 Is that, right? And so we call this a. This is the event a. So here inside this row, this seven. We have all the events where, for example, let me change the example. All the events where I have one tail and two next, right? So we're going to have three of those. And then we define a different event. So what is the probability that the number of tails is odd? So one or three. It turns out that we examine these two events. They overlap. There are situations in which both things happen at the same five, and situations where they do not. Like. So. I know what is the probability of a, I know the physics of the phenomenon. Right? So I have three coins that are perfectly symmetric. So the probability of tails, the probability of heads is the same warehouse. And our three of them, I know how to do Excel. I know all the possible events and their probabilities before I do the experiment. And this is why I call this a prior probability. I observe an event before I observe it, I know its probability. That's a prior. Now. Then somebody says, well, what is the probability now? I observe a and I say, I wonder where I know by then somebody else has signed information about an event that actually happened, but I did not observe. So an Event Hub and I know I'm going to say it, but somebody tells me some side information about this event. So what is the probability of a if I know that B happened? Right? Somebody tossing coins and they say, what is the probability of having one tail? If I tell you that the number of tails is that the probability changes and actually increases. It must be addressed because the universe and probably this has been produced. Right? Nobody asked me, well, I'll happened or it didn't, but what is the probability? And then somebody else says B happened, then what is the probability of a? If B happened? The universe or probabilities collapses to this. Right? So then the probability is different. In my increasing my degrees, of course, right? If somebody beam is B as the event space, no tails, then the probability becomes is probably the changes. If I get, if I know side information, right? So a is an event that I cannot observe. I had to guess the probability. And b is something that can be observed. And that changes the probability. Right? So if I had to bet about a, it's better if I know. So, you know, the probability of a, the probability of the event day, I know and it's up higher. It's up higher distribution of prior probability, right? Because I know it without needing to see anything or the experiment. And then I have B. B is an observation, observation, and I know the probability of B, right? And so the probability of a given the knowledge of B, this is knowledge. Knowledge is my observation. The probability of a given B is equal to this probability here is over this probability. So the probability of a and B given a. This is, this is the definition of conditional probability. And one might argue, Well, I don't know the probability of a and B, okay? If I knew it, then I compute this. So this is a joint probability. This is probability of a and B happen at the same. It's this set of events. And since the probability of a is up higher. Then we can say that the probability of B is, the probability of a given B is our posterior. It is a posted price. And we assume here, we're assuming that a is not observable. It's not observable. This is why I want to know the probabilities. Is that clear for everyone? So we have a prior so far in our state. All right, So then I say, I don't know the probability of a and B. I know the value of a. I know the velocity of B because they are two events and phenomena that they now find the value of a and B. This thing, it might be difficult. But if you take this, this equation is symmetric. If I change the probability of a by probability of B, that this thing doesn't change. And here I will have an a. So the probability of B given a is again, the probability of a and B over the probability of a. B is known. B is an observation. So this is not, that was the deal, right? This is a posterior because a is, I know it's not observable. This is not a posterior. B is now why we call this a livelihood. So this probability, and it has a different interpretation here. The habit, right? And this probably that tells me what is the frequency of appearances or the appearance of B. If a happened? So underline under the hypothesis that be that a hub and how likely is to observe b, right? To be happy. Right? And then what happens here with this probability is that given the hypothesis over a, we can compute how lucky we are to observe this. If this probability is very high, it has a high likelihood. If this probability is very low, then it has allowed. All right, so if we have a low probability, here, we are observing a phenomenon with an algorithm, right? That is the difference between this. They don't need, they don't mean the same, the same because of the fact that B is an observation and a is our latent variable. It's a latent variable meaning that it's not observable. I do not observe it, I cannot. I'm sorry, I will never be able to observe. This is very important. Right? So then we can combine these two because we have this probability here. That bothers me, it bothers me. So I just take you out. And I can say, the probability of a given B times probability of B equal to the probability of B given a times the probability of a. And I come back to this. What is the probability of a given B? Which is what is interesting for me, is the probability of B given a times the probability of a over the probability of V. And this is my posterior. So the probability after the knowledge, this is the prior. And SBS. This here is an IEP. This is a prior. A final I don't know. I'm sorry. I don't know what happened. Right. So then I want to let their mind. Yes. So we don't know. We do have knowledge about the k, but we have we have a knowledge about the phenomenon in general by not about the instance I watched what happened now. All right, so we toss coins. But when I see you or basically we bet on some, we can embed on some outcome that we withdraw them and some money. Right? So then I had to choose a maximum probability. But I don't know, I don't see it. Right? So they can this be an a band? Right. So assume that a is not a given. A yes, Any anything? I don't know it. So I can choose the value of a or they've already been a, that has the maximum probability. So since I know B, I can try here all possible values of a and keep the one that gives me this maximum probability. If I maximize this equation with respect to a. What I am doing is to applying a maximum, a posteriori criteria. In order to maximize this. I do not need be, I don't know. This doesn't depend on n. So again, just ignore it. And here comes the sentence that I say the most. Throughout the year when my glasses. Posterior is proportional to the likelihood times the prior. We will use it. Write an email to my machine learning class next semester. You are here, There's many types because the area is proportional to the likelihood times the prior. Maximizing this is called maximum a posteriori. I will just take a shortcut, can save. I had the likelihood here. I have this like if I know I do because I'm using it. Then given the observations, what I might do is to maximize this likelihood with respect to a. So I can maximize its proper name, the probability, and I chose the corresponding value of a. I can choose the value or the event a that maximizes this probability. This is called maximum plug-in. All right, we're going to combine both the maximization of a posterior than the maximization of value. And with this, we have everything we need. In order to start treatment site. They construct a Gaussian Gaussian processes. They need a probabilistic model for the for the latent variable, for what we want to infer. In a chains. We don't need to cross-pollinate any parameter. Because all the parameters, they will be optimized by maximum a posteriori or maximum. Maximum a posteriori and maximum value when values above. So having this avoids the need for cross-validation, which is a good thing. Because you have been experiencing the difficulties of cross validating parameters image. What does the guards? Well, when in the model for the data, we need a probabilistic model for data which didn't happen in support vector machines. Right? We get something, we have to pay something. Right? Again, than overlaps, theorems appear. Like a mushroom. You fully understand a few theorems. You have the machine learning side, your grades, and you can understand what's going on. So this, what I explained here is let me share screen. Share screen. Where here. What is the share screen? Here? So this is what I explained with an example on the right. So you have this his lungs. This is what we call the Bayes rule, and this operation is called Bayesian inference. We are doing inference on a type of knowledge is the knowledge that we have with the observations which are now other, we translate this into machine learning. Well, I, at this point, I think that you have an idea. We have some observation. And the observation we want to infer some latent variable, right? So we have x. That's my observation. And I want to use it to infer Y, infer the value of the label of the already have regression, regression. And here we do it using a Bayesian perspective or a Bayesian framework. A probabilistic frame is not difficult. It involves, I'm sorry, I'm losing my voice. It involves a lot of those. Algebra from a conceptual point of view is not. So. Now, let's take this and put it inside our machine learning models. So we have no random variable W, which is our set of parameters for my month, right? We have y equal to w transpose x. W is a, w is eternal. It is a random variable, right? Sorry my vocal cords and submerged volume. Because now thank you. Thank you. It's a one because that was what I was just so soluble. That is not a set of parameters that we can adjust. For. Now we consider that W is a random variable and it is made, we do not observe it. We cannot observe it, right? But we can do influence on how do we do now? Well, during a training set, during training, I have a set of observations. X and y plus y is not observable, but during training it is. And so we have y and x as knowledge. And what we want to do is to compute the probability of w given the knowledge of y. We just need to apply the navel. This is a posterior probability because w is not known and x and y they are, No, I insist we are in training. Why applying the Bible? I can say, well, the probability of X and Y, this is, or might be, this is my b, this is my a is equal to the probability of x and y just be given w times the probability of W over the probability of y and x, right? This is nicer. This is W is my a. And X. And one day I might be right here. So what happens is that this thing is not something that I can, that I can obtain in a very natural way. It's kind of weird. I want something that I can interbreed anymore, but an easier way. So this is a joint probability, the joint probability between y, x given the value of. And then here comes another rule. Conditional probability is probability. So it must have the same properties of non conditional probability. In other words, if this is true, then this must be true. As yours. How do I see? I see yes. A condition, a new condition, right? And then this is equal to the probability of B given a and an information times the probability of a given condition or the probability of B given a new edition which is C. So this condition will stay there. I'm going to have to change. If this is true, this must be true because conditional probability is profit. So with There's my first, I am now changing this, this joint probability using the definition of conditional probability. The probability of Y given X is equal to the probability of B divided by the banks. So I'm basically using one of these equations, but I am isolating the joint probability distributions. Here. I do exactly the same by I have a foundation that stays where it was. Sort of product of X and Y given w is equal to the probability of X given Y, given X and W times the probability of X given w, x and w, they are independent. Let me write all this in a white wall. As I said, I bought tickets there. So the probability of Y given X, by definition of the definition of conditional probability can be express. Generality can be expressed like that. And now I add here a condition w. So this must be equal to, Let's write this condition and stays there. And now I said the probability of X given W, basically probability of x. Why is this? Well, let's observe, let's analyze our, our estimator model. We have x here and then we input it in a machine that has a set of parameters and I have an estimation of y. This is what I drove. So it turns out that y will change. If I change the value, right? If I put different values of w, I will get different values of this, basically different ways of whoever they want to do there is to minimize that error. So the output will, they can really panel on w by the observation is given. It's not going to change as a function of w, right? The probability of X given w is equal to the probability of x, right? The knowledge of W doesn't change x, right? So this is called independencies. When this happens, they still, they are independent. Right? You can prove, you can prove using this, that the probability of x, w is equal to the product of probabilities. As an exercise. And the probability of independent events are independent random variables. They are independent. This is true. This is not the definition of independency. By the way, this is not the definition of independence. This is a property of independent random variables. The definition of independency is that we will commonly be more of an idea. Now or so. Now, I have these three equations. These two equations are clear for everyone, right? That's 11, are running on the neonate. They didn't change this. That happened to me twice a year. So it's theorems have that. Perfect. All right, so then I hope that's three questions are clear for everyone. Then if I combine them, if I change this by this and this, by this, then I obtain this expression. And then these probabilities, the probability of X with us, they both way. And this is when I have, this is now my posterior. Remember W is, I know this is my studio. This is my prior, prior distribution. And this is my line, right? And this is a livelihood that they can compute in a very way. Why? Because I know why. I know why. It's an observation because this is training data. And I know tags. And then I can, I can put any value of W here. And then depending on the value of w that I put here, this probability will change. Write this as normal. I want to compute the probability of Y given something that I really know x and something that I don't know W, right? So, so this for me is being, and this is a, right, and a is some event, which I know just a part of it. I know X sub a is partially, I know. And again, what I want to do here is to maximize this probability. Given this observation. This is a posterior. I need to, in order to maximize this, what I need to do is to modify w in order to maximize this product. This, it doesn't need to be maximized. I mean, this is a constant with respect to w, Right? So here again, the probability, the posterior probability is proportional to the prior w times the vacuum. And this is a fundamental equation that we are going to, what are the two fundamental equations that we're going to use in our demo. This is what you have to remember. You remember they shaped the form of the posterior is the probability or whatever I want to infer, which is w, given the observations. And then what is for us the primer. You have any knowledge. And the likelihood, the likelihood of why we use the likelihood of y given this to which this is not an angle. Still, Thanks. Still things I kind of obscure. But they will become clear when we put particular distributions here in sign. By that will be the conclusions of this. Of this part, we have an estimator that has this form w transpose x, and we have an error. This is the estimation error. If this is a set of training data, I know why. I know X and a latent variable. Yes, definitely. Right? And so w transpose x is often called a latent function. This is a linear function. We will use kernels later, but this is called a latent function because we don't know it. We don't know it. We will never know it. All right? But we can assign a value of w that maximizes the posterior probability. This is something of equal. So by using this equation here, W is Y. Y is Gaussian, or more. Particularly, if epsilon the error is Gaussian. Then we have been done this a lot. In order to do this, in order to maximize the posterior without really any Gaussian process and everything. We did that already. In our class. This is the first thing that we'll write. If we maximize the posterior by assuming that epsilon the error is Gaussian, we end up with the French regression. We will see it again, right? So why does the point Gaussian processes, we do not use the maximum a posteriori of w. We use them for Stadium, the whole probability distribution. And then we use this probability distribution to compute the posterior probability of regressor during the test. Once we have this posterior, then we use it to maximize the posterior probability of them. What are the opposite of problem given an observation during a test? And more than that, we do not just keep the maximum a posteriori. We keep the posterior probability of of our latent variable Y. The estimation given the input, given the input data, and given the tranquil bay. And why is this important? So in, in our previous, in our previous developments, Ghoshal processes or sorry, We're micromachine spellers. When we have this function f of x, which is w transpose X, that's it. And then we infer particular values for w. And so then we have different values for this, which is the estimation. Here. Instead of using W, we use the probability distribution of w. And that we would have a probability distribution for our estimation. And it's going to be a Gaussian. Which means that if we construct the distribution of this option, we have is our mean in abeyance. The mean would be our prediction. We'll be predictive. Confidence interval of outbreak. Say the confidence interval is type is small. Then we have a good prediction. In the confidence interval is, is y, then the prediction is not. And this is great because we have a machine that tells me not just the prediction, but it's telling me whether to trust it or not. Right? So we are predicting day and we are predicting day. Algae that when I use tomorrow based on the weather predictions, the observations from the past. And I give a prediction of a photon energy that we're going to use tomorrow in Albuquerque with a high confidence interval, then the utility will use this information to buy or sell energy before producing it, well, before using it. Alright? And so they dams. So thereby it be cheaper. But if I screw up and I don't buy enough energy, then I have to. Combine them as bad and it's going to be hybrid fancy. All right, so this is a big business. People and $8 million after this binding energy. So what I'm offense that when I get 10 milliamps, now, I don't know I've been stopped yet URL. So if I am able to produce a nice distribution with a, with a reasonable calculus or guardians. Then This machines, they can be used in operations that have, that are brisk. I can manage my risk, right? So if the government is tight, confidence is one that I have. Not risk in the sense of panic. And Chairman, I guess by risk in the sense of cost. Money, for example. In my MIB, not money by safety, many other things. So here the point is to compute the likelihood. This likelihood in a good way, probably. So what is the likelihood of line? This is the whole boat. If my likelihood is accurate, then I can use it. And I will obtain a really good posterior. By the likelihood is not accurate. The thing is not going to work. And this is actually what they pay for it, right? They, they are, they know freelance theorems. They said add-on machine learning algorithms. They have the same error if I try them with all possible problems in the universe. So if my machine is good for a given problem, I will for sure with probability 1. Finally, problems for which my machine sucks. So here, the machine will be optimal if my model is Gaussian and the data is actually Gaussian. But if the data is not Belgian, it's going to be worse than ours. So now, basically, at this point, we have to, to just go with this. We have our stereo. W. N is proportional to prior times likelihood. So now we're in a riot and we then, we need a line. Let's just give a hint on how this thing works. Find the same. So Let's use that for linear regression. So now we have, we have a linear estimator, which is like done. And notice that they've bias doesn't appear in the equation. And just because we put it inside a W, remember that I told you. In some cases we will put inside some business attire here. Things are a little bit easier. If we do a W, it will be inside of W, Right? So x, it has a constant here. All right, and this constant is multiplied times b here in this dark brown. And by the way, what we have is exactly the same instructor before. And so, whoops, cubed. Now, the Arab. And this is a key point. What is the error here? We're assuming that the error, yes, that particular Gaussian process. How, how do I define a Gaussian process? So, assume that you have a sequence, a sequence of observations, E epsilon, epsilon one, epsilon two log n, right? And then you get, you take an arbitrary subset of B sequence. You take epsilon 1, seven, epsilon for them, whatever they want. And then you estimate the distribution of this subset. Well, within dependency on some of that, I get this. This. Subsequence is drawn from a Gaussian distribution. All right? Say I have three, any three samples. These three samples, they are drawn from a Gaussian of dimension three. Right? So any subset of the sequence, it has a Gaussian distribution. Multi, multi-dimensional. Gotcha music. And in particular, this is a Gaussian process which is IID independent and identically. So this is a really easy Gaussian process. Every sample is drawn from a Gaussian and univariate Gaussian. And all the samples, they are drawn from the same gosh. And they are independent. In particular, the correlation between or the covariance between any two samples is 0. Right? So it's the simplest possible Gaussian process, is a set of identically independent, identically distributed Gaussian with, all right. So call that particular Gaussian process, we call it, we call it additive white Gaussian box. Additive white Gaussian, most women, why? Because if we compute the spectrum of a sequence, when we get is a constant, we compute the spectrum of a set of identically distributed, identically distributed. Suppose that what we have is a constant. So it has all frequencies. And this is why we call it white. Plus it's Gaussian because the distribution is, is each one of these emission spectrum. Well, we've got a noise, but this is not dogs, alright? It's error. Error or noise. There are two different things. This is my estimation error. And it may contain noise that comes from why it can. We may contain noise that comes from the observation apply. It may contain noise in x, right? So epsilon, the error might be a cause of observation noise, but it's not necessarily that is in general the other, this error is going to be higher or lower depending on my choice of w. So please do not, do not mix it with the concept of noise. And I say that because in the equations later, it's confusing. People use the term noise for the error. And this will have a power or a variance equal to Sigma squared, where n stands for notes. But it's just a traditional way of doing things. This is nonetheless, it's in general better. It may or may not protect those. So with this, we can establish the lagging or the y, right? So for a given sample, for a given sample by x, y will have a mean value. The mean value is equal to x transpose w or w transpose x. This is the mean value of y. And this mean value is, is added an error epsilon. And this error is the mean minus the actual value. All right, So here what we have is epsilon an instant or ensemble n. And since b Epsilon, then the error is Gaussian with power sigma squared, its distribution is like that, right? Yes. Is this fully understood? Does anyone have questions about this? Anyways, I'm going to admit we have a noise or an error, that error, which is Gaussian in nature. And so it's one of the samples or the noise or the error or independent to the effects of 300, right? So we can represent the distribution of one somewhat on this arrow, right? Instead, what we do is to represent the distribution or WhatsApp, all of why the sample, why it has the same distribution. Or the Yammer by the error has 0 mean, 0 mean. And why it doesn't. As the mean, which is x, f of x of w transpose x. So y and epsilon, they have the same distribution, the same variance, except y has a mean. And so the distribution is a Gaussian tense. It has this expression, right? Y minus the mean over two sigma square exponential. This is the normalization factor. So the integral of that touchstone, this is the enlightenment. Is y as a function of x. This is the likelihood of what, this is what I need. But this is just one sample test. For one sample, I want. I want to compute w as a function of all training samples at the same time, Mencius one. So far we use one-sample. This is useless. I want all of them. So what I want is to compute the likelihood of all values of y given x and w. But I assume that I assume that epsilon, they are independent values. So why they are independent? And when two random variables are independent, the joint distribution is equal to the product AB institutions. So if I wanted livelihood of all values of y, this vector, given this matrix with containing all the data and w. Then what I have is when I have to do is to compute the distributions. And I know that these distributions are equal in distribution. Right? So at this point I might say epsilon, epsilon of band in this case are IID. So in particular they are independent. And why they are not necessarily independent. Right? So let's assume that I observe y of n as a function of time. And it goes like that. It might be okay, fine. Right? So I might add a noise and I have something like that. So a given value of y is not independent of what happens before and after. If this is sinusoidal, then this and then suddenly go up to a certain noise. In this case, they're equal, right? And the premiums are an example of why they are almost equal. So they are not independent in any way. By epsilon. Yes, it has. Epsilon is, in this particular example, it's just this noise. I am. And I put a sample of noise. And the next one, it's going to be independent of the first one, the previous one, and so on. But then I'm mixing these two concepts. What am I doing here? Am I saying, do I say that Y are independent? When I do this? Do I say that? I said that the error is a set of independent samples and indigo distributed. Does this mean that y are independent? Because I'm using y here. That's I'm asking. So the answer is no. They are not being selected. Of course, I'm not saying if I said that Y are independent, then I would say probably the 0, y is equal to the product. But here, what I'm saying is the probability of Y given, given W and X is a random probability. Which I mean, which means that Y, Y are conditionally independent. This name here is a sequence which is not in the Bible. But if I know X and I know W, they become independent. So this is a process that comes from, this is y and this is x. And this is a process or business Gibson. This is a process that comes from an input. All right. If I know the input and I know you, then I don't need anything else to compute y because the error is independent of anything else. So I cannot predict that. If I know X and I know W, the knowledge of the previous value of y or the next value of y, or the value of y. After a failure, it's useless, it becomes useless. So if I know w and I know x samples, why they become independent, this is called conditional independency, is called conditional independence. So we might say that the probability of Y i given y j is not equal to the probability of y minus y. Right? So if I know the previous sample or an example that probably changes with, but I won't give you what I want. If I know I plus 1, then with high probability, Hi of y is almost equal, right? So this is not true. The probability of Y given X and W and y of I plus one. Then it is able to list the probability of Y given X. Because the knowledge of an example now becomes useless. If I now I have everything. So this is true events. There's two samples are conditionally interact, right? And this is what I'm saying. Here. They say there's good evidence that because sometimes there is another computer. Whoops. Right? So going back to this, going back to us today, to this line, I have this distribution and I put it here. And then I have the distribution of ball observations of y given the observations of X and W, which is simply product of identical Gaussians. And ending up, they have the same mean and activates the same for all remain the same. And so this product of exponentials can be written like that as a single exponential. Right? So let me go back to the slide today. Or in order to illustrate this a little bit. If I had the product of a given exponential, the exponential of minus. Let's see. Let's use epsilon squared over two sigma squared. This is equal to the exponential minus Theta minus 1 over 2 Sigma squared times the summation of sigma x. Alright? From here to here, it should be clear, right? So it's a product of exponentials. And so it's equal to the exponential of the sum of x. And this is the norm of the error vector. So if I define the error vector as epsilon one to epsilon n, I assume that I have a column vector, we add data, then they ignore the square. No knowledge of epsilon is equal to the sum of its components squared. All right, So I can write this as exponential of minus 1 over 2 Sigma squared times the norm of the error here. And finally, this as the exponential of minus 1 over 2 Sigma squared times what is the error? The error vector is 0, sum of Y minus ensembles minus 0 samples x far away. This is done. So this is a column. So here we have what I, I ignore, I know named constants, they normalization constants, right? But is straightforward. And so this can be also written as the exponential of minus 1 over 2 Sigma squared can see Y squared times y minus W x transpose w transpose times the identity times y minus x transpose w. All right? So when we cases multidimensional Gaussian, sorry. I guess I need to finish, right? Actually this is what they wanted. So I have here the product of exponentials of a random variable that are independent and identically distributed. So this product here can be changed by an exponential whose argument is a sum of arguments here. And so this sum here is equal to the square norm of the error vector. If I assume that the other vector as a column vector with all the errors than the norm of this vector is equal to the sum of its components is correct. So this, I can write it like that. And finally, the error is equal to y minus w transpose x or x transpose w. And this, I can write it as y minus X transpose W to obtain the same. So this is a Gaussian distribution. Covariance is a diagonal. And the elements of the day I would see minds, but yes, is the w's that the scale is a vector. And as always, I'm hyper, says management's, right. So this is a Gaussian whose covariance, the covariance here. And covariance is equal to sigma n squared times the identity, which is what corresponds to a distribution whose components are independent and identically distributed. This is my likelihood is very simplistic. Of course, if I put this right away and then I ignore the rest, It's not simplistic. Write the symbol points to explain everything. And so it should be like that. So for this class, please be sure that you are able to understand and be able to reproduce the concepts and the derivation of the devo. That first. Second, how do we apply the Bayes rule to compute the posterior of w? Second, uncertain given and the posterior is the product and O'Brian likelihood. How do we find this language? Which is what I explained in this in these slides, two slides. And it's very important that we are, that we master these three concepts are sets of concepts in order to progress in Gaussian process. Right? Because it's industrial processes, they are very simple, but they need. This is the most important thing to understand the rest. Of course, there is a little bit more algebra after this because so far we have it. We don't have a machine. We don't have a learning machine. We have just probability and a model for one. Okay? But this is the modern thing. Please be sure that you master these concepts. And it was Dems. Now. So let's stop here. So I give a brush.