 So let's go to the next set of slides. Remember that for Sadie, right? We first need a while. We have paranoid than others, but y is equal to the w transpose F and an error. And then we assume that the error is Gaussian distributed as a Gaussian distribution with mean and a variance, sigma square edges and nodes, right? Several slides. By the way. So let's start with this here. So this is true. Then why can we model set samples drawn from a Gaussian distribution whose mean is w transpose x equal to c squared. So a livelihood vector y and then solve for y. As a column vector. Is Y is normal with mean. Whose mean is x transpose w, where x is the set of all samples, the matrix containing all samples from X1 to Xn, apical end. And covariance matrix, which is sigma squared, times the density. Now, we assume that W is a random variable. W vector W puts the bias here. W is a random variable and we model it arbitrarily as something that has 0 mean and a covariance matrix sigma. And he's also arbitrary. We make querying this up because we don't know anything about WL by what they want here is to compute the posterior probability. Posterior, which is proportional to the prior times likelihood of this being the priors. And with this, we have that w, given that set of training samples, is drawn from a normal, has a mean and covariance matrix as we go a minus one. And w. While a is equal to c minus x, x transpose plus sigma p minus one. So the covariance matrix is the inverse of this quantity here. And then they mean, or a maximum, a posteriori value for W is equal to x, x transpose sigma n square sigma p minus one minus one X one. Based on the matrix system. That is posterior. This is the posterior. So this is the probability distribution of the possible values of W, Dina and the knowledge of x and y, which are a training samples. Finally, we want to know a posterior probability of f star, which is the, which is the prediction power by new symbol x star. This is a latent function, something that we cannot observe because w, x and w cannot be observed. But since we have the distribution of w, Then we have day likelihood of DNA value of w. Then we can compute the posterior probability f star, which is equal to the integral of the probability distribution of F given the sample banks. And an arbitrary value of W times probability distribution of w given x and y. While I may not be funny like that, which is this, this is a probability of w given x and y, right? So, and we integrate with respect to w. So this is a Gaussian, this is a Gaussian. So this integral over all possible values of W, this is a vector. So this is our integral over all elements of this vector. Since these two, they are Gaussians. This will be a univariate Gaussian with a mean and a covariance. And then the mean. The mean. Stein mean, is equal to the maximum a posteriori value of W. And the variance is equal to u x star transpose a minus 1. Thanks time. And this is all we need to characterize the distribution of this. It's important to be aware that the mean changes for as a function of x. Of course, this would be my prediction. Because this is the thing, this is the only thing that I can observe. And this also changes with x. For some values of x, the gradient will be higher, right? And this is usually, here are the five that we don't have training samples close to x star. So we don't have informational part interpolation or a prediction. And a, we have values of x here, inside, here, inside that are close to x star. Then we will have more information like they vary a little bit higher. Of course. We have to buy to this the variance of the error. All right? This is the variance of the latent function f. Then we have here the variance of the nodes, right? In other words, I run out of space, parameterizes the latent value for y star. And this is something that we cannot observe, is equal to f star plus an error like that. We observe them. Well, actually we do not write, but we have also been ever, right? So here we have the mean and F and it's stadiums. And we also have the variance of the error. So sigma y squared is equal to and they are independent, so the same as they can be added together. And this is whether we should not forget about it because it's not, it's not too explicit in the bowl by the name of the guy. I never remember these names, etc, mathematician. The one who who cancel this. Or I don't want to, just to forget the name of that. When we go to Student View. And click here on out of seven. The idea and even put it here, or it will raise, well, you just need to type Gaussian or success. Or question. I have. Here. This is the bug and weights, Rasmussen Williams. They explain Amazon and ads for free, right? It, that original word. And it's for free. It's an MIT above. And you have all the lessons here, condemns. We are basically now our focus in Chapter 2 regression. And then we will talk about covariance functions and model selection by our outcome and more about that later. All right, save. So Rasmussen was not too explicit about this. And so many of my students, when they compute this, when I compute the variance of the prediction, they only include this quantity by not this one. Hi, it's important to include this quantity here, zeta and test. When you are or if you have a training set and a test set for which you actually have the values of y star. You can compute the error. And the error. The value of the error should be 68% of times inside or less than one signal. But this signal, and it should be 95% or something like that of times less than the sigma. All right, so if you do not have this, the numbers done much in some places this one is negligible and things they will work anyways, but sometimes these quantities not make it. We will, we know how to compute less as long as we have this point here and we still don't know about you. Well, I gave you a how to compute this, right? 90. Basically when they said yes. Okay, day the likelihood of their training samples and optimize it with respect to Sigma squared. And then we will have, are they right? And so this last slide, this slide is the one that explains or slave, same that explained here what we want this to be a dysfunction, but it's made that we've done I know it because WB don't know it. But we have a posterior distribution on w and we month unarguable for this, right? And then so we put them together, we get the rate. We have a stereo probability distribution of F, like a lecture, given the test sample and United finding samples only. You is not here, right? And this is a Gaussian, as I said, the solution to. So this is a Gaussian. The only thing that we'd like to do is to compute the mean or typical behaviors and visit them about it. We really need to explicitly compute this. Yes, these two are not valid chance if we choose something else. We need to explicitly solve this integral, which as in many times not possible. So we need to use numerical methods, mostly about Monte-Carlo methods there, right? Right, numerical methods to solve this integral. So that combination operating bridges. And buy. It still works. In general if we do not use this IA, IB, IC, but if you do not use this Dr. W. Transpose Ax, we use others doctors, for example, deep learning, but we still want to apply Asian perspective to it. Than nothing of this is useful because we're going to solve the integrals. So we need to use numerical methods. I want to use Monte Carlo, we have to use what we call variational inference. Its conjugate, priors and everything. So everything is more complicated. It's not difficult to understand when it's cumbersome. But still it's, using. A Bayesian perspective is useful because instead of having a prediction, we have a distribution, our prediction, or an approximation to this division or the religion, which tells me whether I can trust their prediction or not. Right? And that gives a lot of added value to the learning machines that I construct. And of course, again, they're not. They nonetheless theorems role and we have to pay something. I would have to pay for an increased computational burden on a model, for a probabilistic model, for why. This is something that, it's not always straightforward. So they said, I don't know if you have any questions at this point. Remember that I asked you to review all this. So it's clear in your mind because now we're going to rebuild the same. Well, pretty much we're going to use this results. In fact, even this one. This result here, there's one here, these two results. And we simply, we simply put here five. So let's, let's see what happens. If I put here a data transformed into a Hilbert space of infamy nitrogen, then what happens is that my model, my model will be non-linear. But the same considerations hole, right? But it's, it, it takes a little algebra to pass on this, all these equations into a Hebrew space. There are many, well, at least I know three ways of doing things like that. I'm going to present one. So if I finish it, I will post a paper in which I have the rest of the page proofs for you to a table whenever it's not important, the important thing is that we understand the results. So let's go to the next set of slides. Let's go to here is our result. In the data model for my set of parameters w that I end up using the Bayes rule several times and not the biology rule, which is that in the role that last integral. We have a model with a mean and a covariance. The covariance it has is dangerous antibiotic classes expression. And the mean has this expression. So where do we go now? We want to make this non-linear. And the only thing that we need to do is, as we did in support vector machines or any other algorithm that they present, that minimum mean square error Ration when willingness to fly, which is a matrix of samples, training samples that are thrown into space. And then we have to open it, we're done. And we have to use the representer theorem that says that w is a linear combination of the data. So we put this three, these two elements together gives me 15. And here we put a linear combination of the day that rather than W. And this is all that we need to do to find an alternative expression that still gets me. I mean MOUs. And this is why we don't. First, we consider this here. Just ignore it because are they, are there things that were wrong? They were corrected in our new version. So just you guys Crutzen, right? And all the comments about the errors in their book by rational set, you can scratch it and they do not exist anymore. So if you use the last version which isn't a web page, then you will see pretty much what I said here, what I wrote here. So we're done for the data into aerospace with this as far as the four or the expression of phi of x. And that will, let us see what are the expressions for the inverse of the covariance. And then maximum a posteriori value for w is simply this. He says facts we put five here if I transpose right. Now, here, since this is a matrix of infinite times n, and this is a matrix of an infinite. This is infinite times infinite. And this is also infinite times. Right? Now. The same happens with, with this here. And here. Now. We have to apply Little's theorem with this transformation. I modify this proof. I have a better one. Barium ion, which is different from the one in plasma sandwich. So I'm going to post it for you to have, I'm slightly different version of this proof. That doesnt includes or not explained Spanner this trick here. Here. Arbitrarily, I construct a transformation which is not five, but sigma t square root of sigma b times 5. I can do this because in this, this is the space transformation in the same space with our notation. And write re-scaling. So this is a vector in space. This is another vector in the same space because sigma d is positive definite and or semidefinite if you want. And then the square root of n is also a positive semi-definite matrix, right? So this is something that we can. Now. Then you're seeing the representer theorem. I can say enough. I can say that W is a linear combination of the data, which is font for my bad, times Alpha. So Alpha is a column vector or column vector of coefficients. If I have bad samples here, this has dimension n, right? This is infinite times infinite, infinite space and n times 1. So this quantity is a vector of infinite dimensions. Now, my estimator, Yes. My estimator appears now. There's one, because my samples, they are being transformed using this transformation. And w, we can just change it by this quantity here. And what happens with this light while the expression of this slide that the estimator can be written in previous reviews like alright. So my transformation, if we didn't write down phi of x transpose times Sigma, Sigma squared, sigma squared, and 59 spouse. And with this, we can analyze what happens. This is equation 1. I have met with us. All right? So with that, we'll analyze this. What we have here is a vector phi transpose. Here we have vectors phi. And then we can interpret this as a dot product between pre-test sample and all the training samples. All right, so, so phi of x transpose sigma v vector matrix, the matrix, this is, this is a vector, right? This is one times infinity, is infinity. Infinity times as bright and columns. So this is a vector or a row vector, k plus one column and one row and columns. Right? So this is equal to 5 of X star transpose Sigma p phi of x one. And here until up to five of x star sigma be fine of bags. Probably don't. And so this is nothing but a better which is constructed with n dot-product, constructed with this sigma B. So from here, we can define our dot product of x i. J is two vectors as phi of x i, sigma being transpose Phi of x j. This is now my dark brown without brother contains the matrix of the prior that I am using for sick, for w, right? And no matter what function I choose, k. This is interbred it like that. This matrix. We don't say it because it has infinite dimension. This product, we don't say, we don't know how it works when we have cornell dot products. And so the interpretation of this kind of product, they are like that. All right? And this matrix, we won't see it again ever. But we know that if we change the parameters of my brother or my kernel function here, this is equivalent to change this matrix. For example, I might have here I dot product, which is a square exponential with just one parameter. This parameter is changing sigma b. We don't know how we can, we can say, we can have some, in some intuitive explanations of how this changes with respect to the parameters. But actually we don't stamp and we don't care. Right? But remember that sigma b appears here. Biosphere and it's an regularization. So n linear approaches we deployed my money was an identity, right? And then the only element that acts as an organization entirely to this one. But this can be any arbitrary covariance matrix. So it intervenes in the regularization. It changes the way in which a polarizer problem. And this is why we say that the kernels, they play the role of regularization with we change the kernel, which is the realization of the problem. And this matrix with infinite times infinite elements, it might be controlled with a single parameter or a few non-infinite bank, right? So this is my, this is what happens with this thing. Anyways. Now, we will define w. Otherwise we'll find the value of Sigma Phi Alpha by. Now we define it using a different notation. Sense. We use this expression here. And we want to keep this, we want to get this transformation alone. Then we, now we assume that sigma d Phi Alpha is w prime. And then then we can apply this solution. W is equal to this expression that was the original one. And we put together this expression with this one hybrid here, sigma b via able to less, which is my studio. And then I isolate alpha. I's going down. So the first thing they do is to pre multiply by this matrix, obtain the solution. And then I simply level of this. And what we have is, so what we have is what would they do here is I pre-multiply, thanks. Phi, phi transpose. I develop all these expressions. But I don't want this expression. We have phi transpose, phi transpose, and here we see mine, which is a scalar with me. We can put it first, c phi transpose sigma T minus 1 and find Seema, B11, C11 by right? And then what we have finally is, so if I simplify phi transpose phi here, here, here, what I obtain is this simplified expression. And here we have is the kernel dot-product between all the training samples and themselves. So this is my pronoun majors. I isolate alpha, bypassing this to the other side. And is why were they rotate when k is equal to this dot product, the final before? We can't see your screen comes, I say again, sorry. Oh, they, they screen, sorry. So I define w prime as sigma d Phi Alpha because, because I have this expression, but I wanted to give phi alone. And then so I can say that my new set of parameters is not fine. Alpha by Sigma Define of this is my new value for W. I call it w prime. And I do that because my posterior, my expression for the posterior, it takes into account this expression only. So then I simply define w prime as this. And then this expression is equal to this one. I put them together, I operate. And finally, I obtain this expression here, where what I see here is the dot product between our samples, training samples and such. So this is my current image on my kernel matrix. It has to be inside the matrix of the prior over the parameters w. And I have here an identity on. And so since this is my kernel matrix, now I can change it by its rotation. And what they have is an expression which is exactly the same as for ridge regression. And when Kate, since they contain sigma b, i just get rid of it. I have to care about it anymore. So this is my, this is my set of offers. So hi, in this expression we're assuming that the product is constructed like that, but we construct the kernel of products as before, we'd get a positive definite function and that's it. The interpretation is now like that. They're not productive, convinced a positive definite matrix inside. And as I said, I brought in previous classes. If sigma is positive definite, then this has the properties of dot products. This has the properties of the products. And if we use any positive definite function, it is an umbrella. So basically here, this expression, it's telling me that I don't have to get anymore about sigma b power. Similarly. And that's it. So we have now an expression for alpha. And then my, my estimator can be constructed as w. Sorry. Why? Give you an F equal to w transpose x NSS by using this expression, u prime is equal to Phi of x, right? Biobanks. This is equal to the sum of alpha i K of x, xi comma x. That's my estimator. Right? We need to know how to include the bias here. We haven't talked about the virus, right? For linear estimators. It's easy, right? We put a 1 concatenated with a data x. And we point to an additional element here, which is going to be the barriers. Well here, basically we will do the same, but we have to translate the fact that here we have the bias and inside here we have a one. In terms of the kernel. How the kernel, how the kernel is modified. So in God's eyes, we will talk about desolate. That's it. So here you have to be able to make too many elevation of a solution for the parameters Alpha. And drew that the kernel dot product between data samples. Is this. I have an alternative. Yes. The other words was good at science. I'm Jay. Yes. As Jay. Yeah, that's right. Thank you, guys. Thank you. One of the one of this this J, i and I said I actually will save your work. Let's try this. Let's see, spaced in English. I don't even remember while alkyl part of that. So this is English. I will, I don't even remember why language is learned. And it was. So while these compounds is going to take forever. Let's take a look to this book, right? This is the book that I use for for these lessons. I need to put, I don't know why I didn't put the link for that. Probably I misplaced it. And what we have seen here is regression. And Rasmussen explains it very, very fast. Verifier does the standard model with the notation that they use. And then some intuitive explanation of things. And finally, the expression, the expression or the mean and covariance. It's a slightly different because I, I bought here w, w bar the mean and he puts the expression so he gets rid of w right? One. And then in a couple. Yeah, it just in asking a beach, he explains this class. And then the next one, the next or first month or next month. Anyways, it's a summary. It's a summary. And then Hebrew hebrew rights to different proofs for that. I'm not saying that this is easy to read, but it's a summary of what I explained. This. A mathematician, he explains things from recommendations and he takes some knowledge or the reader for granted, right? But once you understand that for my flights from my glasses, then you will understand. There's runaway because it's basically the same but summers. And then so as you can see, it goes from page to page 12, something like that. Page 7 to page 12. And then we can, if we go to, we can go to covariance functions in model selection. This Chapters 2, 4, and 5, they are the ones that they expect, right? So you can have this book for reference. I think it's a simplest book that you can find. The rest, they are a lot more cumbersome. By they will add some additional materials for utility or gear. That's Spanish. But I still have something else here, which is an English. Are they perfect? Yep. Okay, so here I put another added proof or slightly different proof for that. I'll post this next branch. So that's heavy. Lamar, an inventor, an equity. So let's talk about the next part. So we, we now, now what is the solution for the old parameters? Very easy, right again, once we have the derivation, which might be more cumbersome, but the solution below parameters. It's basically the same as in non-linear non-linear ridge regression. This is why I explain it. Took time to, to understand because now we have the same thing except that we have gamma is the variance of the error. And when we have an interpretation of a very sharp guy, right? As I, as I sat and says, Hey, doesn't change, you use the kernel of your choice. It's a mutation that changes. All right? And so with this definition of the kernel, we compute the prediction as 5 times 4 times w. And since in when we change the value of time by this expression sigma b, find alpha. Then what we have here is the set of dot products between the test sample and the training samples. And this is the data that we usually call k of x, is a vector on the corner dot products between all the samples and to try other training samples and the sample. And we have the definition for Alpha, which is this one. So now this is the mean of my prediction. Again, an expression which is nice. Of course you don't do that for all of us. We first compute this file x, right? So you print on your days and then we use it for all the channels. And as I said, K transpose X is the dot product between the test sample and all the training samples. So this is very straightforward to implement. So yes, the computation of the valence here, here we have the mean, we need the v, right? The problem of this is that it is a little more cumbersome to compute by. I just, I don't want you to learn by high this proof. Just I want you to understand, to be able to tell me what is the method for the proof. And that is very easy. So first, we know this, we know that the variance of my prediction, it was before X transpose a minus 1 x now is phi of x a minus 15. And a, it has this expression, right? It has this expression. So in this expression, we have two matrices. We can apply the matrix inversion lemma. So essentially, in order to transform this in a way that can be computed, we just need to solve for a minus one. We need to know what is the reverse of this. We go operations. Everything matches. So they matrix inversion lemma is kind of difficult to, to learn by heart, but not difficult to prove. And you have it in Wikipedia, you have it in many webpages and you'll have it in many books. Alright? So if we go in, we have our a matrix that needs to be inverted, that can be expressed like that. Then the solution is this one. Right? And then we identify, you see, a sigma p minus 1, u and v transpose and B transpose B is equal to five. And then this W is sigma M M 1. So many errors. W is equal to sigma minus two n times the I. So if we put all these things inside of this definition inside provider that here we have w transpose. We basically have, we will have this expression schema where here I have identity matrix, which is W, right? So this is u plus u V transpose, U transpose inside an identity matrix which is w. And this is, and this is z, right? So u, v transpose and W is this much. And so if we substitute here, then we have this expression. So a minus one, it has two elements. We'd have minus sign here. This is very arises minus sign here. Because here I have sigma and sigma b here and sigma. Now, remember that we have to premultiply by phi transpose of x and we have to post-multiply with five, right? They were pre and post-multiply. Here, we find out the problem. If we divide, here, we have another product. And so here we have another corner. And that's it. It solves the thing. So we use this expression with a solution or a minus one. This is phi transpose of x transpose phi of x. And then here we have the cornell dot product of the sample itself. And here we have a Corolla product of the sample with the training examples and here are the same but transpose. So by changing this by regular expression, this is what we want. This is something that we can easily compute because it's parallels the dot product between the SAT, test somebody in itself. The better of vanilla products between the sample and if any samples. Because Pavlov between the matrix of the parallel between test training and training identity. And again, the kernel dot products between the test and the training samples. And that's it. This is a scalar, right? This is a row vector times a matrix times a column vector. This matrix has, is NBA exam. So this should be n times had to sell this product is a scalar. Right? Now. What is the interpretation of this? That's, that's important to understand this as being conveyed. That's dams of whatever be the covariance, that doesn't really get somebody whatever our class here. That's the variance of my prediction. The variance or the latent function over the test sample variance of my prediction. And it has here quantity which is strictly positive, right? I'm here today, which is simply negative because this is some better. Here we have the same vector and here we have a positive definite matrix. So this is positive. This quantity is strictly negative. Which means that this one can, they must be higher than this quantity because the abeyance is non-negative. If you get a variance, which is negative a is everything start over because something's wrong. The math says, No way. Alright? And this is something that I saw many times, right? Negative variances can happen, right? This is strictly positive, strictly. And so this is higher. And that's why I'm here. If I don't have the information about the training samples, this thing disappears. Right? And the only thing that I have is this. Which means that they cornell dot product of a sample itself is a variance of my prediction. Before I know the information, I have the knowledge or the training data. So this and I will approve it. Now. This is the empire. The empire villians behind medical. And once I have the information about the training samples here, recall that here I only have X and Y on X. One, I have the knowledge about my training data here. And the variance increases. Right? So the prior is this, under posterior is this difference. So the posterior covariance, the posterior variance, it is decreases with respect to the CPAP, which is what we want. We can compute what is the variance of my prediction before and then after, and then this difference should be the highest possible. So we want this to be equal to this, right? I mean the variance shrinks and I have a distribution which is another. So I have no answer them. But this never happens. There's never habits unless the prediction error over the training samples is 0 and x is inside here. You can prove that if this quantity sigma squared is 0 and the train that somebody's inside, then this quantity is exactly equal to this one. And then what happens is that this is Lueger. Always. Any questions? Let me, let me see. Well, this is, this is going to be the result, right? The mean and variance of the Gaussian process space. I'll be fine like this easy mobility just named matrix inversion. And say we go, Am, I have this already, but they don't have to do it once. And then it's very easy to compute this product. And I have the amino my prediction, and I'm a measure of the uncertainty of the prediction. And again, the uncertainty of the prediction, it changes. With excess stock. If I change, the prediction will be, the variance will be different. So I have to come back. Also important. Recall that this is something that I have to compute for every sample. Obviously, while it means obvious, it seems obvious, but again, I've seen many people that computed once and that's it now. All right. The variance, it has to recompute that for every samples just as the mean. And so that's the final, the final result. Why? There's more, there's more, we have more effects to expect. So what is the current of the product? This expression, the granola of product, plays a role of fat covalence. And I'm going to prove it in a really straightforward way. Forgot online stuff. So let's say, let's compute the covariance of two samples. One covariance of y i and y j is equal to. K is equal to the covariance is equal to the expectation of y i times y j. We assume that the mean here is 0. The mean of y is u. So this is equal to the expectation of w transpose y times w transpose y one j, sorry, x phi of x. If I, let me write it again, phi of x, phi of x I times, I don't maybe parentheses w transpose Phi of x j. And this can be written as the expectation of phi of x transpose w, w transpose. These are two, This is our main vector. These are hyperthyroid. This is exactly equal to this. Because what I did here is to change phi times W, right? So this is a scalar anyways, and this is now x. I know in some phi of x I know they are not random variables, they are known values. So I can take them, I can think them out. Or the expectation is the expectation, sorry, 5 of X times the expectation of W, W transpose times phi. This is a transpose Phi of x. Is this expectation. The expectation of w, w transpose. I guess. What is the expectation of w? W transpose? As easily times 1, right? And W transpose is one that's infinite. So this is an injury that's even a matrix that's at him. What is the expectation of w, w transpose? What is the expectation of a vector times itself transpose. So again, something that we have mentioned. We have done this 100 times. And a whitewater. We have seen that in the screen that we have done, for example, with X, the expectation of X, X transpose R. By R. We approximate it by 1 over n, the sum of x i transpose. This is a correlation matrix, right? If the mean is 0, we assume the mean is 0 and reparation meetings becomes a covariance matrix. This is a covariance matrix. W, although we call equilibrium stages of w. How do we call it? It's not in this Islam. But do we have here sigma d y d Sigma, the covariance of the prior distribution over W. We assume that W is a Gaussian. It sound from a Gaussian with 0 mean sigma B as the Canadians are, right? So this is sigma, this is sigma b. So here we have this. What is this? Now? This is the kernel dot product is, you say, this is the product of x times y. But this is incomplete. This is an incomplete proof. All right. It's an incomplete growth. So you can scratch it. I'm just stating that we start with the covariance and we are now with a corner. And this is England. Great. All right, let's do it again. Now that you know what's going on here. So there's even scratch, I just make a table. But why should really fine line. I should be defined as W transpose phi of x i plus an error. All right, and I miss this. Let's imploded. And then let's do it again. The expectation of y i, y j is equal to the expectation of W transpose Phi of X plus E. Now I need the parenthesis times W transpose phi of x. That's my f j plus sigma j. And now I need to do that. All the products, I have four product here, this tail plus this two, plus twice, plus this times this, and this times this. But we assume that, we assume that the error has 0 mean. All right, so we just an egg and this error is independent of W transpose phi of i. So the expectation of this product will be 0. So we end up with the expectation of phi of x transpose w transpose w, w transpose Phi of I j plus the expectation of B I times j. So here what we have is a current product, Phi transpose of x i, w transpose Phi of x j plus this expectation. The mean and the samples are independent. So the expectation of the product is 0. Except in the case in which these two are not independent, which are the case in which I is equal to j. When they, when I is equal to j, the expectation of the product is exactly equal to the variance. So we can write this. How does this work? This is a chronic or that if I and j they are equal, this quantity is one. If i and j are not equal, then then this is 0. So finally, the other way around, if I and j are equal 0. Now, if i and j they are equal, then that is one. If the argument is 0, del next one, because that one is not 0. Right? And so we can generalize this and compute the expectation of vector i with vector I'm, I suppose that is that Kobe us are big training samples. And so this is equal to the current product plus sigma squared times the identity. So for depositions where I and j are equal, this quantity will become one. For the positions where I and j are not equal here we have zips. So the identity matrix is nothing but an expression, make magic expression of the Kronecker delta. We now go. And here is my, this is something that I find. And this is the veins of the training samples. Right? What happens with a test sample? Then go to the screen again. So this expression here is the coffee has the training samples, the training regressors. Why? What happens with the test sample? For the sample? And I'm going to finish here. Probably I was too, I was too low. But anyways, but for this sample, we have y star. Y star is equal to W transpose phi of x star plus the error. Right? When we don't have this. So we don't feel it only f star, which is W transpose phi of x star. And so the expectation of f star, f star x squared is equal to simply Phi of x transpose times Sigma be using the same reasoning as before. Phi of x star. So it's a dot product of x star with XR with textile. Which proves that in my expression, in my expression here, that is covariance of my prediction. I have a prior page, which is the variance of the prediction. When I don't have any knowledge about the data. Hence, I use a prior distribution for sigma B. If I want to use a posterior distribution or a posterior distribution for sigma B, instead of having, I'm sorry, at-will status division for w. Instead of having sigma B here, I will have a minus one and we obtain that result. Right? So this is the valence or the answer that the measure for my prediction before I know of auditing data. This is the distribution when I have the knowledge. And the posterior is lower than the prior, but always positive or 0. I wanted to start here. We're trying to do that today. So I'll keep talking about that next class. And then after that, we will see some examples. Good.