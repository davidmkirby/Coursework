 Hi. And said, oh boy, this thing. Back. Hi. Alright. Because a projector is not starting. I'll just say yes. The spirit, I guess this. So login ID that projector, I did it poems like that. And that said, I screen share. So this is not, so I just need to reject this that if instead of them, that's incredible. So a pass There's to connect or to describe this HDMI are for this is that really for us to view and try to restart the computer, forget, maybe, maybe by two Pi. It should not. See. There's nothing you can do from here on two screens. I guess. Yeah. Sure. Yeah. It's a bit worried. Oh, no. Don't have anything connected or pure refeed allow you to do that after because yes. Yes. But it's up here in my office or if I give all right. That's for usb. Yes. That's HDMI or it's alright. Let's, let's do it without the tiles. You can just take this lightly. It is class today and helping us. All right, so venues. And see what everyone says this, then I'm going to do that in the first screen. So this thing is recorded properly. Alright, so let's start by reminding the process where we construct the Gaussian process from scratch by, since we were talking about Gaussian process in Cuba space as I just did, I'm going to change a little bit of notation so we do it from scratch, bite, and I know Cuba space, right? So in my model, we had hi, this is equal to w transpose, which is the set of parameters times 5 of x was x n, right? And then we have an error. And now my data is already in a Hilbert space. It's a future space or whatever you want to cough. So now this is a vector that is in this universe, basically a nice principle, infinite dimensions, right? And my model is such that he had is IID Gaussian. Why does this mean? It means that the arrow is drawn from from a Gaussian distribution. And every, each one of these samples are independent and identically distributed. So all the samples, they are dropped from the same distribution with, with our power or variance sigma square. Writes, How was this? That is drawn from a Gaussian distribution with 0 mean and variance sigma squared. With this, only with this assuming that we have an arbitrary set of values for the module. I'm assuming that we know X. And then so we know phi of x, then y is also Gaussian, either identically. The distribution of the random variable. So y n is then drawn from a Gaussian whose mean is W transpose phi of x. And the variance is the same, C minus one. This is my model and this is what allows us to start Gaussian process. So then what is the likelihood? The sequence? Why a sequence or condense and samples like that while y, which is a sequence from y one to y medical n transpose, it's a column vector. It has a distribution as a function of my set of parameters w and my set of data, fine, we're fine, is equal to a major Is that wrong dates from phi of x 1 to 5 of x heavy now, and this is the product of these Gaussians. So this is the product from n equals one to capital N of a Gaussian and of Y. Given W t phi of x and c minus a product of all these functions and these products of Gaussians. It is a Gaussian, but it is a multivariate Gaussian. And he can be express my gf normal or Gaussian of y, whose mean is equal to phi transpose w. This is our matrix is done. And the variance covariance matrix, sigma n square times the identity. We can do that because they samples and they are independent and they are identically distributed. So we have this distribution for life. And this is what we call a covalence Pissarro a likelihood. The likelihood of Y given the input samples phi of x, and given an arbitrary value for w. Yes, the likelihood of the screen for that like before us. Okay, So probably we can use this, right? And then I will keep going. So this part of the screen is forbidden. That's what we call lagging. But now we know that w is some, or we model w as a random variable. And that random variable, we model it as a Gaussian distribution with 0 mean and a covariance sigma p. Also arbitrary. And this is what we call the prior. The prior distribution for w. And the goal here is to construct a posterior distribution for w given, given their knowledge of y. And fun. This is about right? At the why. So we know that this posterior is proportional to the prior times the likelihood is proportional, and it seems a prior is Gaussian. We modeled as a Gaussian and the likelihood is another Gaussian. Posterior must be a Belgian. So we just need to do to compute the product and then normalize it. So it has an integral equal to one. And with this, we have done the posterior w given. Why I find is a normal w with a mean and a covariance. The covariance we call a minus a minus one. Where w, while the covariance, it has an inverse which is equal to sigma n minus two phi, phi transpose plus sigma a inverse of b. And then w. The mean, is equal to 55 transpose plus sigma square sigma V in bracket. Our minus one here, 5 y. Right? So we are right to this by simply multiplying the prior and the likelihood and assuming that the product is a Gaussian, so it must have been it must have grades. We identify terms. It's kind of a worry. It's algebra. But we'll write to this two expressions is not something visible to, to find. So with this, we have the distribution of w given the knowledge of the input and the output. Why? They mean is the point that maximizes this normal. So this point here is the one that has the maximum probability density. And hence, we call it maximum a posteriori. And it has an expression which is similar to the ridge regression expression that we found a previous classes. Now, if, if we use this posterior to compute the compute power prediction, in order to compute our prediction, we just need to multiplied by the function baby and a set of parameters, W mean. I suppose phi of x star, where x star is a new cell. This is the maximum a posteriori prediction and the expression will be, well, assembly, it's unnecessary. We put this expression here and we've got maximum a posteriori prediction. But instead of finding the maximum a posteriori prediction, we can find the posterior distribution of migration. That gives a, not just a prediction that much anymore because they are very keen on, but also gives me a confidence interval on the prediction. Right? Assuming that the selection is an option, he did. In order to compute the probability of F. Given the input samples phi of x, I'm given the set of training data. We just need to compute the probability or the probability of F given w, w being a random variable arbitrary and Phi of X times the probability of W. By this probability, we have, for this project, we have a posterior, which is the probability of w given fine. I'm given y, which is this probability is, the prompt, is a Gaussian that as this covariance amaze me. So since this is a Gaussian and this is a Gaussian, the interval must be also a Gaussian. In order to compute this integral, we just need to compute the product and then the mean and the variance of it. We don't have to solve an integral. This is an expression equal to the expression that we use for for Jane data. And this is this guy here. So the solution. Here's a normal of f whose mean is equal to W transpose phi of x. This is the maximum a posteriori. And then we have a valence which is phi transpose of x, a minus one times phi transpose of x. Right? So this is the solution on the Gaussian process. We need, we compute the mean. We can compute the games. But the problem here is that we are dealing with our mountain with a maximum, a posteriori value or mean value of w that as easily they measure. And these magic has an infinite dimensional here. If you take all of this, you'll see it easily. This is matrix that ASP, infinite rows and n columns. And there's the same transpose, so that gives me infinite, that's in fact horse. And saying how does here these mating is infinite times infinite. And this matrix is so this, let me see here. Yeah. So this matrix is, this vector is simply have one. So what I have is that one. So there's blips in the Hilbert space. Now, what do we need to do in order to be able, to able to use the result in the hero space. Well, we need to use, they are represented as theorem that says that w can we express the right? Is everything. But it's gone. Let's keep the main results. The mean or the maximum. A posteriori prediction. This is equal to find f star. I forgot to add a bar here for the maximum a posteriori before, right? When I, I bought it, I forgot driver. That's the maximum posterior prediction. F started without the bar, is a random variable with an avid. The maximum a posteriori is this is w transpose mean phi of x, where w and also n. Then the variance is equal to 5 of x times a minus one phi1 of x star. Where this quantity here is equal to five phi transpose plus sigma n square sigma p, y and y is 1, 5, 5 y, and a is equal to sigma star. Sigma square is 55 transpose plus sigma times what? So we need an expression for I have an expression for sigma that we can feel with. So for, for the mean, what do we do? First? First, we assume that W is a linear combination of the data, right? But also I will define, I will define my transformation into a Hilbert space. So instead of phi of x, I am going to use the transpiration sigma p phi of x. I'm assuming that transformation is this, still this question. This is what we have. But assuming this I'm assuming that w transpose w, sorry, is a linear combination of the data. I can say that w can be express bus. That's where here we have all the data or the input data for training. And alpha is a vector of coefficients. So this brother give me a linear combination of the data. And this is a square matrix which is positive definite. So the transformation is body, WE still lives and spaced. All right, this is basically a translation rotation. And so using this and assuming that w has this expression at the same time, this expression that we arrived to alpha equal to 5 sigma d phi transpose Sigma P five plus sigma square times identity minus 1 y. Sorry, my followers. Now. So this is the value of alpha. And then using this, using this to results, then I have F is equal to, well, let me say that. Here we have this expression. And this expression is equal to all input data transform, like that. Transform times the square root of sigma p multiplied by themselves. So here we have all the dot products between data, data. Where did the appellant is defined by Phi of x. I am using a different finite x I. J is equal to five. Thanks I transpose Sigma p Phi of x j. This is still a dot product because this is a positive definite matrix, right? This is simply the dot product between these two vectors, but between these two letters, followed my gut. And say if Sigma is positive definite, sigma d minus the square root of Sigma is positive definite. So this is, this, if phi of x is a vector inside the hero space, this product is also a vector and say the same space, and this is a valid product. Finally, this is what we call k of x i. Thanks change. There is in this space I dot product can be expressed from the input data through a positive definite function. So this function is any function that you want as long as it is positive definite, for example, as financial or polynomial, neglect or whatever you want. And then CMA be just disappears of our eyes. It goes away is implicit in any drug that we have. The sigma b is arbitrary and it will depend on the choice of our doctrine. But we don't see it, right. And then alpha, alpha can be expressed as a product. Pay for the matrix are by-products or the kernel matrix K plus sigma n square I minus 1, 1 matrix matrix. And now you're saying this expression, this expression and this expression. Finally, we arrive to the expression f of x. F star is equal to y transpose K plus sigma n squared by minus one k of x. Where k of x star is the vector of dot products between my test sample and other training. Right? Let me see. My base here, k of x star is equal to this vector k of x tan, x star, x one to k of x transpose. This is a column vector. Column vector. So this is why I put the dashboard. Then. This is here. This is the maximum a posteriori prediction in a way that can be computed. Because now I have infinite matrix. What we have is a matrix of dot products between data, which is n times n. Right? And finally, we need to be obeyed. So we need to see month's time by sigma squared is equal to phi transpose right here, right here. And what we do is simply to apply the matrix inversion lemma here. We compute the inverse of this matrix using the matrix inversion lemma is a dislikes. And the final result is that phi of x phi star, Sigma star squared is equal to the dot product between x star itself minus k of x star as defined here. K plus sigma n squared I. I have one X. And with this, we have a complete definition, the goblin result for the GARCH process. This is the maximum a posteriori prediction here. And this is the variance of my prediction. So phi1 of x squared is all the time defined as the expectation of X squared minus the mean. So this is the variance of the process f, right? In order to compute the variance of Y star squared, the variance of the process, including the error. This is equal to sigma, sigma star squared plus sigma squared. Okay? Now, this is the whole result. Predation or this comes now be the abbreviation of a maximum. A posteriori is easy to understand. This is the maximum point of my distribution. Here we have the variance, it has two terms. This is the variance of the posterior distribution. Miss being in the red is the mediums of the prior distribution minus one. What is a radiance of the prior distribution? Distribution for f star? Computed? Bfss time is equal to W transpose phi of x, where w now is an arbitrary value of four. We have any knowledge needed to compute a distribution for this course there is produced then, assuming that this has 0 mean, write this as 0 mean. So basically this has 0 mean. Then the variance, the prior, the prior variance on the right. Is equal to the expectation of f star squared, which is equal to the expectation of W. It's pretty like that. Five star transpose W, W transpose phi of x. This quantity is known. This is another random variable is a constant. So we take it out of the expectations. And what we have is fine transpose of X times the expectation of W. W transpose this two vectors times phi of stars. And this here is nothing but the covariance of the prior distribution of W. Right? So we have one, maybe two things. If we don't have a posterior for W, we have to use the brayer. And that is prayer. It has a variance which is sigma p. Right? If we have a stimulus which I'm part of w, that this quantity is a minus one. So there's two solutions for that. The first solution would be Sigma transpose of x, phi of x, I think sigma times phi of x transpose. And if we have devastated evaluation for w, we don't need to use sigma. Good, but we have a minus one where A's here. All right? It has a minus one there. We have this statistic. But if we don't, we use the prior. And this is defined as the current of the product because when x is time and it's up to you. So here what we have is the primal is division. And this quantity is negative. This is negative because this matrix is positive definite. This vector and this vector are the same as this product should be positive. We have a minus sign here. So this quantity always decreases the valence somebody posterior with respect to the right answer. And since this quantity is non-negative, then this quantity here should be less or equal to this. But this quantity might not be. You cannot be higher than this one that will be involved. Because any urban neighborhoods variance, we will have debates, which is a privilege, right? So when these two are equal, we need to have two situations here. First, see my n must be 0, or this sample here must be included inside the training samples. If these habits, it's difficult to prove that this quantity will become just this.name. And in general, in general, the dot-product between samples is radians. So the kernel matrix or the kernel, the kernel function, K of x i x j is coherence, right? Indeed, the expectation of y i times y j during the training, right? So these are two training samples. Pipes. This is equal to the expectation of W for W transpose y i plus the error times w transpose Y j times the error for a sample j. Now the y, sorry, x, x i, y i is w transpose x i plus the error Sanford J. This leads to the expression. And this is not x but phi of x I. This is Phi of x j. And this leads to k of x i x j plus sigma n squared delta of I minus j. How does this work? Well, if we do all the products, one of the products, as we have semaphore, becomes the kernel and the the other, among the other products. Well, this quantity is independent of anything else. So the expectation is 0. Independent extirpation of the prompts are 0, except in the case in which I and j are equal. This somebody, somebody saying. And the covariance is equal to the veins. And here, this is why we put a delta here. If i and j they are equal, then this quantity is one. Otherwise it is 0. And if we compute using this expression, the expectation of Y one transpose, this is our column vectors. So this is a row vector. This becomes the kernel dot product to create a matrix sigma n squared times v, which is when we have. So this kernel dot product plus this identity here. It is also a kernel. This is a kernel because it's positive definite. So this itself, it can be considered a kernel. So this is the covariance of the set of samples. Why? Now we can do the same with the expectation between y and f star. So the expectation of this is equal to this equal to w transpose x, i, sorry, 5. Y is equal to all the samples, fine transpose times W. And here we have the expectation that that's fine transpose w plus the error vector of errors times W transpose phi of x star. And so we do all this when we have is equal to the product between this element and this element, which is the expectations of fine W transpose W, W transpose phi of x plus this product. By this and this, they are independent. And this has 0 mean. So the expectation of the product is 0. So this is what we have. And by definition of the dot product, what we have is the expectation of W, W transpose phi of x. And this is sigma p. So this is not valid between the test sample and all the training samples. So this is what we call came. Okay? Let me see. This is a column or a row here. This is transpose. So n rows, one column. So a way to rod it has these four bits. The way I wrote it is transpose gets how relevant. It doesn't matter, right? So we change the products where they get. These are colored by the point here is that the expectation between, Why are they? Covariance between Y and F is nothing but the vector of the products between training. And with this, we can write something else. We can write the covariance of the whole process, training and test. The covariance. Of the process y and F is equal to, well, we have to do your cross-products. And when we have this four numerology between training samples and examples, we have this, Hey, plus sigma square by 4, y and f, or home-based quantity here, which is k. Let's see. We have to put this as a column k of x. Here we have the product, we're going to have y k transpose. And here we have k as the variance between F star itself. This right, shall we have a matrix here. Then here we have a column, here we have a row, and here we have a scale. And this is the covariance of the whole process. And then I assuming that the mean or the expectation of Y, the joint expectation of Y F is 0. And assuming also that this RNA mean and the covariance of the Gaussian. Then we have the probability distribution of Y, the joint distribution, which is normal with 0 mean. And this covariance this time. Okay? So this is a joint distribution. Using the joint distribution, using also the distribution of using this division of y only that we already have. And using the base rule. Then we can compute the probability of F given fine line. Hi, This is, I didn't write it because I forgot about it. But this is the joint distribution of y given the training samples and it has some rights. I forgot to highlight here, but five, Y and X they are site. So this Kobe answer is B. They are the covariance of BB given phi y next time. So here we have the joint distribution, then we have the marginal distribution, the probability of Y two. By applying Bayes rule. We can compute this. And we're right to exactly the same result or result in which we compute. We have the maximum a posteriori, or the mean and the variance. So by proceeding this way, we get exactly the same results as we got using the procedural edges that just rubbing their Wi-Fi. And this is another way to prove what else? Something else and I'm not I can't remember. Okay. So questions about that? This is this is recorded. I hope you can take note of this. It's not still messy. So it's recorded. So that's something else. Well, two more things. First, we never talked about how to find Sigma squared. Yeah. And also the cornell dot product here. It has parameters. So we have parameters inside. We have this which are unknown, and maybe some others that we will see now. We might have a lot of parameters here. And if we could not, hard to do cross-validation of all these parameters, then the Gaussian process will become something overly complex computation of. And we don't have too many data, these things might not work. So instead of cross-validation, what we have to do is to optimize these parameters using the maximum likelihood criteria, right? And the maximum likelihood criterion simply uses the livelihoods of the training data. And we maximize this likelihood with respect to the parameters. Let's see the complete set of parameters first. Why do I explain all this thing? It's not a mathematical curiosity. Alright? It is important for you to understand how rational processes work. Now, you can put together a calcium rushes in Judah and understand how where did you cannot interpret how parameters work, then you won't be able to optimize your partial process and it will not work. And it will not work. And you want to understand why. So once we have a clear idea of how to, on how to construct my Gaussian process from the beginning to the last whiteboard that I wrote. After we do that, we need to first see what parameters are inside and why the mean. And we have to interpret than from their point of view of the covariance. This is why I insist in the interpretation of the gut or the kernel dot product as a covariance. This is very important. And I seen many students, many people that do not give importance to this. And then what happens is that the Gaussian process doesn't really work right? Now. Let's start from the fact that in the Gaussian process, that is a bias hilum in that Gaussian process sticking out. So my Gaussian process can be written as y is equal to w transpose x plus b plus an error. So now, what I'm doing is to take this out. Of course I'm going to put it back right now. How do I do that? Let's call this W, is this five-year-old NOT gate. And so this can be written as w b transpose w tilde transpose phi of x n 1. That's right. So in order to give this very generality, instead of this, we arrive W, W 0 for example. And here, and here we put in, let's call this sigma one, sigma two. And instead of putting a one here, I add an arbitrary parameter. And so B is this product. Now, this is w transpose. Okay? And now with this expression in mind, how is my dot product? The dot product of this vector and a vector like that. So the top row k of x by x, k is equal to negative five. Let's see five. Let me doing complete wave transpose Sigma P of F I, j. This is the way I defined my product, but now it has two components. So this is equal to Phi of x i sigma transpose sigma p Phi of x j. Sigma. And now we assume that sigma b inside as the original sigma v. But this last row and last column, they are zeros except the last one, right? So sigma v, without loss of generality, is a matrix whose last row is all zeros except the last one. And the last column is all zeros except the last one which is one. And so when I do this, what they have is this times, this times this, which is k of x i plus sigma squared. For my model to have a bias, my kernel must have a constant. All right? So what is the covariance now? The covariance, the expectations between y n and y j is now the cornell dot product. Whenever we have, whatever we choose, sigma, sigma squared plus sigma squared, sorry, delta delta. By checking I'm a DJ. And in particular my kernel matrix. My kernel matrix, which is the expectations between y and y transpose product, is equal to any kernel matrix constructed with any function of k plus sigma and Sigma squared. And some agents at once plus c squared times the identity. Right? So this is itself a Cornell. Why? Because a kernel sum of kernels, a sum of positive definite functions. It is positive definite function, so it is a kernel. This is positive definite. This is positive-definite two, and this is positive semi-definite. But it has a single eigenvalue, which is sigma squared. The rest are 0. So here we have a single eigenvalue equal to one residue. So this is positive definite, positive definite, positive semi-definite. So this is a kernel matrix, right? So it works as a corner. This is wife inside here a space holes. Now, why do I put sigma here instead of a 11? Let's delay a little bit. Right? Now. Let's label that. What is the prior variance for my prediction? I know that a try or is it too fast or not? I know that the higher is equal to the kernel dot product, right? We just proved. Let's compute the prior variance for that. So the prior, and we know it's the expectation of f star, star square. This is equal to the kernel k of x star, x star, which is any function. And now we, out of this, we take a constant, Sigma squared, which is positive. So this is part of my veins. I cannot put a one. Because otherwise I will be assuming that the higher variance is higher than one. Right? Or it might be the case that they have a process with a high hour. And I am assuming here that the variance is almost well this course. But let's take a look at this with an example. What if k x, x prime is equal to the exponential of minus X minus x prime squared over 2 Sigma. Now we have to put a one here and a 2 here. Let's assume that this is true. Then the expectation of f star is equal to 1 plus sigma squared. If I choose this kernel, I am assuming that the prior conveyance is one plus sigma one. So if edges are 1 here, if we choose this, this, this corner, and I choose sigma equal to one, I am assuming that the variance is two. All right? And this is highly restricted. What if my process is something that can be measured in millivolts? I need to normalize everything or what, or megawatts, for example, or megawatts. The radians is not to. The variance is something else. In general, I have to choose k of x, x prime us. For example, sigma. Let's put three here. Three Sigma 2 squared, the expectation of minus x minus x prime squared over 2 Sigma squared 3 plus Sigma 1 squared. And now we have three parameters. It will have these three parameters. This doesn't count right here. But if we have this till then, the prior is equal to Sigma square plus Sigma 2 squared. Now we have a variance that we can fully control. If you simply say, well, yeah, and you're saying the x, this exponential satisfy shop. And I'm using here a one. Then my prior covariance is always two. And it doesn't make sense. If I choose this amplitude here, this constant parameter here, then I have full control over the veins. Alright? So it is very important when you program your reduction process to unfold this tip. Now if you look at my slides, I said something else. So I had to, Geoffrey Moore, part of this has to be a couple of parameters. So Gaussian process, for example, this Gaussian process that has this exponential. At minimum. At least we have 1234 parameters. Minimum. This parameter inside implicitly control Sigma P right here. Now what it does, it has more or less expressive capacity to my regression, right? And these two, together with this, they control prior covariance and the posterior of course. All right? And now you know everything except of course how to optimize this. We have to, we have to choose sigma one, sigma two, sigma three, sigma L. So the likelihood of the training data is maximized. And this is the best we can do. And we will do it in a way that avoids overfitting. And that's the last thing that you need to be able to interpret, be able to reproduce, be given to program. If you want to put together a Gaussian process. Alright? So when you get there are some process and you see, oh wow, My, it for a test sample, for a test set. My maximum a posteriori page. Really nice. It follows whatever it's supposed to do hearing tests. But when you compute the covariance or the variance, it does work for something and it's because of this. Alright? With, without any particular choice of sigma one, sigma two, you might have a nice, a nice posterior. You might have a good prediction when the covariance is not going to work. And then you have to go back and see I gave you tell us good barriers or similar watch it. But you don't do it using cross-validation. The best thing to do, the best way to do it is to use the for their livelihood, training me. And this is the last thing that we're going to see this. But when I said during the last class, right? Questions. All right, so I'm going to insist the NADH, we will see examples until you really get the whole idea of 10. Thank you.