 So let's go show you. That's it. This is pretty much a summary of the solution of the GARCH process. And in the case where we have our new sample not included in finding a test sample x star. Then we compute prediction for the sample that consists of bi mean for this shape with this expression and a variance of the bridge that has this expression. So they did aliens, but dipolar, the mean is very easy to understand. It's similar or equal to their solution. For a solution out there, which regression? Except that sigma n is to be of the mindset of demise, not by cross-validation by we will use a procedure that will say No. Sir. Yes. Could you share the screen? All right. I'll see you in a verbal online. You can see just the camera. Sorry. Yes. So we are done here. Can you see now? Yes. Yeah. Thank you. Thank you. So yeah, I know that I was forgetting something in the process. So the mean has a same chambers this bridge regression, except that sigma square has to be optimized using a procedure which is different from cross-validation. And then we have the variance of the prediction, which is the novelty or one of the two main novel this or the Gaussian process with respect to other methods. So this variant has this component which is a prior variance, and this quantity which is always negative. So the posterior, the variance of the posterior or the posterior variance if you want, it has a value which is always less than the bridegroom. And so regarding to this, in a assume that here at this point you understand, all of you understand the notation. Here is the kernel, the vector of Cornell that broken between the test data and the training data. Here we have the kernel, the kernel matrix, which is the mix of products with when most of their training sad, this is a and this is our vector of their regressors or outputs beside outwards or the training set. So run into this. But we interpreted the kernel dot product as a covariance. And here is a summary of what I explained in yesterday. So the kernel matrix K is the covariance matrix of the Gaussian processes dimension of x n, where x is the set of training samples. So we will compute the expectation of f of x of n times f of x of m. What F is that as dimensional function w dot phi of x. Then with all the computation so that we end up with this expression here, which is the training sample n transpose times the parameter covariance matrix, sigma times phi of x, which is defined as the cornell dot dot. So that proves that the kernel matrix is the covariance matrix of the Gaussian process estimation. For the. That if we add it, we add noise to the, to the estimation, then we have K plus sigma times the identity. So, and then this here, yes, so if we add the estimation error, what we have is y for the desired output. And so it will repeat the process. We compute the covariance between y of n and y of m. Here. What we obtain is this expression here, which is that kernel dot product plus sigma squared, the error power our obedience than zeta of minus band. So this is one only if m minus n. So the covariance matrix of the whole process, of the process, why are the training sample, the training set escape plus sigma square by this, which appears in the solution here, right? So and also I, the tail and the whiteboard. And the way I detail. How do we, how do we write this expression here? So if we compute in Wakefield's the process of we put together the process y, which is the outputs corresponding to the desired outputs corresponding to a training set. And there the prediction for the test set, this prediction is a random variable of which we only know the mean and the variance. By. So then we can compute the mean of this process. We assume the mean is 0, and we also compute the covariance of this process. The covariance of this process is equal to the covariance of the process Y, which is this big covariance between Y and F, which is here. The covariance between F and y, which is here. And finally the variance of the process, which is again, this is the joint process y and f. Using the Bayes rule, we can compute the probability, the pdf of f condition on x and y and of course x, y and x star. And it gives, as a result the Gaussian with the mean and the variance of, of slide seven here. All right, So using that method we are right to the same result. So that's it. And this is an example. And you know, that next week this is what I'm going to do. We're going to see the software. I'm going to use my math lab because it's much faster, easier to present. But we can do this exactly the same thing in Python. This is an example. I don't know if you guys see to change the colors here. You see some yellow or orange process around here. And this is the training data. So in this example, the input, it has one they mention it goes between minus 1.5 to 1.5, something like that. And they are good. Y. And the output y is some process that we don't know of waves. We have several samples. And the samples, they have added some Gaussian noise. Using a Gaussian process regression method, we obtain this. This is the mission or the function of the latent function f, the red line. That is the mean of the function f, or the maximum a posteriori, which is exactly this expression here. Oops. Here. For every value of x, which is a scalar in this case. And these bands is one standard deviation. I believe that this is I think that this is two standard deviations. It doesn't matter. I believe it's two standard deviations. So with liver some Here is the meaning in red. And the mean plus minus twice the standard deviation that we obtained and every form. So what we see here is that where we have training data, we have information to estimate them, to estimate the latent function f. But out of this area, we get minus 1.50.5. Here we only have one sample. So there is a lot of uncertainty between this point here, 0.5 and this point here, which is there'll be 1.5. There is a lot of uncertainty because actually with an Africa and this is why the Gaussian process is telling me. And here we have the same. So whatever we predict in this area, it will have a high answer that we should not trusted balancer. So I guess here, the standard deviation or the variance. The variance is very high. So two standard deviations emitters. Two standard deviations means the the area where 95% or 98, I remember, percent above the average power. Right? So this graph is telling me your, your prediction error will be almost always in this gray area. So this area here, depending on the application business, just for example, this area, it looks like the standard deviation or the uncertainty is small. But here, the uncertainty is unreasonable. High wife, because there's no information. There's no information. There's nothing recap of period. Unless we have assigned information. Unless we have other knowledge of a linear function, we cannot do anything. So with machine learning, this is the passive dependent, right? If we had more information or the physical process that produces f, that we would maybe be able to widely. But this is beyond the scope of my shot. Right? Or at least so. Well, I have more examples, but before this, let's go and see how do we choose the right parameters. So this works properly. Here. What we have is a non-linear Gaussian process. Alright? We use kernel function, right? I believe we have other functions are other examples later. But in this example, we have to choose sigma n square. We have to choose the amplitude of the kernel or the parameter that multiplies to the function. And we have to choose the parameter of a bias, right? And also an additional parameter of the kernel function, which is day, the white, right? So at least four parameters. We could use. A simple cross validation by this would be very computationally heavy. Plus it, it's not going to work properly because within half a lot of data for training, right? Or for cross-validation here we only have a few they. So let's see how can we do it better? And this is what we call model selection for regression. So let's say the Gaussian process uses Cornell's for convenience functions that have free parameters or what we call hyperparameters. And as I said several times, a proper selection of this parameters will optimize the performance of the estimator. Plus, if we do not choose probably our parameters, the Gaussian process is not going to work. So we have to do this. We have to do this as a client is something that is a binder of the Gaussian process and it's unavoidable, right? We have to optimize these parameters here by Jews and kernel has sigma sub f. This is notation and you will find in the end what the math is, the amplitude. Here we have a covariance matrix instead of a Sigma. Right? Here. Here we have these parameters if we want. Right? Usually this matrix is an identity. Dimes. Parameter, single parameter. So now I'm usually here we have 1.5 of x minus z squared divided by two sigma. By. What we have here is a matrix. In MATLAB and Python. In Python. For example, you have a particular case in which the kernel, it has one single parameter, a site. In another case in which this matrix is a diagonal of parameters. So you have as a one parameter for y parameter per dimension. Hi there, This has a different purpose, but then we have Sigma squared here. I'm showing here by we need to add another parameter, which is they buy. So here and here we have all those. Whether we do. Let's see. Let's talk about the noise parameter. The parameter tends to attenuate the importance above that, those dimensions that condemned just notes. Right? And that is a intuitive or physical, physical interpretation of this parameter. This parameter is very important because it constitutes a, it's a, it's an important level polarization here. And this organization is done by attenuating those dimensions that contain only noise, but they do not retain information. Let's see how it works. It is a single parameter. So F, F bar. All is the notation that we use here. To this innovation. We use here to express the predictions for all the training data. Right? So for just one, just one sample, one, sample two left. It was just one sample. Prediction is f of x. And this is our prediction. Bob at a training sample, not necessarily tests training. So this is equal to the weight. Press here that will be k of x of n times n plus c n squared I minus 1, 1. Right? Now, if I compute the vector f of x one to f of x capital M. So their predictions over all the training data. Let me see. This is n times one. This is also n times 1. So this is a term and you had to propose here one times n. Okay? Let me see. So this is, this is one times, and that works. This is equal to this K transpose of X one to k transpose of x n times k plus C minus I minus 1 y. Right? So if here I, but instead of the dot product of sample and the rest of our times, all the training samples. I put all the more as all the vectors k and corresponding to n equals one to n. Can I do this, brother? I have the predictions for the training samples. But this here is also what we call kf. Kf, and this gave F is the matrix of kernels dot products between training samples. And what we have here is nothing but the dot product between training example. So this is also k f Sigma square. I am using here the notation of the book where F is the kernel, the kernel matrix that you and you compute using any cornell dot product, any positive definite function. And to that your hard drive. This point the picture. Alright, so and this is what we call albedo. Sorry, well, bot. So also a later in the book. Instead of k, they use K1, which is KF for the current image, the training samples that you compute using a dot product of any Cornell positive definite function plus this quantity here is, I guess. But the point is, this kernel matrix can be decomposed into eigenvalues, eigenvectors. So this is the set of eigenvalues. And then this is satisfying and back. And this can be also written like that. The sum of one eigenvalue, eigenvalue, and eigenvector QN times Q transpose. And so in this case, if this is true, then this matrix can be this matrix here. Remember, we didn't like us. Kf plus C minus I has eigenvalue lambda n plus Sigma square. Similarly. So the previous expression can be written like that. Sedna liberation expression. Sorry, this expression like that can be written like that because KF plus sigma squared I has eigen values lambda plus c square. And this inverse as has eigenvalues one over lambda and sigma squared. So this matrix and build with them simply like that. Also. Given that there are factors of n components. Because matrix kf, while this may, this is Pepperdine and Skype. So these vectors, they live in a space of n dimensions. So does Y. Y has n components, is a vector of n dimensions. And hence, it can be expressed as a linear combination of the eigenvalues, eigenvectors and the elements of this combinations, I call them gamma iron. Gamma being some arbitrary. So this expression exists. This expression exists still even if we don't know mine. But then we put everything together. We put KF times this quantity times y. And this is equal to S, right? So we take this expression, we change k f by this k f plus sigma square minus one. Where they, and why with this two expressions, and we are right after simplifying, f has this expression f. So the predictions, they are the predictions on the training data. They are a linear combination of the eigenvectors. So every eigenvector gears contributes in a different way to the estimate. How do we do that? How did they do that? Well, first, f is I mentioned why without noise. And f. F is the maximum likelihood estimation of that or maximum a posteriori estimation habits. We know that y is a random variable and y is the same a always a minus there. So every, so it has big contribution. Our gamma and belongs to Y on the island packers in the eigenvector and pass in the eigenvector has an associated eigenvalue which is high. Then, then this quantity is negligible. Lambda n is high. This quantity is negligible. And then in this case, the law 99 here, they almost cancel. In this case, the contribution of the eigenvector to F is equal to minus q. Same as here. But when they eigenvector is, it has an eigenvalue which is small. Then it is negligible here. Right? So we assume that gamma and see why it is much higher than this one. And so what happens is that the contribution is equal to land. I got mine over Sigma squared. Well, thanks, buh-bye. All right, So if sigma is very high and Comparative Law 9th, then the contribution of the corresponding eigenvalue eigenvector, really small. That's it. So this noise here regularizes f with respect to, all right? So we are constructing in the estimation of the training data. We have uncertainty in f bar. We've been fighting aversion of y, which adenoids, those components that has notes that have-nots that said. And this is why it is important to properly choosing minds. But otherwise this is not going away because we're not going to paint has already dried properly. The eigenvectors, depending on and off. Well, this is this is the explanation a party I mentioned, I say all that. Then the contribution is government. If the dimension is not important, so we only have noise in that dimension, that is what we have. Since sigma squared is much higher than that, this quantity will be adequate it, right? And so these are more examples. Here. This is copied from, from the, this is coming from the software that is provided to better with work. So an example, a knife and I give here is an example of a rational where the input is a scalar from minus five to five and day out with this one. So the red dots, they are the samples that we have for training. And they let me see. One of these lines is the right one and the other one. The dust function is the function the actual function that produces that data. Of course, there's interact with unbounded since H and I can plot this here. This is the, this is the estimation. This is F bar line from the data outside the bounds. In this case is it says confidence interval of 95 percent. Now, this is real function here. Now, here. And this is not, by, this must be 1, 1 sigma, which is 65, right? But this is the optimized version. This is the perfect. Pretty good actually. Because yes, because I didn't hear anything. I yield the parameters in it. That's an Ebola parameters. Now that's because I made it up. Alright, so this is the Bureau optimize you will, of tablets. It says all the parameters have been optimally selected. They haven't been optimized. All right, In this example, I kneel, what is, what was the optimal paths? Now, let's optimize. Let's optimize by chill seeing the noise parameter ten times higher than the optimal one. As you can see, the mean where we have data is not that bad, right? It's not that bad. It's worse than before. The confidence in parallel, this is not 95. It's i'm I'm sure that if it's just once a month, so 65 or whatever, by here, when we're choosing is our confidence. Don't want, which is to as domestic, right? We are, I'm assuming that the noise parameter, or they are sigma square is 10 times higher than it actually is. So then my variance is too big. And then if I choose a nice But amygdala, which is 10 times less than the optimal One or the anthro month. This is why that gap. And this is very dangerous as well. Actually, they function. It's just good. It works. The, the estimation is good. Bye. I am telling the user that the confidence interval of 95 percent and 60 something percent confidence interval is 95. And this is absolutely not true. Because almost all the samples on, until all of that there. Maybe, maybe not here, right? All right. So I am just saying something which is not true. Right? Now. The kernel with a square exponential, but the dependent parameter of this course, but I shall is still the perfect. What, what happens if I chose a square exponential parameter and y, which is set to twice the optimal. Once again, we obtain something that they might be in my worry when our minds. But anyways, But here, here, I am producing a confidence interval which is two, but this is an actual confidence interval. So if we count the number of samples that are out of ignorance into its correct, right? So the next parameter is the right one. But my square exponential is not the best one. So the estimation error increases. Right? So we can see that many times or sometimes the average is outside of the confidence interval. Muslim scientists inside. In general, if you count the number of times the samples are inside the confidence interval, you will get a 65, right? But what I'm saying here is that the solution could be better if we chose a better parameter. If we do then contrary, this is what we get. So if we put a pump or white of the square exponential, which is half the optimum one. What we have is a solution which is too complex. All right? And we're clearly overfitting here. Again the same because it is always good. I mean, it's correct, not good. But the solution is why it's so here where we have a lot of data, I think somehow we're fitting. I am close to the real one here, probably the same, but outside of the areas where am I were a half data, that science is the overfitting is so high. I'm not able to extrapolate that out. All right. And the machine is actually telling me a year to year. It really bad. So don't trust it, right? So this is the actual one, and this is what we get. Same habits here. Because in this happens because the wider the square exponential yes, ist is too small. So they spoke Spanish as they are there. They are thick or thin, right? So we are overfitting. So compare their solution of this. And here we are under fitting, right? So I convert with this, with this solid, this solution. Which has the perfect set of parameters, is pretty close to the actual one here and here. And the extrapolation is not that accurate. Right? So and this is, and this is the set of examples we will see that in action, right? I have two ways. I have program would run this in two ways. They soft way. So using the software that we have available, hallway, programming, everything by myself. So you can see, I will make this available when I forgot about it. But I will have this available for you, right? And then we will see the examples and the difference between my programming, which is simplified of course, and what is in the software in Python or MATLAB. And so we go here. Here basically, we explain what is the method for this constant, kernel parameter. Just forget about slides, about this. Slides. All right? Just forget about this. Actually, I want to reward them, right? Forget about this. This is not right. That's about here, which is what, at what they say. So what is the criterion for parameter adjustment? Usually we use validation, Cross-validation. This might be going to college situations. First when the dataset is small and then when the number of hyperparameters is high, which is what happens in industrial breasts, right? So we have to think that in our case, which is the most easiest one, with square exponential, we have four parameters to adjust. And we're Gaussian processes is primarily meant to go with when we don't have too many data, it has a stronger organization. And on the other side, if we have many data, the cornell dot product matrix is n times n. So it might become unbearable. You have 1 million samples. Just look for something else. We fashion process. I still use Gaussian processes by doing some smart the upsampling using sparse Gaussian processes. There are methods for now. Alright, bye. I advise always, I advise you to use other, other things. For example, we might be using deep Gaussian process. So here in this situation, this is what we have. So we have to explore other session on Friday. So first, let's talk about the marginal likelihood. So we know the likelihood of y given x, given f, which is a, which is w transpose x, or w dot phi of x, y. I am using here the notation on the ball. Here it is a five-day down. Here we have somebody transpose x, and here we have a set of parameters that alright, so here we have x, we have inside of W, and here we have the set of parameters that are additional. So sigma one, sigma, two sigma. And so IBS is a probability estimation. Given the input data. These two, we're going to peel them. Compute this integral, and then we have a lagging about likelihood of y given x on it. So then this expression is the one that we need to maximize with respect to the parameters. Here we have all the hyperparameters that we have in our estimation. So let's kind of guys this thing. The prediction or the vector of all the prediction for the training data. We assume it's a normal with 0 mean and that has a variance covariance matrix. And this covariance matrix, as we have proven, is k is the current. We're now f, y, write y is equal to f plus the error. Here. We are benign the error with just f, f as late. And we can say, we say point y is equal to f plus the error, right? So we see why. When we don't see the only thing that we see is the mean of f hat is a random variable. We assume that the covariance matrix is this one. And we know that the likelihood of f of y is a hormone too. And he's not wrong. It has a mean which is F, this is w transpose x or w dot phi of x. And we know this is the covariance matrix, right? So these two things we have proven now, and here I omitted the fact that their parameters inside. Here we have one parameter. And here we have all the parameters of the kernel matrix by the kernel functions for so that we have these two probability of Y given X and a variety of F given x here is a. Now we put these two distributions, this two in this integral and then we obtain an expression whose logarithm is this line. Right? And so gay wind is what we call KF plus sigma n square here. And Kf is a kind of dot product, a kernel matrix that human thought with any positive definite function, for example, Herzberg function y will. So the logarithm of this probability, since this probability is a Gaussian, that often is this. Here we had the argument inside the exponential. And here we have whatever is multiplying the exponential. Right? I am not just adding the details and above, does or not that may seek to buy, just take any exponent, any, any Gaussian computed already built into us. Now, this has all the parameters inside. We have C minus squared. And here the side, we have sigma one, sigma two, whatever all the parameters of my girl. So the idea, well, first, yeah, something, something which is interesting here. Besides what I want to explain. Something which is interesting. Here we have two terms. These two terms, they have an interrelation. We are very advanced in the glass. But you might still remember about support vector machines. You the right super vector machines. We try to minimize the same time, the empirical error. And they start our error. Right? Here we have two terms. If we optimize this, we are minimizing some term which is empirical that contains the training and test data. Right? And at the same time, we are, we are minimizing a term that doesn't contain y. It's a structure after it contains only the whatever is needed to construct. Wow is a linear combination of the data for the input data. And this is a constant, right? This second remark, this is the number of eight. So here we have a struct author and here we have an a. And there is no see. Remember that in super vector machines we have an empirical term. C plus a stack. Rather, write w squared plus c times the sum of slack variables. C is a free parameter that we needed to tweak. Here. We have this parameter, the imbalance between structural risk, an embedding of reds here itself balance. It's automatic. We do not mean to cross-validate tradeoff between them because it's done automatically with an oven. This is what I'm explaining. This is terms. They started the ban on every recipe is an embedded, but this is the equivalent of paper that has the best army of x. So now this is a staccato risk. We still got up. So we can now go to this expression and change one parameter sweep, one parameter sigma x, y, and z, or the other parameter here we assume that we only have one parameter inside. So let's save this and let's read this and see what habits here. So this is an example which has been copied by reproduce exactly as it is and above. So here we have the log probability, the marginal probability that we have seen. And here what we're doing is sleeping. They parameter, one parameter of a kernel. They take what is on the back there is the privacy lungs landscape in the book, which is the why of the kernel. We certainly on the right. And so we, we brought the log probability, the probability here. Write this probability as blue. And in red, we have the black city term or the structural term that increases. And here what we have is what it's called data fit Gaussian processes. It's the embedding of risk that decreases. We add them together, right? They have to be multiplied times 1.5, which you add them together. And we obtain a maximum log probability. Maximum likelihood here at 10, pretty much close to 1000 beds. 1 something times 10 to the 0 something. Right. And so we do that for we do that for, I don't remember. I don't remember how many data I used here. But it's, as I said, exactly copied from the example in the book. And here we have the same experiment, but done with a lot more training data, right? So if we revisit training data here we have the empirical term, the Stotz-Potter and the sum of both. Now sorry, this is this is three different marginal log-likelihood, log-likelihood with different number of samples. Here we have 21 samples. Here we are. So A1 samples. If we use more samples, if D5, we have a higher log-likelihood. And if we use only eight, we have a lower log-likelihood. By in the three cases, the maximum of the log-likelihood is pretty much the same position. And this is exactly what I want you to, what I want you to see here, to Interbrand here. Alright, first, here we have 120 one neighbor there. I don't remember 2001 data. We have this log-likelihood and these two terms and being off and start their own. And a maximum likelihood disadvantages there. If we use only the data, we have this one here and we use 55. Hardness by freely minus the maximum bears are on the same part, same message. And so they said base functions, they can be computed. Alright? And we can find the maximum. You're saying, gradient ascent. Or if we change a win since the sign using radius distance. Right? So instead of sweetening the parameter, what we do is we choose an arbitrary value for this parameter. And we go, we go to the maximum of using brilliance. And then write this as an example in which we chains. We sweep the characteristic length scale, which is the white of the kernel. Here we sweep the standard deviation of the noise or sigma squared, and we see that there is a maximum here. And this maximum is 10 to 0. So one for the, for the white and about 10 to minus 1 for the nodes, which are the values, the actual parameters. So we say here in this example, this example is interesting because we see here that there is an area which is smooth. And if I start with a set parameters anywhere around this area, using gradient ascent, I will go, I will find this way. If I am here, this region is totally flat, right? So the way to interpret this is like an I-map arrive, we will design action. We fall abruptly because this is step by step. And then suddenly we have a flat, hey, say we are here, the gradient is 0 and so we're not going to work. When we do gradient descent or reading a semi Gaussian processes, we have to start with several sets of parameters. And so we start randomly around this area, some of these parallelizable on here, they go anywhere. By Oliver editors that started around this area. Then we'll work on birds respond. And so what we have to do, what we do usually is to start with randomly chosen parameters and then we Check, check that the solutions, they converge to the same point or different points or to nothing it up maybe also it might be the case that instead of having one maxima, we have several. Alright? So we have to start many times and then see what is the best solution. Alright, a solution that refers to the maximum likely. That said, how to choose them, how to choose name, the date, the initial parameters. There are many techniques. All right, I prefer that you learn by just playing with it, using it. Otherwise, we want never fix this. So if we take this logarithm and we compute it with respect to any, would be the derivative with respect to any parameter. We arrived to this expression. The trace of this, where alpha is the solution. They are all k minus 1, 1, right? That's KEY. So kf processing my C minus I minus one. This expression is very easy to program. So when we do this though, we started with one set of parameters. We want be the derivative with respect to each one of those. We move in the direction of the derivative. And with Andover, that's pretty easy. That's it. Alright? By synthesis, this is that we start with a given set of parameters and the next set of parameters Zip onto the previous one was meal, a small one times the gradient. The gradient of the logarithm of the log-likelihood with respect to the parameters. And there is our expression for each one of these derivatives here. Very easy to grasp. The problem here is that we are optimizing with respect to the training data and that will lead to overfitting. So we do something else which is a little bit more cumbersome. Alright? And it was published before they bought by Erasmus app. So what is the idea here? I hadn't, and I don't need you to understand all the all the algebra. I just need you to, I need you to understand this. So we take all the samples minus one minus I, and we got the likelihood of sum of y with respect to x. To which we have taken, as I can here, y two, which obviously when I've taken x, y by tissue, right? So we compute the likelihood of one sample, one y pi with respect to the rest. So in this case. I I I I they are not part of the training. I'm sorry, we do this. We are right to this expression which is a scalar seen, since this is a scalar, this is a scalar expression, right? And so we have the logarithm of this semi minus y I minus mu i's. And see my MAOI. They have their own expressions. Here. I don't really need you to understand this is just algebra. So this is the expression that we have to differentiate that in order to compute the gradient with respect to the parameters. And so what we do is we compute the gradient of this with respect to all these parameters. We do it for all samples. We add that together, and here's my grade. That's it. And so this is the expression. We hope you will take one sample out. We're not get this, It's livelihood. We compute its gradient with respect to all the parameters. We do a while for the samples and we add them together. We obtain this expression where Z, j, it has this expression here that the derivative of matrix k with respect to the parameters. And that's it. And this is what is supplemented in the subway. All the parameters. And it's in, this is important to do it. Otherwise, the Russian process another hour. If we do this, usually the Gaussian process works nicely. And that's it. That's it. So Monday, we're going to see examples of the Gaussian process. We will, I will first show some linear examples in which we compute the mean. We compute the covariance or the valence, and we compute the parameters. In particular, we compute the parameter Sigma X square. That is the most simplest one. All right, and I will show we open the software so you see how I did it. Basically, I programmed this. Okay. And then we will see examples using the MATLAB software provided by a rational Sanger workers in 12 minimum, right? But I recommend you to use by George, all right, barriers. That is as baggage which is bouts of genes, p, y, voice. It's very powerful. And there is a midterm between MATLAB, which is the last powerful, and forage, which for me is the most powerful, which is included in JetBrains. Second, JetBrains is that company that provides student account for tigers? I didn't know that. Yeah. Now, but that is I was talking about no software, which is very common use as this singular circular. You have a varchar of the Gaussian process. I don't recommend you to use it. Scikit learn is good for, for learning, for doing good things, but you don't want to do, is it? Thanks. All right. You want to do the real thing again and not just go to the real thing. This is more of them to get it right. But I, it works nicely. And of course, when you go to deep learning, I recommend you to take a log to TensorFlow and Keras, right? Use biters for, for learning because there's a lot to learn. Of course, by then. Do not forget about the pure heart warmed types of load. All right, which is the most powerful. Well, there are more things out but don't use just things that are meant to learn. Because you already know postulates. In this case, let's stop here and I see you on Monday. Thank you very much.