 >> Let us talk about the Cloud Edge, that is the front layer of the Cloud. Clients can talk to Clouds using web browsers or the web services standards, but this only gets the outer skin of the Cloud data center, not the interior. So for example, considering on Amazon Web Services, it can host entire websites, for example, target.com, netflix.com, Walmart, and many others. Then it can also use data to actually store the objects like the files, the images, and videos within the website, and then the web service itself that is the server is hosted on top of an EC2 instance, or even it can provide virtual machines that you can do more things like more complicated logic. At the same time, the Cloud provides the ability to scale the service to handle increasing traffic through what we said before as the auto-scaling groups. So the high level, you see the users are trying to access the net and they usually hit the outer part of the Cloud. Client requests are handled by the first tier, and these can be like PHP or ASP pages. Then there's some logic that is being handled and this is what we call the frontend [inaudible] . [NOISE] The frontend developers [inaudible] [NOISE] These are lightweight services and very nimble, and many of these actually use caching. The reason we use caching is because you don't what for every request that comes to hit the [inaudible] [NOISE] information about a specific product that is associated with an ID. We don't want to always look for a product within the database [inaudible] you want to use a cache that you can optimize the queries being passed. In many cases, those caches are usually in memory. So you can really get past performance that have [inaudible] The way we have to achieve scalability is by using shards. So then you effectively use different shards of data that so you can horizontally scale the graphics across all the caches. So let us assume that you have about 1,000 keys that they represent specific approach objects in Amazon, like specific products. Then actually, what you do is you allocate a specific range of the keys within one shard, a specific range of the keys with another shard. So whenever you need, for example, to take a specific product, like buying from Whole Foods, some oranges, then you go to one shard, if you go to buy strawberries, you may go to the other shard. So then what you do is you load balance between different layers in the caching. [NOISE] So many styles of systems near the edge of the Cloud, when you focus on vast numbers of clients and rapid response, you want to really return back the response pretty fast because you know people want to have a great experience when they access websites. Inside, we find high volume service that operate in pipeline manner and they can operate as asynchronously. The reason they operate asynchronously is because if one of the services has an issue, for example, it has a congestion in the CPU or the network, you don't want to block the rest of the services, and let me give an example. When you go to amazon.com and you download the website, what happens is you actually have about 10,000 services that work simultaneously to give you this experience. One service may be the recommendation engine, the other service may be the cart, and so forth. So deep inside the Cloud, we see a world virtual computer clusters that are scheduled to share sources, on which public applications like MapReduce are very popular, and this is effectively the clusters that do what we know in big data. They analyze the data and effectively they provide recommendations back to the users, and the users of these are deep in the Cloud than being in the front layer, and these are done asynchronously. So when you access a website, let's say you go to netflix.com, you can select to see all the catalog. When you see the catalog, the whole machine learning happens. The backend that has [inaudible] data clusters to provide you the recommended movies for you to see. When we click a movie, the latency of actually getting it must be really small. So that's why these have to go to specific caches that sit, as we said in the previous slide, in the front, in number two here. [NOISE] In the outer tiers, replication is a key. So you want to have as many replicas as possible because for example, if everybody is asking for oranges from Whole Foods, especially during a crisis, that's where you effectively may be bottling. So you want to have 10 times those oranges or 10 times the key that represent the oranges between different data shards so that you can be able now to serve the tasks really fast. So we need to replicate this processing. Each client has what seems to be a private dedicated service for a little while. So you make a request, you buy the oranges. If somebody else doesn't request for an orange, then he may go to a different server on the same as you did so you don't get bottleneck. At data, you need to shard the replicated data as much as possible to have copies because you don't have delay. At the same time, you want to replicate the data because one of the instances goes down, and it happens a lot in the Cloud. We want to make sure your request does not fail, but rather you will be able to reroute that request to another node that has the data. So replication is also important for availability and provide proper service level agreements back to the users. In terms of control information, the entire structures managing an agreed [inaudible] [NOISE] by a decentralized Cloud managed infrastructure. Again, the notion of decentralization means that you're able to have multiple replicas. You're able to effectively shard the load so you don't bottleneck a specific instance, and that's why replication and sharding are the two most important elements when we discuss about distributed systems and we discuss about Cloud computing. You're able to achieve scalability because now you can scale the services, as well as being able to have replicas across these scalable services.