 >> So let's talk about sharding. We briefly discussed before about the front layers and the edge of the Cloud. What about the shards? These are the caching components running in the tier-two, are central to the responsiveness of tier-one services. The basic idea is to always use cached data if at all possible, so the inner services, here the database or a search index stored in a set of files, are shielded from online load, we mentioned before of how we could have different caching technologies to do that. Many famous cache technology include Memcached or Redis, and many of the reverse indexing systems that we use to search, most common is Elasticsearch. So I can give you an example, when you go to Amazon.com and you search for a product, that's where you actually use the search index store to effectively identify what you're looking for. Then you return back replication, the key of the specific element that you're looking, for example this may be a dryer that you request from the database, and then from a database you get a list of all the images that you then request from the object store to get the images for those dryers. So you try to use caching as much as possible to effectively shield your databases from the load, because caches are usually run memory compared to database that need to store the data in a more durable store, like the hard disk or even an Elastic Block Storage if you use AWS or other services. We need to replicate data within our cache to spread loads and provide fault-tolerance. Again, we've mentioned about that, how for example if one of the instances that run the caching load goes down, how we should effectively have multiple replicas of the data so we can failover to another instance. In the background, once the new instance comes back with the same software, it's actually empty because it's a cache, so you lost the data in the memory, so you need to find ways to warm-up the data from the other replica. But not everything needs to be replicated, hence we often use shards with just a few replicas. So what is sharding? We touched it a bit more earlier on, but then we didn't really define what it is. So sharding is the process of storing data across multiple machines. So as the size of the data increases, a single machine may not be sufficient to store the data, nor provide an acceptable read and write throughput. So think about it. If you only have a single instance and you want to send thousands of queries per second, the network of that instance may not be able to handle it. If you have like one gigabit per second network or even 10 gigabit per second or 100 gigabit per second, still you may get a lot of load in a single instance. So what you need is you need to spread the load across multiple instances, and be able to use sharding technology to do that. With sharding, you add more machines to support data growth and the demands of read and write operations. So how do you do that? So since the tables are divided and distributed into multiple servers, the total number of rows in each table in each database is reduced. So think about it. Instead of having a table that has 10,000 rows, now if you have let's say four, you can have effectively 2,500 leads of the instances and you reduce that. So now what happens is the load per shard decreases substantially by effectively one-forth. So if you now escalate that to services like Amazon, Netflix, Facebook, Google, that tend to get like millions and billions of queries per second, then having shards is very important in order to be able to do that. Then on top of shards, you need replicas of the data systems themselves or the caches, such that you can achieve high availability at failover. So by sharding you reduce the index size of each of the instances, which generally improves the search performance as well. So you need to also shard your search indexes because if everybody is doing searches, not everybody is going to search the same item all the time, so that's important to shard the logic. But at the same time even if you have what we call hotkeys or hot shards, you need to find clever ways that you can effectively distribute the load in multiple systems. A database shard can be placed on separate hardware, and multiple shards can be placed on multiple machines, greatly improving the performance. Now, as we move from virtual machines to containers, now you may actually have containers in the same machine or different machines and you can also move containers, but we'll discuss containers later on. So think about the virtual machines on separate hardware, you need to be able to distribute the load across those virtual machines. So let's go and get an example here. The second tiers could be any of a number of caching services, I said Memcached/Redis are common technology, and I highly advise you to google these services and look at them, they're actually key-value stores. In memory key-value stores, they have what we call a distributed hash table that runs across all the shards, and then you use a key-value API. So for example, you want to search a specific item like a balloon, that balloon has an ID, let's say ID 100. Then it goes to that distributed hash table that says, "Okay, I have a 100, which node of all the shards is going to help me out?" So in this example, for example you have, this is the shard where you effectively split one terabyte of data in different shards. Another great across is Dynamo, Dynamo is a service created by Amazon, a scalable way to represent shopping cart and similar data. This was DynamoDB, Dynamo is the paper that actually was just created, DynamoDB has changed in the last like almost 30 years since the paper was written, so this is the database that Amazon uses to store all the data. Google uses Bigtable, it's another very elaborate key-value store created by Google, it's built on GFS. Then IBM has WebSphere eXtreme Scale though not so popular as the other ones. But effectively, if you want to read more, I have in the advanced paper at the end of the course, all these technologies like Memcached, Dynamo, and Bigtable, I highly advise you to look at them. It would be very useful to understand what is a distributed hash table, and how we can use these sharding technologies. Then also sharding is cross-cutting, most of the systems replicate data to some degree as well. So as I said, you can't achieve everything with just sharding, you need to also use replication, they have to come together. You need to have multiple replicas of data and shard the data at the same time. So the question is; do we always need to shard data? So imagine a tier-one service running 100,000 nodes, can it ever make sense to replicate data on the entire dataset? Yes, if some information might be so valuable that almost every external request touches it. So if you have what we call a hotkey, you must think hard about patterns of data, access, and use. For example, you may have a unique and specific crisis, everybody wants to use toilet paper or a specific brand of a toilet paper. You are now able to have replicas and shards, because sharding won't work. Sharding effectively splits the datasets, as we said before in four different pieces, but the same key will be made on the same shards. So if you have replicas, now you can spread out the load in multiple instances. This is a good example in the case of a beehive. If we don't make a dynamic decision about level of replication required, the principle is similar. So if you have a beehive that increase, increase, increases, at some point the branch may break if you have it in only one dimension. But if you have across all dimensions, you may be able to spread the load and the branch may sustain. So this is the same idea in the Cloud, as I said in a crisis everybody goes for a toilet paper, so what happens is you better replicate the data across different systems, because sharding will not help, because in sharding you effectively break the keys, the key ranges in four pieces here. So if the toilet paper that you need is in shard one, that means that this will be overloaded and not much load will go to the other shards, so being able to replicate and shard together is where there's the gold dimension. So let's see about queries. So what happens if you try to do updates? So for example you have now three replicas, and you try to update one of them. For example, let's say the count of how many toilet papers you have bought. So let's say you update from, let's say one million to like one million and two or one million and one. How now are you able to make sure that when you make an update in one of the instances, in one of the replicas, that this information will actually propagate into other replicas as well? So you should also think about patterns that arise when doing reads. Queries are easy, queries is once you do an update, for example if all this is now one million and one and the load is like one million, and you try to a read, what you get is you may get incorrect information from one of the instances if the data have not been properly replicated. So effectively what we need to do is we need to do what we call a transaction, where you increase the count by one in one of the systems, and you make sure that you increase the count by one to the other one. So you have a coding system and you have the data, and that's very important when you think about updates.