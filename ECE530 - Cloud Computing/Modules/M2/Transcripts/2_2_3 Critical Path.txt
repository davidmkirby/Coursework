 >> So we have covered a lot of items in regards to sorting, in regards to cast casing, in regards to replication. Let's discuss about the critical path. [NOISE] The critical path is effectively a notion of what is important when a query is made or service so that it sorts properly. So what does critical path means? Let's focus on the delay until a client receives a reply, and that delay must be really small. The reason is because when you go and browse websites, you don't want to get responses slower than a few milliseconds. Just imagine, the blink of the eye is 400 milliseconds. So you need to get back responses and information really faster as a user. So it feels natural to browse across websites and feed the website. So critical path are actions that contribute to these delay. So let's assume now we get update this monitoring in the last criteria for Mrs. Marsh as follows. You get to ask for something on a service instance, on a Cloud, and then you have a delay before you say that. For example, you need to feeds all the data to reconstruct a website. An example is when you go and open your Facebook account, you want the news feed algorithm to work properly, you want the image to appear instantly, you want your messages and the list of the people to appear properly, and then the list of the groups and so forth. So the amount of time it takes from the click to login up to the time you actually get the reports back with all that information on Facebook, that's effectively this service a response delay. Then it's not only just how much the service is going to take, but it's also the network propagation, because when you make a request, you have to go to a server that runs by Facebook that may be a little distant. So that's why you see all these Cloud providers providing regions very close to where the users are or distribute across the globe with six regions and more than that, because they want to be close to that, so they want to decrease the delay in terms of the network. Assume now you're actually going in and updating your shopping cart numbers. You just added a new broccoli that you want to check it out. So if the updates are done asynchronously, we might not experience much delay of the critical path. For example, if you add a broccoli and you add an orange, and then once you to go the checkout, maybe broccoli is not available at the whole [inaudible] store, so Amazon will their back hold, we can find that oranges maybe available. If you actually block the request for all the things you have in the shopping cart, that may slow down, but if you're able to parallelise and do it asynchronously, you may have information in the shopping cart much faster. So updates are done asynchronously, we might not experienced much delay on the critical path. Cloud systems often need to work this way. So we need to avoid waiting for slow services to process the updates, but we may force the tier-1 service to guess the outcome. This maybe the case. For example, in Netflix, when you go and click that you want to see the catalog of the movies when you login, if the machine learning algorithm that runs and provides the recommendation does not work properly, and therefore, you have a slow response, what you do is you probably would fail back to a default response, the default list of the list of movies. So that's the thing, when you don't have proper response, you need sometimes to guess the response, even if this is your audience, and that's fine. Another example is when we go to the shopping cart, you have, let's say five broccolis, and then on the checkout, you actually see three broccoli, say, what the heck? There was an error. So that sometimes means that there was an error in the background that caused this, but then as a user, you can fix that. So sometimes inconsistencies that happened within the website, you propagate back to the user to fix. This avoids the slow service and whenever there's a problem, you actually can let the users know how to fix that. For example, you could optimistically apply update to value from a cache and just update this with the right answer. Many Cloud systems use these tricks to speed up response time. Parallelism is vital for speeding up first-tier service. When you, for example, go to website, you don't only want open one connection to save the whole website. You probably open like hundreds of these connections and therefore, your first-tier services have to be able to accommodate, not just 100 connections, but 100 multiplied by the customers who are accessing them. So parallelism is very important. That's why you you don't have a lot of threads, you need to have a series of the tier-1 because you want to have this parallelism across how you serve the request. So the request has reached some surface instance X. Will it be faster for X to just compute the response or for X to subdivide the work by asking sub-services to do the work? This is where you would go back to starting where effectively you make a request on other service to get that response. For example, you want to find out how many movies does Brad Pitt play. Then you want to have a contract service where you have the contract of the Brad Pitt. So what you do is you ask the service, give me the contract for Brad Pitt. Then you want to ask the service about how many movies has he played. So it's better to divide the work across the different instances of the services to be able to load, to search faster, and to find this formation united. Werner Vogels, a very famous person, he's the CTO of AWS, commented that many [inaudible] pages have content for 50 or more power subservices that are add a real-time audio requests. I can tell you that this was probably a like an outdated quote. Right now, when you try to access a website like Netflix, you have like 5-500 services that run in the background to serve that request. Not a single request, but overall. So 50 for a single client without pre-compute stat backup may be okay even today. So let's see now more examples, specific example. You go to the first-tier, you ask for a specific thing, and then you have this service response delay. The service response delay is comprised of all the latencies, of all the services that will help serve this request. As I said, when you go to amazon.com, you see the shopping cart, you see the in-purchase, you see the recommendation for other services, you see the related products, you see the ratings. All these are different services and all the services have to give you a single website. So if the rating website is very very slow, you need to guess the rating in some cases. Therefore, the service response delay is the cumulative delay across all of these services. So this is the critical path, this is the critical path, this is the critical path and you can see, you parallel some of the request effectively, you can decrease the critical path delay. Now, let's go to another thing, what if we are unable to load balance between effectively some of the services? For example, we don't have a service instance per service, but we have hundreds or thousands per instance per service. Now we can effectively load balance out of that and be able to decrease the services response delay. So as you can see in this case, the most complicated actually is when you do updates. Updates are the most difficult because you update an item. Therefore, what has to happen is, you have to commit to that message to change and then the message has to be propagated back to the initial service. So for example, in this case, response delay usually would also include either latencies not measured in our work now the delay associated with the waiting for the multicasts to finish could impact the critical path even in a single service. So if you have like one service and you send a request for an update, you have to wait for everything to return back and then the cumulative delay is effectively the response from all of them. So you cannot really guess anything because these are updates and have to happen. So now, assume that you send an update, you want update the key, let's say the shopping cart, but you don't want to wait because all that rest of the saves are [inaudible] You load the website, you load the shopping cart, but for some reason, some has not loaded properly in your Amazon.com. So at the same time, you don't want to delay a user from checking out. You want to check him out so you can scout the other users. So the question may happen, several issues just can arise. Are all the replicas applying updates in the same order? What happens if you update, let's say for example, broccoli from 2-3, and then you update and all of a sudden, somebody else also buys a broccoli and now you don't have broccolis? So how can we effectively keep the order that the customer who ordered first is actually getting that and checks out, and then the other customer receives a consistent view that, it doesn't have to go like three websites on their own to see, now Amazon doesn't have broccolis for me. So are all the replicas applying updates in the same order? It might not matter unless the same data item is being changed. But then clearly we do need to have some agreement in terms of the order. What if the leader replies to the end user, but then crashes it and some of other updates were lost in the network? So you may update some. You may say, I won't update my broccoli. I want to have five broccoli, but then on the response, the service got that and updated your shopping cart, but then once you get back the response, the packets on the network were lost because there was some interruption. Then what you see is, you still don't see your broccoli is updated your shopping cart. This may happen. So we have to accept that networks can failed, datacenters can fail, or even things can happen that things may queue up. Still, you may have received response, but they queue up have in your network, so you haven't really seen that website yet. All these things is what would create what we call inconsistencies. Inconsistency is a big problem. When we discuss about Cloud computing, we'll discuss about distributed systems.