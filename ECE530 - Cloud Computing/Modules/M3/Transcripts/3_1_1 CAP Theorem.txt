 >> So welcome. Today we'll discuss the CAP theorem. We're going to start covering in these sections a little more about distributed systems and some of the basics on distributed systems. The reason we're going to do that is because a lot of the Cloud infrastructures we know today is actually based on distributed systems. So the fundamentals of distributed systems are super important when you discuss about the Cloud. In summary, the sections we covered about sorting in availability and reliability, those principles is how you design systems, on how you deploy them on the Cloud. So Eric Brewer is a famous scientist from Berkeley, he developed the CAP theorem. In his famous 2000 keynote at a conference called ACM PODC, he proposed that you can just do two from consistency, availability, and partition tolerance. Bear in mind, when you go especially to interview with big companies and to working in a Cloud business this is the baseline question they may ask, "What is the CAP theorem?" Effectively think about a small database and a database where you want to deploy this database that you want to store some data, and you want to deploy that on the Cloud. There are some principles that this database can have. These principles can be either consistency and availability, or availability and partition tolerance, or the consistency and partition tolerance. You can only pick two, you can't have all three. So consistency means that all nodes see the same data at the same time. So for example, if you have, as we discussed before, a sorted database where you house or if you have multiple replicas of the same data for availability reasons and you try to see the data, you have to see the data that are the same across two different nodes. This is really hard and the reason is because if you try to update the date in one node, the question is, how are these data being replicated to other node so you can get the same view of the data in the other node? Availability is a guarantee that every request that receives a response about whether it was successful or failed. [NOISE] As long as a response can come back, that is important and that notions of availability. But if the node is not accessible because it is down, then obviously this node is not available. Partition tolerance means the system continues to operate despite an arbitrary message loss or failure of part of the system. So let's think about partition tolerance. Partition tolerance is more focus, more about assuming that you have three replicas of the same data, like node 1, node 2, node 3. Then there is a network interruption between one of the nodes that is not accessible. What partition tolerance means that even if you do have this arbitrary message loss, the system can still continue to operate, and that's really important. We can see this in the triangle. I have a few of the most famous data systems here. In order of, where do they really fit in this triangle. For example, an RDBMS database like MySQL and Postgres or Aurora in AWS [NOISE] based on consistent availability. Whereas some others like CockroachDB, Cassandra, [inaudible] , DynamoDB, and CouchDB probably like a pretty old one, and Riak is also dead as well. R:Base on an AP system availability and partition-tolerance. Some other database like Mongo, HBase or HybridTable, or Bigtable, RCP Systems. When we discuss about interactions with web services, we have think about operations commit or fail in their entirety. So this has to happen. You cannot have, for example, "Hey, I just added this item in my shopping cart and half of it is there, but half of it is not there." This cannot happen. This is one we define atomicity, which means that the operation commit or either failed in its entirety. The second aspect is consistent. When you make a transaction and this transaction must be visible to all future transactions. So for example, let's assume that you created [inaudible] updates some keys in the database, and then you try to read those keys, you don't want to mentioned that the second transaction, it will see the data after they have been updated. The third demo we were going to discuss about uncommitted transactions are isolated from each other. So for example, you start transaction, then say, "I want to commit these three keys." But you say started transaction, you can meet the three keys, but you have any piece of transaction and this is uncommitted. So this has to be isolated from all the other transaction you can do. The last is durability. Once something has been committed, it's permanent. In the data systems, especially the ones who deployed the Cloud. When you have a transaction has been committed, that means the data have been flask down to the drive, so it is durable, that is permanent. So the most natural way of formalizing the idea of consistent service is an atomic data object. So definition is, there must exist a total order on all operations, such that it's operation looks as if it were complete at a single instant. This is equivalent to requiring requests, if the distributed shared memory to act as if they were existing in a single node, responding to all operations at a time. So what we go now is to go more deeper into the atomicity and the consistency. So atomic read and writes stored memory, any read operations that begins after the write operation completes must return the same value, or the result of a later write operation. What this means is that, whenever you do a write after read, we must make sure that what we read from the data system is the data that is the most recent after the write. As you used the CAP theorem, consistency is about two things. First, updates at the same data item are applied in some agreed-upon order. So as we go deeper and deeper in Cloud and distribute systems, we see that ordering is one of the hardest problem you can achieve. But what this means is, if you do update one, update two, update three, and update four, you want to make sure that your data system, when you read the data, they will showcase the same ordering. Second, at that once an update is acknowledges to an external user, it won't be forgotten. What this means is that, you want to make sure that once you have something durable, it's going to be there, it's not going to be lost. When you request for that, it's not going to be forgotten. Now, obviously, make sense if you think about single node system, where you have a single transaction, you finish your edited on the hard drive. But now as you have three replicas and four replicas, and maybe 20 replicas or whatever it is, how can you think about consistency when you have multiple load? That's like a super hard problem. In some cases, not systems actually need both properties. When we design a system, we have to understand why we're designing that system. Sometimes we may relax one of these actually elements. If we do that, then we can design a more [inaudible] system, that can still have a market value and that can still serve a number of use cases. Let's go and see where the consistency is important. Because in some cases, consistency or inconsistency can cause substantial risks. So let's see. All of a sudden, you want to pay, you have a check that you want to effectively go to the bank and deposit it. All of a sudden you deposit a check, but then the data are being lost. So let's discuss about what happens when you deposit a check. You put a check, your Skype with your mobile phone, it gets to the banks. The bank, what it does, they open a transaction, they say, "Listen, this person has $10. Now he's going to have $20 because he just deposit a $10." The money cannot be lost. So when you discuss about banks or money, that's when you have to think about consistency. If we go back to what is important for the bank, it's important to be consistent and partition tolerance, CP system. They don't care about availability. What I mean by that, they don't care if the bank is down, fine, it's down. As long as the people were going to see a consistent view of the data after they post a check. They care about consistency and partition tolerance. Partition-tolerance means, for example, your dad access the bank from another state, they will be able to see the same data. So if you have a network partition between these two regions and the data not being propagated properly, all of us on the other area may not see the data. That's really bad. You want to make sure that whoever sees the data, can't see the data there. So weak or best effort consistency. Can we achieve something like, yeah, it's not going to be perfect, but let's assume there is a notion of best effort consistency. Strong security guarantees demand consistency, as we just said like security banks, money, these banks consist. But would you trust a medical electronic health record system or a bank that use weak consistency for better scalability? So what does weak consistency means? Personally, I will not trust. If all officer [inaudible] , the doctor believes that I have high blood pressure from what I told the database to have. So the question is probably no. You want to make sure that consistency is there. We discussed about atomic, consistent, isolated, and durable. Now this forms what we saw called an ACID. This is an acronym that many database developers use. Let's go a little deeper into that. So atomic, everything in a transaction succeeds or the entire transaction is rolled back, period. You do a transaction, you want to write some data in a database, if you don't finish, you have to rolled back. Then consistent, a transaction cannot leave the database in an inconsistent state. If you want to develop and you altered some data and your bank account. You cannot five dollars and then add the $10 later on. You have to make sure that the data is going to be isolated. Transaction cannot interfere with each other. How can this happen? Let's go back to the bank example. You have two people depositing money in the bank account. Two transactions happening at the same time. If those interfere with each other, all of a sudden you will see adding consistent view of how much money you have in the bank account. So that's why transactions cannot interfere with each other. Completed transactions persist, that's the durability even if where the servers restart. Believe me, servers can restart. Whether you call a bank, you call a Cloud provider or whatever it is, servers can dyke and restart, can do whatever it states. So if you have to have this ACID guaranteed in some very consistent system. This quality seems indispensable, and yet they are incompatible with availability and performance in very large systems. So going again back to the bank exam. We don't care about availability. Bank is not going to go down, we can deposit check a few minutes later. It's not a big deal. Of course, not many people deposit banks at millions per second. You see not a few of them in every hour or so, but you don't see millions per second. But when you discuss about online web services like Amazon, then you have millions of people accessing the service at the same time. Then consistency is probably not what you need. I'm going to go later on how you can achieve that. How inconsistency can be okay, actually.