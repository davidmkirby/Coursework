 >> Let us discuss about consistency. I know we covered the CAP theorem, we covered a few of these elements. But consistency is still something that is a very interesting topic to go a little further into details especially when it is about distributed systems. Let's recall the cloud tiers, which actually many cases represents similar to what we see the tiers of the cake. Up to now our focus has been on the client systems and the network, and the way that the cloud has reshaped both. We looked very superficially at the tiered structure of the cloud itself, where tier 1 is very lightweight, responsive web page builders that can also route or handle Web Services method invocations. They're maybe limited to some soft state, so they don't really keep a state. In Tier 2 is where we have a key value stores and similar services that support tier 1. Basically, various forms of caches that can cache the data so the responses to the customer can be really fast. Then during the inner tiers, we have this online services that handle requests not handles in the first tier. These can store persistent files, run transactional services, but we shield all of them from the load. Effectively, the back end runs offline services that do things like indexing the web overnight for use by tomorrow morning's tier 1 services. Effectively, what we see is you have like tier 1, tier 2, inner tiers and then you also have the back end system. Central feature of the cloud is of course the replication. The reason you do that is because you need to handle more work. In order to do that, you need to make more copies. In the first tier, which is highly elastic data center management layer, pre-positions inactive copies of virtual machines for the services we might run. Exactly like installing a program on some machine. If load increases, we create more instances. Effectively, we create more nodes over the instances themselves. Then we use a load balancer that will spray the requests across the nodes. If load drops, so that means that we just kill the unwanted copies. We don't need any really warning for that. The elasticity of the cloud allows through by having replicas of the system itself. Things we can replicate that a Cloud, file or other forms of data used to handle requests. If all our first tier systems replicate the data needed for end-user requests, then they can handle all the work. There are two cases to consider, in one the data itself is write once like a photo. Either you have a replica or you don't. In the other the data evolves over time, like the current inventory count for the latest Ipad in the Apple Store. We can also replicate computations. Here we replicate some request and then the work of computing the answer can be spread across multiple programs in the cloud, some a scatter-gather way. We benefit from parallelism by getting a faster answer because now we can spread the requests across different services. At the same time, this can provide some form of a fault-tolerance, so if one service fails, you still are able to respond back with the query, a based on what you have received from other services. Many things map to replication as we just saw data or databases computation, fault-tolerant request processing, coordination and synchronization like who's in charge of the air traffic control sector over Paris, parameters and configuration data, security keys and list of possible users and the rules for who is permitted to do what, and membership information in a DHT or some other service that has many participants. All these effectively got it so far, get replicated so that we increase parallelism and increase the performance of the system of the services. But replication then becomes the issue of consistency. We would say that a replicated entity behave in a consistent manner if it mimics the behavior of a non-replicated entity, and this is actually really hard. For example, if I ask it some question, and it answers and then you ask if that question, your answer is either the same or reflects some update to the underlying state. Many copies but acts like just one, so what you have with replication is you have all these copies across the board. But if you ask the same question twice, you need to get the same response back. An inconsistent service is one that seems to be broken like you ask for a request, you get less [inaudible] but you are who you ask I requested it would be you actually get something different and that inconsistency is what appears to be as a broken system. A consistent distributed system will often have many components, but users observe behavior indistinguishable from that of a single-component reference system. You have a reference system and then you have an implementation. As you can see, looking at the clock itself, only it appears [inaudible] because it says the date and the time. But it's about more complicated inside that. The dangers of inconsistency is that it can cause bugs. Clients would never be able to trust servers. The problem with these bugs is that because it's inconsistency, bugs appear at random times without the need to know what actually really caused the problem. Weak or best effort consistency which is common in most of today's replication systems. But strong security guarantees demand consistency. The question is, would you trust a medical electronic-health records or a bank that used weak consistency for better scalability? I think the answer here is very interesting. Sometimes you can, sometimes you cannot. Let's say you rent a check and then it bounced back, that unit had insufficient funds. You know, that's a major problem.