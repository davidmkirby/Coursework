 >> In the first part of this series of lectures around [NOISE] distribute systems, and replication, and availability. We discussed about being able to effectively have multiple replicas of the data. As we went over a little on the surface of some of the problems of distribute systems, the consistency between the replicas was a very hard problem to solve. Let's recall from the previous slides that Cloud-scale performance centers on replication, effectively have more applicants because you can have multithreaded access, you can have multiple machines setting the workload. Then consistency of replication depends on our ability to talk about notions of time. Let us use the terminology like If B accesses serves S after A, then B receives a response that is at least as current as the state on which A's response was based, right? That means that we have to keep some sequence of operations based on when it happened. One of the very famous scientists around distribute systems, his name is Leslie Lamport, said don't use clocks because when you have multiple replicas, one thing that is going to happen is that you may have what we call clock skewness. So usually in cloud computing, what happens is we have a centralized time machine. Then all these different replicas it try to synchronize with that time machine. But there are different failures that can happen with that for example, what if you have a network partition and all of a sudden, one of the instances cannot access the time. What Leslie Lamport says, instead of using time and the real clocks, let's use logical clocks. If A happens after B, sorry, if B happens after A, then that's a sequence of ordering. That's when we went to logical clocks. Then it went deeper into vector clocks. But we don't have to cover it in the details right now. We also explained the notion of an instant in time and related it to something like a consistent cut. The next step is, we'll create the second kind of building block for effectively keeping. Trying to figure out how can keep effective replicas in sync, and that's the two-phase commit. There's also a more complicated one which is called the three-phase commit. These commit protocols or any similar pattern arise often in distributed systems that replicate data as we just said. This is very common framework that happens, especially NoSQL databases. Closely tied to consensus or agreement on events and event order, and hence replication. The two-phase commit problem is the problem that first was encountered in database systems. Suppose a database system is updating some complicated data structures that include parts of residing on more than one machine. Effectively what you want is you have this huge data structure that exist across two machines and therefore you have to effectively take it from both. As they execute a transaction is built up in which participants join as they are contacted. So what's the problem? Suppose that the transaction is interrupted by a crash before it finishes. But it was initiated by a leader process which we call it L. By now, we've done some work at P and Q, but a crash causes P to reboot and forget the work L has started. This implicitly assumes that P might be keeping the pending working in memory rather than in a safe place like on disk. But this is actually very common to speed things up, right? You keep in memory, you don't write this. Forces writes to a disk very slow compared to in-memory logging of information, and persistent RAM is usually very costly. How can Q learn that it needs to back out? The problem here is we have a transaction initiated by L, some work done by P and Q. But a crash courses P to reboot and forget the work L has started. Effectively, the question is, how can we get back to the state and how Q learns that effectively it needs to back out because P has been rebooted. We make a rule that P and Q and other participants in it can extend to many instances, treat pending work as transient. You can safely crash and restart and discard it. If such a sequence occurs, we call it a forced abort. Assume that if P fails, then the whole thing aborts. Transactional systems often treat commit and abort as a special kind of keyword, so effectively it says I'm starting a transaction, then I'm committing that, and if nothing happens then I'm going to abort the transaction. Let's see, L executed transactions begin. You read some stuff, get some locks, do some updates in P Q, and R and finally good commit that. So if something goes wrong, you execute on a board. If something goes wrong, then abort message goes back to L. Begin has some kind of system aside id. Effectively what this means is you are assigning a transactional id. You know the information. It acquires pending state, updated it at various places it visited. Read and update or write locks as it acquired. If something goes horribly wrong, you can abort it. Otherwise, if all went well, we can request a commit, but even the commit itself can fail. This is where two-phase commit, and three-phase commits algorithms are effectively used. Effectively if during the transaction you fail, fine, you can abort it. But if everything is successful and then you commit, but then the commit fails, that's where the problem usually arises, right? That's where effectively you need some more complicated logic to handle the two-phase commit and three-phase commit. The two-phase commit problem, the leader L has set of places like PQ it visited. Each place may have some pending state for this, it takes form of pending updates or locks held. L asks, can you still commit? P and Q must reply, no. If something has caused them to discard the state of this transaction like last updates or even broken locks. Usually occurs if a member crashes and then restarts. No reply treated as no, like once you have a no, effectively it's a failed member. If it's a yes, if a member replies, yes. It moves to a phase where we call it prepared to commit. This is very important because we said in the whole transactional items, if anything fails, the whole operation fails. But if the commit fails is where the difficult part is. But if everything's good and we get the prepared to commit state. After when it could just abort and you tie the way, as we said, if data or locks were lost, you at the crash or a timer. But once it says, "I've prepared commit." It must not lose locks or data. It will probably need to force data to disk at this stage. Effectively, you can still have performance by keeping data in memory, but once you get to the prepare to commit phase, that's when you need to write to the disk. Many systems push data to disk in backgrounds so all they need to do is update a single bit on disk. Is called prepared equal true. But this disk that writes is still considered a costly event. Then you can reply back. 'Yes" So L sends out, "Are you prepared?" It waits and eventually has replies from P and Q. "NO" if somebody replies no or if a timeout occurs. "Yes" if only that participant actually replies, yes and hence is now in the prepared to commit state. If all practice participants are prepared to commit, then L can send a commit message, else L must send an abort notice. Note that there could be a mistake in abort, and this is okay. Because even if it aborts [inaudible] it's just aborted it. But if everybody is prepared to commit state, you're in this phase, you can select commit message and then you are done. If a participant is prepared to commit, it waits for an outcome to be known. It learns that a leader decided to commit, it finalizes the state by making updates permanent. Unless the Leader decided to abort, it discards an update, and then effectively it can release the lock. As you can see, we went to the prepare to commit phase. We've got the commit, then finally everything has been finalized.