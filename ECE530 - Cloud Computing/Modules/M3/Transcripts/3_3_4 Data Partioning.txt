 >> Let us discuss today about data partitioning. In the past, we have discussed about the DynamoDB data. One of the features for DynamoDB was the hash-based partitioning algorithm that it uses to distribute the data across different nodes. We will discuss a few of the partitioning methods, the possibilities of how you can partition some of the data. Let's define what is data partitioning. Data partitioning is a technique to break up a big database into many smaller parts. It is a process of splitting up a database or a table within a database across multiple machines to improve the manageability, performance, availability, and load balancing of an application. Effectively, if we have a database, we break it into smaller pieces, so every access to part of the database can represent partially the load, at the same time, if that node goes down, we still have replicas, but the same time we don't lose full availability of the cluster. The justification for data partitioning is that after a certain scale point, it is cheaper and more feasible to scale horizontally by adding more machines than to grow it vertically by adding beefier servers. Of course, this is one of the substance and privileges of Cloud computing. We discussed how we wanted to scale up systems in the Cloud horizontally versus vertical. Now there are many different schemes one could use to decide how to break up an application database into multiple smaller databases. Below, we're going to describe three of the most popular schemes which is used across various large-scale applications. One is horizontal partitioning, the second is vertical partitioning, and the last one is directory-based partitioning. In horizontal partitioning, we put different rows in different tables. Let's now see an example. If we're storing in different places in a table, we can decide that locations with ZIP codes less than 10,000 are stored in one table, and places with ZIP codes greater than 10,000 are stored in a separate table. This is also called range-based partitioning as we're storing different ranges of data in separate tables. Horizontal partitioning can also be referred to as data sharding, and of course, we have defined sharding earlier in this course. The key problem with this approach is that if the value whose range is used for this partition isn't chosen carefully, then the partitioning scheme will lead to unbalanced servers. An example is splitting locations based on their ZIP Codes assumes that places will be evenly distributed across the different ZIP codes. The assumption, of course, is not valid because we have thickly populated areas like Manhattan or Los Angeles, or Chicago. Then we have suburban cities, and we have also smaller areas where there is huge areas that cover ZIP Code with much less population. In this example, if we use horizontal partitioning, the problem is that the load in Manhattan, maybe higher than the load in other places. This is a problem because let's say within the COVID situation that we had a lot of sequences in Manhattan area, that if we had stored this data in another database to analyze that would have become what we call a hot site or hot partitions. Vertical partitioning is when we divide our data to store tables related to a specific feature in their own server. An example is if we're building Instagram-like application, well, we need to store data related to user photos, users upload and people users follow. We can decide to place user profile information on one database setup, like friends list on another, and photos on a third server. Vertical partitioning is straightforward to implement and has very low impact with replication. The main problem with this approach is that if our application experiences additional growth, then it may be necessary to further partition feature specific database across various servers. It will not be feasible for a single server to handle all the metadata queries for 10 billion photos by, let's say 140 million subscribers. Directory-based partitioning, a loosely coupled approach to work around the issues mentioned in the above scheme is to create a lookup service which knows your current partitioning scheme and abstracts it away from the database access code. To find out where a particular data entity resides, we query the directory server that holds the mapping between each tuple key to its database server. This loosely coupled approach means we can perform tasks like adding servers to the database pool or changing our partitioning scheme without having to impact on the application. Now let's focus a little moment in the partitioning methods to the partitioning criteria. Let's say we use key or hash-based partitioning. We apply a hash function to some key based on the entity we are storing that yields the partitioner number. We saw that in the DynamoDB data, how we apply the hash function in order to determine which raw the data will go. Let's say we have 100 database servers and our ID is a numeric value that gets incremented by one each time a new record is inserted. The hash function could be, let's do the ID module of 100, which will give us the server number where we can store in that record. This approach should ensure a uniform allocation of data among serves. The fundamental problem with this approach is that effectively fixes that total number of database servers since adding new service means changing the hash function, would require redistribution of data and downtime for the service. A workaround this problem, of course, is consistent hashing. The other way is list partitioning. Each partition is assigned a list of values, so whenever we want to insert a new record, we will see which partition contains our key and then store it there, for example, can decide all users leaving Iceland, or Norway or Sweden or Finland, Denmark. They will be stored in what we call the Nordic countries table or partition. Finally, Round-robin partitioning algorithm is a very simple strategy that ensures uniform data distributions with N partitions and the I tuples is assigned to a partition. We do like an I module N, this is the very simplest round-robin partitioning algorithm. The composite partitioning is we combine any of the above partitioning schemes to devise a new scheme, for example, first applying a list partitioning scheme and then a hash-based partitioning. Consistent hashing could be considered a composite of hash and list partitioning where the hash reduces the keyspace to a size that can be listed.