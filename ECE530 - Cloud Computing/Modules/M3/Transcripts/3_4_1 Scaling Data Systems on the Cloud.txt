 >> So let us discuss now how we can scale data systems on the Cloud. Back in the days, back in early 2000s, read scaled out by adding replication slaves to a master database, so this is mainly for relation database systems. So what you would do is you'd have a user that will write some data to the master node and then the data would actually be replicated to different types of slaves, so master node and slaves would have actually the same types of data. In this analogy if the slave went down, for example it was terminated, then the master will actually still have the data fine as the writes would go there. Then the reader, which is usually some user, will actually, instead of reading from slave 0 will now read data from slave 1 or slave 2 and so forth, so it would feed off on other nodes. So this would work well in the early days of the Internet because there were a few people that were publishing data to the Internet, a few experts, and then a lot of people that were actually reading data. So effectively writes with the master nodes and then reads with the template slave node, and then a slave could actually also have some transaction. So you will commit some data and unless data is being replicated then you acknowledge the transaction and so forth. Now the problem becomes interesting because now if your slave nodes are across different regions, your transactions may have very long latency. At the same time, you may actually have slaves in other regions just to have backups. So at small-scale things are okay, with relational database systems, having a single node to write data, the node can become overloaded with high write throughput. So on a small-scale it's fine, but once you start discussing about high write throughput cases, then having a single node where you can write the data, it becomes a bottleneck. At the same time nodes can be terminated or restarted by the Cloud provider, you don't know when this happens. RDBMS are what we call CP systems (consistency and partition tolerance systems). They are not designed to be highly available, hence when the master node gets terminated, we need to the re-elect a new master node out of the slaves, which means that now you're blocking all the writes. You are not allowing any writes to happen because you need to wait to re-elect a new master node, otherwise you may end up in a system that is not consistent. Within the last 10 years we've seen also the growth of online platforms, Facebook, Instagram, Twitter, and so forth. A lot of machine learning algorithms that are being run by the data that you are actually publishing online. So that means now we are not being in a case where we have a few writes and a lot of reads, but we're going towards patents where you actually can have a lot of writes. So let's now think two types of designs, an optimistic design and pessimistic design. Now pessimistic design, design with high concurrency, you effectively punish 99.9 percent of the users all the time. But you can achieve a high consistency, but then the problem as I said is you can achieve a higher latency, that results in diminishing the user experience. In an optimistic design you can trust your data store, like know your business and your application, always ask yourself, "Is it really that important to have consistency?" Then figure out maybe a backup plan for that. So one of the database that have actually been very famous for the high availability and actually at scale is Cassandra. It is one of the implementation of the Dynamo protocol in Cassandra. Cassandra is not now strictly consistent, it is eventually consistent, so the data will eventually get there once you publish the data. But what does eventually mean? Eventually means that they could get there a day from now, a minute or a second from now. Actually, what happens in many cases the data get in the other node in milliseconds. So Cassandra, you can think about that as a multi-master system, where you can have like multiple people writing nodes differently, to different elements of the database.