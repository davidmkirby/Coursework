 >> Now, let us go back to the Dynamo Partition Algorithm. So we are going to go a little deeper on the algorithm itself. I hope that many of you have read the reading assignment, which describes parts of the Dynamo protocol. So you may be already aware of these elements. If you haven't, please go ahead and study that paper even after this lecture. One of the things on a system like Dynamo, it uses something we call consistent hashing. What happens is, say you have some input like a text or some information, and then what Dynamo does, it applies a hash function. So a hash function is a form of randomizing the input in some sense. Hash functions are very commonly used in cryptography. When you say, I want to create this, it takes a little bit and then it produces some random outputs. This is simpler methodology. It uses, again, a hash function. The only difference is, within the encryption domain, we use a hash function that is hard to actually reverse them. They're very advanced, computation intensive, whereas in situations like databases, we don't need really too advance hashing functions that take a lot of computation. So what happens is that assuming that you have nodes like A, B, C, D, E, F, G, and then you have a key like K that you want to store it. So what happens now is, you have a range from, let's say 0-100, you take an input A or take an input K, and you hash it. So the output will be a number between 0-100. Assume that number is, let's say five or 10 and assume that the range between A, B is from 0-10. So that means that the key K will actually belong somewhere in between A and B. Therefore, the data will be stored probably, let's say in A, in that node. So that's how, at a high level, it works. So virtual node, it's not categorically possible for more than one virtual node. Let's discuss about virtual nodes. If a node becomes unavailable, the load handled by this node is evenly dispersed across the remaining available nodes. So the notion of virtual node means that now, we don't have a single range that a node handles, but rather it becomes virtual node. So it handles multiple different ranges. So nodes B, C, and D store keys in range A and B, including K. So that means that now, these three nodes include everything within A and B, so that becomes a larger virtual node. When a node becomes available again, the newly available node accepts a roughly equivalent amount of load from each of the other available nodes. So the number of virtual nodes that a node is responsible can be decided based on its capacity, account for heterogeneity in the physical infrastructure. So what this means, it means that assuming you're deploying Dynamo in an infrastructure, what do you have? One, it's something that has two eyes, the hard disk, then the other ones. You want to make sure that the corresponding capacity of a virtual node is evenly distributed based of the capacity of the nodes in infrastructure. So let's see variant of consistent hashing. Each node now is assigned multiple points in the ring, like B, C, D stored key range A, B. The number of points can be assigned based on node's capacity, as we said. If node becomes unavailable, load is now distributed to other nodes as we discussed before. So assuming now you store data key K, the coordinator for that may be, let's say node B. Node B maintains a preference list for each data item, specifically nodes storing that item. So what happens is preference list skips virtual nodes in favor of physical nodes. So D stores A and B, B and C, C and D. So effectively, what happens now is that the data from B is now replicated to C and it's replicated to D. So D now stores everything from A and B, B and C, and C and D. That's the whole range it handles. E now handles everything from B to E. F handles everything from C to F. G handles everything from C to G, and so forth. So then what happens if you receive the data? Then one node becomes a coordinator and make sure that it replicates the data to the other nodes. Each data is replicated at N hosts, in this case, at three, it has this notion of preference list. So that's the list of nodes that is responsible for storing a particular key, as we discussed. Data versioning. So DynamoDB also uses data versioning. So let's see about data versioning. A put that is a right call may return to its caller before the update has been applied at all the replicas. We do that because of low latencies, otherwise, it will be like a transactional system. A get call may return many versions of the same object. That is because when you do a put, let's say you add an item to your shopping cart, that ends up becoming a write in the database. Once you try to fetch it, maybe the data have not been replicated, or maybe there has been a network partition that didn't allow the data to be replicated. So the challenge, an object having distinct versions sub-histories which the system will need now to reconcile in the future. What means is you may have, let's say you give it a number from one to two, now the aggregate of the master node equal to 2, but all the other nodes are still at 1. So what Dynamo uses, it uses the notion of vector clocks. A vector clock in order to capture causality between different versions of the same object. So in a shopping cart example, Add to cart and delete an item from cart operations are translated as put requests in Dynamo. Add to cart operation can never be forgotten or rejected. When a customer adds items to a shopping cart and the latest version is available, the item is added, the older version and the divergent versions are reconciled later on. Dynamo treats the results of each modification as new and immutable versions of the data. Immutable means they cannot be changed. Most of the times, the new versions subsume the previous versions. Version branching may happen. In the presence of failures combined with concurrent updates, resulting in conflicting versions of our object. So what may happened is, now you to a put in a node and another node that's a write or an update, now you have federally conflicted nodes, you cannot really reconcile. In this case, the system cannot reconcile multiple versions of the same object and the client must perform what we call a reconciliation. Example is merging different versions of a customer's shopping cart. Add to cart operation is never lost. Deleted item can resurface. Let's have a single example in the case. Assume you add some data in your shopping cart from your laptop. At the same time, you sell your cart with somebody else. There's some data in the shopping cart from their smartphone. That results now in version, different updates happening from two different systems effectively. So the way we do that is through vector clocks. A vector clock is a list of node counter pairs. Every version of every object associated with one vector clock. If the counters of the first object clock are less than or equal to all the nodes is [NOISE] in the second clock, [NOISE] then the first is an ancestor of the second and can be forgotten. Otherwise, what happens is that the client has to do the reconciliation. Let's go a little deeper around the algorithm of the vector clock. So assuming now a client writes a new object D1, the node Sx that handles the write for this increases, the sequence number. Until the client updates the object D1 and D2, D2 overrides D1. There might be replicas of D1 lingering at nodes that have not seen D2. So in this case, for example, you may have a write handled by Sx, you may have from one, it became to two. [NOISE] Then you have a write. Now, for example, you have another increment that is now handled by Sy and another that is handled by Sz. So what happens now is the write, since it's handled by two different nodes, you may have diverse information in the system. Effectively, if the next write now is reconciled Sx, you now may have three different versions in the system. So you need to find ways that will need to reconcile that because you cannot  establish a relationship. In this case, with D1 and D2, within that same relation because it was handled by the same node, that is Sx. But once the data handled by different nodes, it's hard to handle some source of causality so you can figure out what is the most recent update. Eventually, what happens is you need to expose the divergence of the put operations up to the customer or make a random choice. A random choice obviously is the wrong way to go, because if you make a random choice, you will deal with a lot of problems. But what happens most of the case, the system, the application may see three different versions. So you can route its request through a generic load balancer that will select a node based on load information. So you can use a partition-aware client library that routes requests directly to the appropriate coordinator nodes. So this is the most important. Assuming now you write some data to a node Sx, and all of a sudden, what happens is you have a generic load balancer or a client library that says Sx is overloaded. When I push y, all of a sudden, now you cannot establish causality. But you have to do that because sometimes, essentially, it become too overloaded, so you may have to distribute the workload. If you do that, now you have to have the promise of eventual consistency. One of the reasons you can solve this problem is, as we said, with vector clocks. There are many other ways you can solve the problem, but if the problem cannot be solved, you have to propagate up to the customer as we said before. Sloppy quorum;  reads and writes is the minimum number of nodes that must participate in a successful read and write operation. Setting R plus W greater than N yields what we call a quorum-like system. So what happens is [inaudible] now a divergence, assuming that you write the data in three nodes as we said in the beginning. So you write the data in three nodes. All of a sudden, two of the nodes have wrong information, or let's say two of the nodes have correct information, one does not. So if you try to say, I'm going to do a quorum write, that means that as long as the write has been accepted by at least two nodes out of the three, then the write will be accepted by the system. If it doesn't, then the write will be accepted by the system. In this model, the latency of a get is dictated by the slowest apps. So if you have, let's say, three columns of data, and you won't acknowledge two nodes, now over [inaudible] data will be higher because you have to get acknowledgment from two nodes. For this reason, R/W are usually configured to be less than N to provide better latency. So assume that N is equal to 3. When A is temporarily down or unreachable during a write, send replica to D. D is hinted that the replica is belong to A and it will deliver to A where A is recovered. Again, always writeable. What it means is that you can also hold off some hint, that the data have been deleted and you can update the data back to A. What may happen though is that there might be another update date before D sends the data to A. In this case, you have to use some, let's say last writer. Let's say you have some timestamps, that will actually win. So if there is an update date before the sends data back to A, then that is what you effectively have, do not update the date. So other techniques that DynamoDB uses is replica synchronization like Merkle hash trees or membership like Gossip. I'm not going into detail about that. I think the whitepaper which I posted light should have a lot of these details. Dynamo was developed in Java. Java is not always the best technique for web services, though it has a lot of easiness because of JVN issues. Locally persistence component allow for different storage engines to be plugged in. So you can use Berkeley database, MySQL, BDB, and so forth. So you can use different storage engines [inaudible]. Persistent store either Berkeley DB transactional data store, BDB, MySQL, or in-memory buffer. All are in Java. The common setting for N, R, and W for Dynamo is 3, 2, 2. Results are from several hundreds nodes configured as 3, 2, 2 nodes [inaudible] on a single data center. Conclusion, Dynamo is scalable. Easy to shovel in more capacity at Christmas. It's simple, you have two APIs, get, put. It's flexible. You can set N, R, W. It's inflexible. Apps have to set to match the needs. Apps have to do their own conflict resolution. They claim it's easy to set these. This mean that they have many intersecting points. So bottom line is it constricts, builds, and flexibilities depending on how you view the system. So sometimes application have to set it, it becomes complicated, so they don't like that. But it has to be a collaboration between the app developer and the DynamoDB engineer. [NOISE]