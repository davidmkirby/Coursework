 >> Let us discuss today about the active cloud storage. By active, I mean the cloud storage that can be easily accessible by different types of virtual machines. The infrastructure as a service providers supports different types of storage, and hardware storage believe they provide storage for the users. One of them is the ephemeral storage, the second is the block storage, the third is the object storage, at the last one is the data archiving. Of course, there are other elements that they provide like file systems social service and a few occasion like caches of data before they actually get to the storage elements. But fundamentally, these are the four elements of storage. One important aspect here is that in many cases, object storage and data archiving are effectively in the same category [NOISE]. It depends on the storage access you have, like the block. A block is a fixed-size block in a specific location. The way you expose the block to the instances is through a specific file system. The second way you can access the data is through files. This is a specific folder or a file structure. When you have the location and also has the properties. The final one is the object, where you have a specific data and metadata that you have collected. Usually you need some form of APIs, like the data that use it as POSIX semantics as you would use in a file system. Ephemeral storage is effectively the old instance storage. Ephemeral storage is used to store data in the compute instance. The lifetime of the data are based on the lifetime of the instance, which means that when the instance goes down or when the data needs to be replaced, the data would also be used. An example is if we deploy a platform as a service data store like a database, we need to have a logical for data replication. An easy way to have that is because if the instance goes down, we may lose the data. An infrastructure as a service provider provides an instance family that include high storage instances. That means sometimes very fast SSD-backed instance storage, optimized for high random IO, provide high IOPS at a low cost. An example is i3's, i4, i5 families right now. The use cases could be like a NoSQL databases like Cassandra, MongoDB, scale out transactional databases, data warehousing, Hadoop, or even a cluster file systems. All of these we actually use [NOISE] their data storage because it's fast and because it's local axis. [NOISE] Another way that you could access data is through a block storage, and the block storage services allows the user to create storage volumes and attach them to VMs. From the operating system perspective, whether this is block storage or even the local storage, it actually appears as the same. Once the volume is attached, the user can create a file system on top of these volumes, ran a database, or use them in other ways. The fundamental aspect here is when you attach a block storage, you probably need the file system because users have been accustomed to files and folders format. Block storage volumes by most Cloud vendors tend to be placed at specific availability zones where there are automatically replicated to protect you from the failure of a single component. When specificity means that some of the high availability aspects of the block storage have now been taken care by the provider who provides the block storage. In comparison to, let's say, using the oldest storage, in which case, the user is now responsible for the replication of the data and make sure that there's high availability. In the block storage space, the vendor is now providing a highly available ecosystem. The block storage volume types offers durable snapshot capabilities that are designed for at least five dots of availability. This means that, [NOISE] for example, the block storage service can provide snapshots to allow that more durable storage that may provide durability. Then for that, when you come back, you cannot actually see the data. You don't really use date. The block storage system balances the creation, attaching and detaching to the block devices to servers. Usually this is done either through SAP for Buffett API or it could come through the CLI or data. You got instance you buy backs up like local interface callbacks and you try to attach the volume itself. [NOISE] Block storage is appropriate for performance sensitive scenarios such as database storage, expandable file systems, or providing a server with access to raw block level storage. When you think about the difference between the local storage, in most cases it is a network hub. The blocks storage case, because you attach the volumes, now you need to go over the network. However, by the Cloud providers, they provide specialized high-throughput interfaces only for the block storage. So you actually get really high throughput for the network. But then the latency is what matters from that point thereafter. The user can have different factors of the block storage. They can dynamically increase the capacity. They could potentially tune the performance, or even change the type of live volumes with no downtime or even performance impact. Data flexibility does not exist on a [inaudible] storage because of the instance, you cannot dynamically change the storage of the instance, but with blocks storage, you can say, "I need more space," so you can dynamically [NOISE] add more capacity. Signal with the snapshot management, maybe they provide this functionality of backing up data stored on block storage volumes. Network stops can be restored or used to create new block storage volumes. For example, you can snapshot something, say, "I'm going to create a unit stop and I'm going to copy the data that I snapshot that." That makes a lot of flexibility there for applications. You access these stops, you create another idea and you just need to copy the data. You don't really need copy because the provider provides the snapshot and restore mechanism to do that. [NOISE] The most famous block storage system which the AWS, EPS, [NOISE] there are different varieties they provide. They provide varieties where the regular solid-state drives and even hard drives. At the same time different performance and also cost will meet you out of them. So in the volume type, there is what we call EBS Provisioned IOPS SSD, and EBS General Purpose SSD. The provisioned IOPS means that you actually get specialized IOPS to the disk. Effectively, your operations now have a low bow that they provide these SLAs. In some cases, it is the highest performance SSD volume designed for latency-sensitive transactional workloads, whereas the gp2 is focused on the general purpose SSD volume that balances price performance for a wide variety of transactional workloads. Now, sometimes you may want to use EBS with hard drives like st1 or sc1. These are usually low cost. The access and the throughput are not that important, but you want this flexibility of scaling up and down your storage as you attached it to the instance. You still need to work to use these file semantics. [NOISE] If you see a use cases, one use case would be for very high [inaudible] like those SQL relational database like you do thousands of queries per second, whereas it cope HDD, the file drives for block storage, this is when you have enough few scars per day, a few complicated operations. At the same time the data varies, but fundamentally, when you go down to the throughput, you can see the difference. One is obviously like four types backs throughput compared to the SUR, the IOR is four times larger. In terms of max IOPS, the IOR Cadillac batch faster. What it means is you don't only get the ability to attach blocks devices to instances, but you also get a variety of SLAs across these different services. [NOISE] In EBS, and this holds for many Cloud providers, at no additional charge for the customer, Amazon EBS volume data is replicated across multiple servers and availability zones to prevent the loss of data from the failure of any single component. That's the beauty. With the fibula storage, they usually has partner that with EBS, the provider provides that. Your EBS volumes are designed for a general failure rate of about 0.1-0.2 percent. Failure refers to a complete or partial loss of the volume depending on the size or the performance of the volume. Of course, when we think about the equivalent of, what if we used our old hard drives, this is about 20 types more that our typical commodity disk drives, AFR is about 4 percent. For example, you have about 1,000 EBS volumes for one year, you should expect one or two will have a failure. I get the failures are handled by the provider, not by the user, but this is the maximum you should expect. You hope this might be some service disruption, but it is low, because the provider provides them. EBS also supports a snapshot feature which we discussed in case you wanted to do some backup. If something fails, you still have a backup somewhere else. [NOISE].