 >> Data pipeline without a messaging system. Let's start with this concept. Message in the Cloud. Messaging deals with sending data over the internet to scale applications. It is stateless, i.e., does not store data. It is used to synchronize Stateful applications. It basically carries the state but itself is stateless. The question is now, how are we able to send messages within the cloud? What released here is effectively some of the properties that those messages will have. Message broker is an intermediary program module that translates a message from the formal messaging protocol of the sender to the formal messaging protocol of the receiver. Some examples of some of the messaging broker is: Apache Kafka, Apache ActiveMQ, RabbitMQ, Celery Task Queue, et cetera. Messaging is an important piece of infrastructure for moving data between systems. To see why, let's look at a data pipeline without a messaging system. Before I go to this example, I want to mention that a lot of these systems are currently used in productions by many companies. This is the fact way of how you communicate between different types of services. They provide some added guarantees as well, that we'll discuss later on, like availability, and so forth. Let's assume that we have no messaging system. The system starts with Hadoop for storage and data processing. Hadoop isn't very useful without data, so the first stage is to get the data inside Hadoop. So far it's not ideal, but you can still get a data to Hadoop. Unfortunately, in the real world, data exists on many systems in parallel. The sources of the data that exist in so many applications, for example, if you have built a service that deals with, let's say, Amazon, you have got so much data that you generate with your try to shop some products. Effectively, you want to send the data to a system, big data system to do some analytics. In the first place you have at least all the systems we'll have to interact with, Hadoop. Let's provide more clarity. The situation quickly becomes more complex, but it's ending with a system where multiple data systems are talking to one another over many channels. Each of these channels requires their own custom protocols and communication methods, and moving data between these systems becomes a full-time job for a team developer. What we see here is we have systems like DevOps Metrics Applications that a [inaudible]. All of these are actually sending messages with Hadoop. But then you would extract some of these messages for a job to do some fraud analytics, or to do some monitoring, or to invest in some of the security measures. As you can see here, Hadoop now becomes a central system of injecting a certain data, whereas it was developed for specifically to big data. Now data effectively Hadoop becomes a data pipeline itself, which was not what it was created for. What will happen eventually is, you have a single system and all these channels and you end up in this case. When you have substantial congested roads because your system cannot hold the workload. The question is, how can you achieve some congestion avoidance [inaudible] control all this traffic that goes through your big data systems. [NOISE]