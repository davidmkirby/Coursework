 >> Today, we'll discuss about a messaging system which is called Apache Kafka. It's part of the open-source Apache foundation which most of the code is open-source and a lot of the documentation can also be found online. Kafka is a major component across the world in many industries, it is used by most companies right now because it provides a lot of great guarantees in terms of how the data is moved between different systems. Kafka comes from the German novelist Franz Kafka. Jay Kreps, who is one of the co-authors of Kafka, chose the name since it is a system atomized for writing. Kafka is a publish-subscribe messaging and it is rethought as a distributed log. I have posted a link here of the Kafka documentation incase you want to go a little bit deeper and I totally suggest that you would. Look, the data would not be lost that means it's highly available, so people were actually be able to write at all times. It is also resilient if the data gets lost and so forth. It does that by keeping multiple copies. In the real world data systems, these characteristics by Kafka are ideal fit for publication integration between coupon rates of large-scale data systems. An example that we provide here is where you have like the other basis, what do we have connectors that connected databases to the Kafka Cluster. It writes data to the Kafka Cluster and then what you do is you could also have applications, that write data to Kafka cluster. Then what you have is you have what we call stream processors like systems that actually takes up data, breachs data or takes data. An example would be, assuming you want to write the data for the actors. But you ought to enrich the data of the actors with some specific colleges they have that you have an application. You may save the data [inaudible] enrichment of the data that I maybe have like consumer applications. The basic architecture of Kafka is organized around a few key steps. The first one is that topics, that is all Kafka messages are organized in to topics. If you wish to send a message, you send it to a specific topic and if you wish to read a message, you read it from a specific topic. The second data is producers, you have the people who are actually all applications. What I'm actually producing the data. Producers effectively, what they do is they push messages it to a Kafka topic. Consumers on the other hand, is the applications that you're scripted data out of the Kafka Cluster. Now the brokers Kafka, as a distributed system, runs in a cluster. Each node in the cluster is called the Kafka broker. That means that you could actually have a broker of the cluster itself. Now let's go and see in more detail about the topics the brokers that consumers and the producer. Kafka topics are divided in partitions. Partitions allowing you to parallelize a topic by splitting the data into a particular topic across multiple brackets. Each partition can be placed on a separate machine to allow for multiple consumers to read from a topic in parallel and as we do that is both for availability as well as for performance. Consumers can also be parallelized so that consumers can read from multiple partitions in a topic allowing for very high message processing throughput. What you need here is you have partition was zero, one and two. You have the best is being posted in parallel so you have high availability but the same time you can have parallelization of how you consume the data from the producers. Each message within a partition has identified, it's called an offset. The offset is the ordering of the messages as an immutable sequence. Effectively we put that offset asset ID to make sure that we know where the data. For example, here, you can see also maybe 0,1,2,3,4 and so forth. Kafka maintains this message ordering so it provides ordering guarantees. Consumers can read messages starting from a specific offset and are allowed to read from any offset point they choose. I like consumers to join the class at any point in time as they see fit. It means that now you have this queue and now you have consumers that either data to Kafka and that means they can consume data from any point during the queue, they can consume data at any offset. Offset is used as an identifier to know where and what you have read. Given these constraints, each specific message in a Kafka cluster can be uniquely identified by a tuple consisting of the message's topic, the partition, and the offset within the partition. If we go back, this is the topic, this is the partition and there we have the offset. A topic has an ID, a partial ID. All of these elements is to identify the offset of what this message belongs to which topic, which partition and what offset it has. Let's go ahead to Kafka partition. Another way to view a partition is as a log. A data source writes messages to the log and one or more consumers reads from the log, and at the point in time they choose. In this case we have the log 0, 1, 2, 3, 4, 5, 6, 7. We have a data source that writes the data and then you have a destination that uses offset there at four and a destination that uses offset there at seven. In this diagram, a data source is writing to the log and consumers A and B are reading from the log at different offsets, which is perfectly fine. Now each holder holds a number of partitions and each of these partitions can be either a leader or a replica for a topic. Of course, a replica means that you have a liability. All the writes and reads to a topic go through the leader and the leader coordinates updating replicas with new data. If a leader fails, a replica takes over as the new leader. In this case you have leader partition 0, leader partition 1 and leader partition 2. They take the writes and the reads and they manage to the replicate the data across the board.