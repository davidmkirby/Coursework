 >> Now, discuss the introduction for Kafka. Let's now go and discuss about producers and consumers, and then we'll go a little deeper about consumer groups as well. Producers write to a single leader. This provides a means of load balancing production, so that each write can be serviced by separate broker and machine. In this case the client writes partition 0, and the data being replicated to the different brokers, like Broker 2 and Broker 3. The producers is writing to Partition 0 of the topic, and Partition 0 replicates that write to the available replicas. Associate leader is red, replica is blue. Now, let's assume that the client is writing to a different partition. Then that partition broker number 2, partition 1 will now replicate the writes to different replicas in blue in other red brokers. So irregardless of what the client writes to his broker, the data will be replicated across different brokers. Consumers read from any single partition allowing to scale throughput of message consumption. This is similar to what we said before in a master-slave database, where effectively the client writes the buster note, in this case, for Kafka, it is the leader, and then reads can happen from any partition, whether it's blue or red. Consumers can also be organized into consumer groups for a given topic. Each consumers within the group reads from a unique partition and the group as a whole consumes all messages from the entire topic. [NOISE] Now, let's see the possibilities of how consumers and partitions can be distributed. In the first case, if consumers is larger than partitions, consumers will be idle, because they have no partitions to read from. Now, if partitions are more than consumers, consumers will receive messages from multiple partitions. This is a good thing, right? Because you don't have any consumers being idle. Now if the consumers and the number of partitions are equal, each consumer reads messages in order from exactly one partition. So, you don't have neither consumers that are idle or partitions, that are idle. Let's see an example here. In this case we have Kafka Cluster. We have server 1 and two partitions, zero and three. We have clustered server 2, which has two partitions one and two. We have two consumers group, in this case, consumer group A, and consumer group B. Consumer group A has two consumers, C1 and C2, whereas consumer group B has four, consumers 3, 4, 5, and 6. As we see here, in consumer A, now you have consumer number 1 consuming from two different partitions that is because the number of consumers is less than the number of partitions. [NOISE] Whereas, in consumer B, you have equal number of partitions and equal number of consumers within the group itself. Now, let's move to a few more interesting topics. We have discussed earlier in this course about distributed systems, consistency, availability, and partition tolerance. Let's go through our assumption. Assumption is producing to one partition and consuming for one partition. This does not hold when you're reading it from the same partition, using two consumers or at the same partition used to produce, as we said before. So, Kafka makes the following guarantees. Message sent to a topic partition will be appended to the commit log in the order they are sent. That means, within a partition, you can keep ordering. A single consumer instance will see messages in the order they appear in the log. Once you consume the data from a single consumer, you will see exactly the same number as it is in the partition. A message is committed when all in sync replicas have applied it to the log. That means that once you write data to one of the leaders. If the data has been committed to all the replicas then you can say that this has been applied to the log. Any committed messages will not be lost, as long as at least one in sync replica is alive. So this means that if all sync replicas have acknowledged the write from the leader, that means that if the two dots go down, still the last replica can still serve the data to the consumers. [NOISE] So the one and two guaranteed ensure that message ordering is preserved for each partition. Note that message ordering for the entire topic is not guaranteed. What we see is within a partition, actual data, you definitely will have more of it. But across partitions, you may not. Now, the three of the four, the commitment type of data guarantee ensures that committed messages can be retrieved. In Kafka, the partition that is elected the leader is responsible for synching any messages received to replicas. That means that the leader would get the right and make sure that all the replicas are in sync. Once a replica has acknowledged the message, that replica is considered to be in synch with the leader. If it does not acknowledge that means its not in sync. If we go back here, let's see. This is the partition he tried to replicate. Partition 1 and partition 1. If both replicas have actually acknowledged that the write has been made that we consider all three in sync. Let's continue. That all messages are sent to the partition's leader. The leader now is responsible for writing the message to it's own in sync replica and once the message has been committed is responsible for propagating the message to additional replicas on different brokers. So we see it gets a data. Make sure that broker 2, broker 3 also receive the messages. Each replica, that is two and three, acknowledge that they have received the message and now can now be called in sync. By doing that, you can effectively have a multiple broker going down or going out. So let's go through the different failures. The writes will not, let's assume you have a replica failure. Writes will no longer reach the failed replica and it will no longer receive messages, failing further and further out of sync with the leader. So if this fails, this is out of sync. We see now second replica failure. The second replica will also no longer receive messages and too will become out of sync from the leader. At this point in time, the only replica, that is in sync, that is the leader itself. In Kafka terminology, we still have one in sync replica, even though that replica happens to be the leader of this partition. So this means that these are out. This is the only available. Effectively, if the leader also dies, we're left with three dead replicas. Now replica 1 is actually still in sync because it received the write. But it cannot receive any new data. But it is in sync with everything that was possible to receive. Replica 2 is missing some data. Replica 3 is also missing some data. So, we still have the data replica 1 up to the point that it failed. But no new writes will be accepted. But the other half, partition 2 and 3, are out of scope because they have failed. So there are two possible solutions. The first scenario is to wait until the leader is back before continuing. Once the leader is back it will begin receiving and writing messages and as the replicas are brought back online, they will be made in sync with a leader. So what this means is, any new writes will fail, but once the leader comes back, it will start acknowledging the writes and it will be back in sync. Now the second scenario is to elect the first broker to come back up as the new leader. This broker will be out of sync with the existing leader and all data written between the time where this broker went down and when it was elected the leader will be lost. The other way around is effectively to wipe the whole state that existed in the commit log and effectively start from zero. So that's the two options you have right now in these failures. [NOISE]