 >> Let's do a summary of the data streaming technologies that exist. One of the technologies that exist out there is RabbitMQ, and many times you'll hear that technology as well. Use Kafka when you have a fire hose of events right? It needs to be delivered to the partition order at least once, that means one consumer must mix at least on data. With a mix of online versus batch consumers, you won't be able to re-read the messages continue with current limitation that are around node level HA, high availability or you could even use like the trunk code that is posted online. Well, the other had RabbitMQ. It's another event streaming system. It's for lower number of messages, right? You need to be routed in complex ways to consumers, you want to prepare message delivery guarantees, you take care about all the delivery HA at the cluster node level. However, I'll have to say that Kafka has become the most popular technology overall in streaming. Especially because it's open source and many companies are actually using it. There's also other services like Amazon Kinesis. If you use AWS and even other Cloud providers, many of them actually they provide their own messaging system and it all streaming system. One of them is Kinesis, which comes in AWS. Some of the similar feature include like messaging system for large-scale real-time data processing, high-performance, highly scalable, low latency, fault tolerant. But there are differences. The other is like fully managed by AWS versus effectively Kafka, because you need to run it by yourself as a company. You need to trust the technically open source code or effectively you need to pay another common like Courtflow or something else that will host Kafka for you. However, there are many variations of Kafka that may exist in different cloud providers that you can also use. The second aspect is from Kinesis you actually have all these beautiful services the grazers like, for example, you could use DynamoDB, key-value store that effectively sends the data to Kinesis stream and that you can consume this data to do something else, so It's very well integrated. Whereas if you were to plug in like Kafka to DynamoDB, you have to build some of these technologies how you actually take data out of the 20 different queuing system. In the day and this goes with everything within AWS, all the services they provide, they try to make it very well integrated with each other. The reason is that the benefit of actually staying within the ecosystem is the fact that they provide you all the variety of services to get. Data durability and performance trade-offs, again like Kafka probably is better as an ecosystem itself, It's more durable it has better performance guarantees, It has like better limitations on the size of the best, as you can send. But I guess this balances out this on whether you are the man of service or to run it by yourself. I will propose to take a look at some of these websites that actually includes some of the information which I discussed in the section and actually the Kafka documentation is very important for you to understand this ecosystem. Thank you very much. [NOISE]