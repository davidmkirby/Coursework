 >> So in the previous lectures, we discussed about Containers, what is a container, what is a control group, and what is the elements that you can build in a container itself. Now let's go to the next phase where it's, how do you orchestrate containers? By Container Orchestration, I mean, how do you orchestrate billions of containers across perhaps millions of each virtual machines? So if you try to find the equivalents of that in the freight example we gave in the freight ships, it's how do you actually start putting down all these processes that they do it differently inside, let's say a specific packaging, packaged in a specific way? For example, how do you put these containers inside the freight ship to ship the goods? So this is what we're trying to do. It's more like the logistics from what we discussed in a good space to how do we do that in the container ecosystem. So all containers share the same resources like CPU, RAM, disk, various kernel things, and so forth, and what we need is we need that fair distribution of goods so everyone gets the fair share. So we need also to prevent denial of service attacks. When we think about prioritization, this is a great saying by George Orwell, "All animals are equal, but some animals are more equal than others." So what we did this we did away, effectively, to figure out priorities across containers, we need to figure out how we optimize the use of resources across containers, and how do we do that across a number of virtual machines? Because when you deploy a service at scale, you will deploy tens, maybe hundreds, maybe thousands of virtual machines and maybe thousands of containers on top of that. So we need container orchestration systems that can facilitate microservices architecture by provisioning containers, balancing the container dependencies, enabling discovery. So we want to identify whether this container, if it's healthy or not. Handle container failure. So if a container goes down, then you bring a new one so it rejoins back the applications, the pool of the containers that serve the application. We want to scale the containers and we want say, now you have the containers, next day, I want 10,000 containers. There are different flavors that can do that, one is the Docker, the other one is Kubernetes, and the other Mesos. Kubernetes is probably the one that is the most famous right now, but the other two are also fairly used across the industry. So you have so many possibilities. So the question is, which are you going to choose? Kubernetes is based on Google's experience of many years working with Linux containers. It has a number of abilities like mount persistent volumes that allows to move containers without music data. What this means is now you can effectively have some block storage. You add a block storage or file system to the container. The container moves down, but still, your block storage is somewhere else. It has a integrated load balancer. So with this, now you can actually move the load across different types of containers. It uses a framework called etcd for service discovery. I totally advise you to go and google that and see what it does. Kubernetes use a different CLI, a different API, a different YAML definitions, and it cannot use Docker CLI. So effectively, Kubernetes has its own way that you can access the container through the command line interface. Kubernetes has the notion of pods. So what is a pod? Pod is a collection of containers that are co-scheduled. Pods are groups of containers that are deployed and scheduled together. A pod will typically include about 1-5 containers which work together as a service. Kubernetes will run other containers to provide logging and monitoring services. Pods are treated as ephemeral in Kubernetes framework. Let's now discuss about services and replication controllers. These are two other elements of Kubernetes. Services are stable endpoints that can be addressed by a name. Services can be connected to pods by using label selectors. For example, this is a database container, this is the Cassandra container, and so forth. The service will automatically round-robin requests between the pods. It also uses what we call replication controllers. Replication controllers are used to instantiate pods. So effectively, if we want to set up a specific set of containers, as we said, a pod, then that's what a replication controller does. Replication controllers control and monitor the number of running pods. They call them replicas for a service. So effectively, we create these replicas of services across the board so that people can access these pods, these set of containers. So it makes it easy to effectively load-balance. Kubernetes has a flat networking space. Containers within a pod share an IP address, but the address space is flat across all pods. All pods can talk to each other without any network address translation. This makes multi-host clusters much more easy to balance at the cost of not supporting links and supporting single host networking a little tricky. As containers in the same pod share an IP, they can communicate by using ports on the local host address. So the beautiful about that is that you don't really need to think about IP addresses translation. Within the pod, you have the same, you have one IP, which means that you can just access each of the containers with pod address. Labels are key value pairs attached to objects in containers, primarily pods, used to describe identifying characteristics of the object, like the version, the tier, the frontend. Labels are not normally unique. They're expected to identify groups of containers. Label selectors can then be used to identify objects or groups of objects. For example, all the pods in the frontend tier with environment set to production. So if you think about that, if you want to deploy a set of pods together, in production, what you do is you create these labels. So if you have 100 pods that you want to deploy or you want to scale to one million pods, all of a sudden, you can use this label to do that. So that makes it easy to effectively balance the number of containers within the pod or even the pods themselves. Another system is called Docker Swarm. Docker Swarm was developed by Docker. Docker is a company, so they have their own framework. It exposes standard Docker API, meaning that the territory that you use to communicate with Docker, like Docker CLI, Docker-compose, Dokku, Krane. These are all tools within the Docker ecosystem. They can work with Docker Swarm. Of course, they're bound by the limitations of Docker API. So if you think about a Docker Swarm, you can think about having this Docker containers and then you have a Swarm, which is a set of containers coming together. Then you have the CLI, Docker CLI, that you could access also the Swarm as similar way that you are accessing the Docker containers. So in architecture, each host runs a Swarm agent and one host runs a Swarm manager. On small test clusters, this host may also run an agent. The manager is responsive for the orchestration and scheduling of containers on the host. It's obvious. Swarm can be used in a highly available mode where one of etcd, Consul, or Zookeeper is used to handle fail-over to a backup manager. So Consul in Kubernetes, they're different frameworks that you can find online. There are different methods for how host are found and added to a cluster, which is known as discovery in Swarm. By default, token-based discovery is used where the addresses of hosts are kept in a list stored on the Docker Hub.