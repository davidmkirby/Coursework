 >> At the final, finalize the discussion about containers with the pros and the cons, and what should you do and what you should not do when they use containers. Containers are immutable. That is, the operating system, library versions, configurations, folders, and applications are all wrapped inside a container. That means that you have a full object or a full entity of what you can write. This guarantees that the same image that was tested in quality assurance would reach the production environment with the same behavior. This is very important, because you can start building early integrations tests for your environment before it gets out the production. Containers are lightweight. That means that the memory that a container takes to be speedup is very small. That keeps the resources utilization at very low amount. So instead of hundreds or thousands of megabytes, the container will only allocate the memory for the main process. Finally, containers are fast. You can start the container as fast as a Linux process takes to start. Instead of minutes, you can start a new container in a few seconds. Instead of minutes, I mean to start virtual machines which takes 50 minutes, 10 minutes, five minutes, and so forth. Container can start in a few seconds. However, many users are still treating container just like typical virtual machines, and forget that containers have one important characteristic, containers are disposable. So when you try to persist data within a container, data may be lost. So let's see now, based on these characteristics of the containers and the properties of themselves, what are the things that one should avoid? One should not persist data in containers. Container just can be stopped, destroyed, and replaced. Which means that, if you start adding data in the container itself, the data will be lost. The only way you can actually write a container with data is to attach a file system to that or attach a block slots device to them. You do not ship the application in pieces. You must ship the application in its totality. One must not create large images. The problem with large images is that it is very hard to distribute to other people. For example, if you're not a yum update which updates all the packages within the Linux operating system, that downloads a new image and becomes large. Do not use a single layer image. To make an effective layered file system, always create your base image for your operating system and then add another layer for the usual definition, then another layer for the runtime installation, and another for the configuration, and another layer for the application. Now, this makes easy to recreate, manage and distribute the image. You can actually include those structures on how you build a container within the Dockerfile. Do not create images from running containers. Like, do not use docker commit to create an image. This method is not reproducible. So which means that you should actually build a container that may commit an already running container. Let's now discuss a little more information about docker. There are different terminologies or how persisted snapshots that can be run. So when you do images, you list all the images. When you do docker run, you create a container from an image and then execute command in it. When you do docker tag, what you do is you tag an image. You do docker pull, is you download that image from the absolute repository. In docker rmi, you're deleting a local image. These are the very basic commands within docker that makes your life much easier. When you run an instance of an image, you do docker ps. What you see is you list all the containers that you have in your operating system. Docker ps minus a, you list all containers including the ones that have been stopped. Docker top is you display processes of a container. Start, you start a stopped container. Stop, you stop a running container. Pause, is you pause all process within the container. Rm is you delete a container. Commit is when you create an image from a container itself. So if we do docker run minus it ubuntu /bin/bash, effectively you run an Ubuntu container. You make some modifications. We can create a new image with the modifications, and then we can run it as a deamon, docker run minus d, and then when the docker logs to figure out whether it's running or not. Dockerfile is docker can build images automatically by reading some sort of instructions from a Dockerfile. Dockerfile is more like a sequence of commands that you include in a file, that docker could use to say, "I'm going to spin up a container with these specific instructions." Dockerfile is a text document that contains all the commands that users could call on the command line to assemble an image. Using docker build, a user can create an automated build that executes several command-line instructions in succession. For example, let's say that you want to install a package in your container. Then what you do is you go and say, install. You say, start a container, install the package, stop the container, and then you can commit the container after. Can be versioned in a version control system like Git or SVN, along with all dependencies. Which means that you can add your Dockerfile with your Git repository in your GitHub account so that when somebody wants to download and to build something, they effectively build it through the instructions of the Dockerfile. There's also Docker Hub, which effectively can automatically build images based on Dockerfiles or GitHub. So Docker Hub is a public repository of Docker images. You can find out many of those. You can find somebody who have published. It's automated, and has been automatically built from Dockerfile. Source for build is available on GitHub. These are very interesting links that I highly advise somebody to read, and these are some of the references that I have also used to build some of these slides. Thank you.