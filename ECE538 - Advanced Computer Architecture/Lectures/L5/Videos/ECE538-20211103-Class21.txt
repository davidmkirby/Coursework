 Okay, Good afternoon, everyone. This is the class, of course, ECE 5 through 8 and the University of New Mexico. So last class, we ended here. Yet in the previous lecture about that instruction level parallelism and also as a planning operations including, Yeah, there are some approaches including parallel executions per plant operations, as well as the types of the heptane has that. Yeah, these hazards are bad, but they might happen during the pipelining executions, including the structure or has that the data hazard as well as the control hazard, or we call it the branch hazard, right? So we have shown the reasons why this has happened, ends up happening and also the approaches to solve the hazards are detected as a hot thoughts, like storing all the forewarning to solve the hazards. So in perverse classes is we have learned that the implementations, yeah, and also the exploration of the instruction level parallelism, as well as though we're just saying that this issues, right? Pipeline hazards. Actually besides that, we also need to be aware of the limitations of the one way deployed the instruction level parallelism execution. So when we're exploring the instruction level parallelism to increase the computing per fathers begin with the first permanent crosses are actually that was the 1980s, late 60s. So after that, in the 1980s and needs around the 1990s, these techniques, what the, hey, you know, the, the instruction level parallelism approach, right? This kind of techniques were the hay to a shift. You mean the rapid performance improvement in the computing system. So as we've shown before, that there will be, the question was, how much the instruction level parallelism exist that was created go to our long-term ability to enhance the performance and a rate that exceeds the increase in speed of the base and greatest technology. So unless shoulder scale the critical question, what is needed to explore more instruction level parallelism? This is very crucial, crucial to boast or computer designers and also the compiler writers. So that data in this study can provide us with a way to exempt the venue and also at the ideas that we have introduced in the lecture, right? And enough, sorry, we will show more about the, yeah, actually in previous lecture we have Show more about the holidays are deployed on the computing system by using the instruction level parallelism techniques. So as we will say, no, the hardware modals agenda is high level. Those that might have reasonable cause to write as though it is unlikely that the cost of variable receives regulations can be justified. This maze, the efficiencies of base hardwares in, for example, in the power, in the use of the silicon. Actually, they are simply too high. So many in the research community and also the major processor manufacturers they choose to use. Or they were always bedding them In favor of much greater exploitable instruction-level parallelism. You know, yeah, instead of the hardware modules and you're not also this process were manufactured. They were initially reluctant to establish this possibility to develop the stuff who were opposed his legs are preparing in the paradine them. So yeah, by 2005, there were funds that you hit their minds like to do at the hardware store. Well, what could the thigh, right? So therefore, we look to enqueues the limitations on the instruction level parallelism for improving the performance of that. You know, that relies upon processors. For example. There's limitations including the following list in 3. Yeah, this adjuster at those three amount all done limitations out the instruction level parallelism. Yeah, it's better for the parallel arable processors. So limitations in routing, the first one about the clock rate, yeah. As what the logic complexity and those are the power. So you might own that up. 64 instruction issues per kg. Who is no issue? Restrictions? In our more than TEN comes off the total issue Y of the Y it is to process or in 2011. So as we discussed before, the partial implementations of the Thoreau what asia ways that the design wise on the right, the logic, complexity and also the power, maybe the most important limitations on the exploit exploring the instruction level parallelism. Them. Also, in addition to the weight or the logic complexity and the power, you know, there are some limitations due to the small issue rate. And also the small relax us more size of the buffers for the load operations and also the store operations. So these AMD integration of the memory reference can be done. Typically though they ceased embrasures about it, perhaps a tenable for small window sizes. And this will hinge without aids, more issue rate. And also the load store buffers or suit or dress. Nice and prediction. So that will be done limitation to further exploit the instruction level parallelism. Yen. There is a certain limitation here. Yeah, that is, the longer latency it will have in a might have. So as weighty starts to also the approach of the radiator Room relay me, like with the commonly used as 64 additional integer in additional functional processor registers. So this is the slightly less than the most aggressive processor in 2011. Actually a lot that manufactured manufacturer's. They have legs or Intel cost seven. Yeah, this processor has one. Hungary tended to interests in reorder buffer. Yeah, Also they are not just beneath between the integer in the flow point where I'm going up on the main, the main time that the processor by AB, AB, I'm ABM. Yeah. They have the power Stefan. It has almost a 200 instead of the two hundred, one hundred and twenty eight interests than the actual. This is much more than the Intel Core, right? So we need to note that long assume pipeline latency on for one cycle, right? I significantly, they can reduce the need for lead to reorder buffer entries in as way as when he's got that both of the entire power seven power, that power stems from IBM and also the Intel Core I7. They have latencies of the ten cycles, all it will be greater. So this might be the limitations on the instruction-level parallelism for us to further improve the performance on the processors. So based on all of this above nation, the instruction level parallelism and also the relative approaches. Y will be some next. Or Y will be a hint of the instruction level parallelism for the parallel arable processors. In the next step. In order to answer this question, actually for the ahead up the instruction level parallelism for the processors. Yeah, we can first go back to view the development of first to say, what is the trade-off that development and why can we do for the further exploration. So while I go back to the history of the development of that and that the processors or their instruction and what paradigms them used to in the processor. Actually, we can see that it almost as 2000 beginning. The focus on exploring the instruction level parallelism was at its peak. Now means it is. Why do you use, are all popularly used? Especially accomplish like the Intel. Also we mentioned IBM. Them we're about to introduce Atana. A high issue rages that equally scheduled processor that relies on very long instruction word. That is that the IDA mirror-like approach we mentioned before, a true wins the intensive compiler support in this processor. So some canned of the processors like so and IPS offer and also IBM processors. Yeah, this, this, we have talked before. They are these Dine ways that dynamically scheduled as speculative exclusion. And they were in their cellular generation and they have gotten wider in also can be faster. So the processor legs, Pentium 4, I try to use the speculative scattering. So the Pentium 4 had also being alone instead that year. Yeah, in a way, it is weighs seven functional units in our pet them more than 20. Stay deep. Yeah, the deep off the stage. That is Donald was 20. But, you know, there was storm clouds on the horizon, right? So after that, almost by 2005, Intel and all other major processor manufacturers, they had revamped their approaches and their burgers on the processor with multiple processor course. So definitely, you know, higher performance will be achieved. Their suit, the, you know, like the square level paralyze them rather than the conventional instruction level parallelism, right? And also the risk responsibility for using the processor efficiency. And edit will largely shifted from the how, which is a software programmer. So this ten was the most significant change in the processor architectures. Uh, yeah, actually compare with the early days of the PR planning and a instruction level parallelism. You know, now maybe more than 25 years earlier. So many researchers predicted that a major read tree retreat bind in the use of the instruction level parallelism. So they predicted that to issue superscalar processors in the larger numbers, of course, will be the future or actually would be the filter at that time as they predicted. So after finishing the content of the panning and also the paradigm, some flow, you know, exploring the instruction level parallelism in the previous lecture. Yeah, we know that it is highly unlikely that future processors, they will try to increase the whites of the issues significant GNA, right? Yeah. It is simply too efficient both from the viewpoint of the silicon utilization and also for the power efficiency that you do. This. So I feel can still remember. When we go back to the very beginning of the previous lecture, we have shown that, yes, three types of paradigms, them approaches including premieres, introduced the instruction level parallelism and also the other two types including data level parallelism in US where level parallelism, yeah. Always the technology development, meaning well, the focus has been moved beyond the instruction level parallelism as we've shown people. Yeah, she's, there are a lot of limitations for the instruction level parallelism in the computing systems. Yeah, Actually, for further performance improvement. So it has been moved to the focus on approaches to exclude the two and the other types lags, the Datalab will paralyze them and also that thread level parallelism, especially for the computing systems. Nowadays, we use the ways multiple processors or multiple cores, right? So as we measure it, in the introduction of the dusting more instruction, my work data streaming nowadays is IMD write. These architectures in also. There are various variations. Yeah, I still remember that a veteran or processors. Also the single instruction mind what data stream instructions then extensions that we caught the SIMD ISE, as well as the alpha amino with the graphics processing units or GPUs, right? So what I want to mention it that way, all we have the idea about this exam and D and also they're very variations here, right? So in the following classes, I'm sure we will go Yeah, to the lecture 5 and talk about these two types of parallelism, data level parallelism in thread level parallelism in a, how they deploy to all exploit in the current. Computing on patriarchs waste the same octane jury. And also there are variations, including the vector, SIMD, IAC and although GPU architectures. So let's begin. Yeah, as we said that thought that they don't have a power lines. Before that. We need first to know. There is a question for the single instruction and data architecture I mentioned before. Yeah, I have always been just how wide a set of applications wars. Yeah, they ha, they have significant data level paradigm, some an, a. In this lecture about the data level parallelism, we will introduce how they can be exploited. So they'll continuity in this lecture about that They data about POWER9 Server will cover the three variations of the SIMD. Like always say that the vector architectures, modern media is IMG's at Instructure cell extensions and also the GPUs. So let's go to the this three types of variations, versa, to have an overview here. Yes, You're a year. There. These are the three variations of the single instruction might about data architecture, SVM. So do you have a brief introduction about this? Architecture's the first variation? Yeah, that is a vector. Our teachers. They predate the author to buy more almost 30 years and amaze a sagely pipeline execution of many data operations on vector architectures. So this vector architectures, they are easier to understand and also is there to compiled to the, to the computing picture of the author is and be variations like the SIMD, ISE and also the GPUs. But they were considered too expensive for them. Microprocessors until recently. Yeah, do, do the design cost. So part of Data Exposed was in the transistors and also part off though Carson was in the cartel, does sufficient accurate though the REM batteries yeah. One giving the widespread and red is on the caches to meet the memory performance demands on the conventional microprocessors. So they will, they will consider too expensive to this part of the cars in the ventral architectures. So for the center one, SIMD vibration about the my videos SIMD instruction set extensions, right? So this vibration borrows the SIMD name to me. Basically the Montagnards, they parallel operations. And eight is the funding The amongst the instruction set architectures today at Yad that it can support the multimedia applications, right? So this is a communist singing the current procedure architectures, like the X86 architectures. As though etching the X86 architectures, the SIMD instruction a station started ways the multimedia extensions in around 1996. So it will actually, this processors were followed by several ss E, that's the streaming SIMD extensions and aids. The next decades after 1996. This piano variation continued to this day, waste other bonds to vector extensions compared with the vector textures. So in order to get the highest the computational way to, from the existing X86 computer in around 1996. Yeah. The ones often need to use those SIMD instructions, especially I thought the floating point programs, right? So there are, there is another variation on the SIMD. They serve GPU that the CRT Haida about the GPU columns from the GPO community, yay to count, offer higher potential performers. Actually compare with that funding the traditional multi-core computers today. So well as you might know that you've used, you share features with the vector our teachers. They have their own distinguishing characteristics. Yeah, in part due to the, you know, the ecosystem humming which they involved, right? So the environment has no Does system processor in those days to memory in addition to the GPU and also its graphics memory. So in fact, rid of the nice, those tensions. Agenda, GPO community refers to this type of architecture as though heterogeneous. So nice. The extreme, the FAB might've bought the computing processors by USDA heterogeneous GPUs or the heterogeneous architecture with the, both the combination of CPU and also the unbalanced architecture like the GPU here. When we mention the heterogeneous systems. So for this one will show a lot of details about them. These three types of variations from the SIMD and architectures. So a summary, this is IMB architectures can provide the power nap or that data operations, right? So that is why do we want to explore about the data level paradigms them? Yeah, in order to improve the computing performance. All we can say that in order to further improve the computing performance in addition to the instruction level power lines of a way we discussed in the previous slide, previous lecture. So yet before going into the details of them, yeah, sweet half of the variations. I feel we would like to first review the power lines them approaches and square mentioning that Premiers lecture. So in last lecture we have discussed the first one demonstrated here about the instruction level parallelism, right? So the instruction level parallel lines that I'm actually, it is based on the on-off order execution right? In the instruction level paralyze the main execution. Actually multiple independent instructions identified. And these independent instructions can be grouped into the process and to be executed concurrently in different functional units, right? Even in a single processor, right? So therefore, the cycles per instruction can be reduced or to be less than one ray as we calculated before. So based on this operations by using the instruction level parallelism. Instruction level parallelism is deployed in the superscalar processor and also the very long instruction word processors to two, execute several instructions, several independent instructions simultaneously to improve or to decrease da, da, da, da cycles per instruction into improve the throughput. So that is why Mayor of that performance, right? So that is what the instruction level parallelism can do, as we discussed before. So in addition to the instruction level timelines them and yet there are also author lab, most of the parent einsum legs that data level parallelism and also the through our level paradigms them as we've shown here. I showed that for the US for a level paradigms, um, uh, we'll also call it then the task level parallelism, right? So does thread level parallelism. Yeah, it is implemented in multicore, all multipole processor systems. Yeah, you know, yeah, it is easy to understand that this computing systems with multicore or money processors, right? They can provide the hardwares. And that's how we can enable multiple threads, right? As we mined for amino wisdom, Marty, spreading. So several independence read all we can quote several independent tasks. Yeah, they can be executed simultaneously. Yeah, easiest way. It can reduce the total execution time by executing the task at the end the same time, right? For nasa, Wow, about data level parallelism as shown here. Yeah. Actually, the same operation is performed on the amount about data venues concavity in multiple processing units. One way uses a data, parallel lines them during the execution. Yet in order to reduce the instruction count. Therefore we can E has the system performance, right? So these are the processors, all the data processor architectures where the soil level them. It also has a Beta level paradigms and can be used in how they can be used to improve the performance in different ways. Like to execute the multiple task is or concurrent. Concurrently doing that. That they perform the multiple data venues, a multiple proceedings to improve the performance, right? Actually in different ways at different stages, right. So again, Yeah, to have a quick overview or the comparision between these three types of the power lines. I'm approaches here, as we discussed before. The instruction level parallelism or can, you know, overlaps the processing stages of the different instructions. Therefore, it can decrease the CPI. Yeah, I wish you guys the cycles per instruction, right? In order to increase the instructions per cycle. So that is why it can shift the better performance. Well know, there are some fundamental problems. One way, they implement the instruction level parallelism. Yeah, oh, I audition to the limitations as we showed before, right? So for instance, because you are already know that the instruction level power lines, some of the other programs, that it tends to be relatively low because it added more Exclusion Unit 2 o sugar scholar or to the the, you know, the out of the order processors. Yeah, in order to give diminishing returns. This has come operations during the operation. So another problem is that the memory is much slower. Yeah, When we do in the instruction level parallelism actuate actually not with the instruction level parallelism. Basically, the computing system. We already know that gap. That is the performance between the memory and also the procedure. And it will already know that that is the problem about da. Da, da, da. Memory is much slower than the processor, right? So due to this reason, most of load operations in also the most out of the store operations are conducted to a smaller memory. And also that the foster memory, right? Just smarter and also faster memory. We caught the cash, right? So however, one of the instructions, all that data, they are not available in the cache actually had the procedure may store for more clock cycles. This will definitely decrease in performance out the operations, right? So and this time it will retry the inflammation from the main memory yeah, that we call the cash meets. It will take more time Yan, decrease in performance. So definitely way disgust are the predators classes, operations will increase the data hazards, right? This is one that there might be one types off the Hazare do ORing the word, the introduction about the data. Hi guys, are right after raid, raid author rating also read after write, write. So this data hard, That's our band, right? So a structural level paradigms them. Yeah, it will also result in higher eyepiece, the instructions per cycle. And eta will in turn slowdowns or COGS. One way doing the executions is a computing system. So they sought them scruple want to state the limitations, although these are the one they use of the instruction level parallelism. And a, we can also still remembered than limitations as shown. In the very beginning of this lecture about the limitations only during the instruction level parallelism. So besides that, one way comparing waste thread level parallelism, as you know, the thread level parallelism, it needs to be implemented in the legs, multiprocessor systems or on the computer architectures with a hardware where the hardware can support the Manchus read. So any, you'll also note that the data level parallel lines them arises course there are many data I terms and these data items can be, I'll read it and the same time, right? By different task all threads. So data level parallelism is used for single operation. Repeat it. What data elements. Actually, you know, it is less general than the instruction level parallelism CTO anode, the parallel instructions aren't the same operation. So aim the following. We'll explore that data level parallelism, sure, dark variations of the single instruction, multiple data. That is, SIMD architectures, including done. Yet we're mention about the vector architecture. Multimedia single instruction might about data instructions that exchanges and also the graphics processing units to show that data more parallel lines. So always explore that data level parallelism way chauffeur. So no da, da, da, da classifications of them. Computing models. Yeah, actually in the predators classes we have discussed the different techniques for exploring the instruction level parallelism, next level parallelism. And before Target does single instruction multiple data architecture. Yeah, we need to know the following four classifications here. Actually, it is also called the flames classification here. Yeah, As the name shown here, actually, Michael Flynn started the parallel computing and make a lot of efforts in the 1960s. He looked and paralyze them in the instruction and also the data stream. Then pay phone or classification to divide the computers into the four major groups that demonstrated here. So they include the a single instruction stream, single data stream that is as ISD here. And the signal one, the single instruction multiple data stream is IMD, as we mentioned before. Third law might be multiple instructions dream single data stream, that is the AMI SD. And the last one is MIMD about the multiple instruction stream, multiple data stream. So he'll make more explanation about these four classifications here. Actually offload that single instruction stream, single data stream. They state that very traditional Von Neumann single CPU computer. Yeah, you have already have the view about doubtful no single step your computer as ratio in the first two classes, right? So this one, the traditional Voronoi, missing both CPU computer and a is a waste of conventional sequential processor. That is to say this category is ISD. It is the union processor. Yeah, we have already demonstrated what is the union procedure in previous class tomb. So the programmer thinks of it as the standard sequential computer. But the SIMD architecture can exploit the instruction level parallelism. So it is not suitable to realize that data level parallelism because it's a single or a structure and a single data stream for the sequential procession. So for the second one does single instruction multiple data stream. These are, this type of architecture is a vector processor with fine-grained data parallelism in the computing. So the same instruction is executed by multiple processors using different data streams. So. A single instruction, multiple data stream computers can exploit that data label paradise them, achieved by applying the same operations to a medical AI terms of data at exactly empowered out. So each processor has its own data memory. Hence, you will no doubt that the memory device, all the memory disk or the SIMD, step by. You note that there is a single instruction, memory and control processor, and they can fetch and dispatch the instructions. And then for the third one, the multiple instruction single data string, maybe the da, da, da, the type of the peptide and computers. And eight has no commercial programmable systems. And the last one about the multiple instruction multiple data stream. This type of Dell computing architecture is too much CPU processor system. And this system can Asia Madhu instructions in your computer and also has a multiple instruction, multiple data stream architecture can support that. They dilemma paralyze them too, as well as the instruction level parallelism Rice's, they have multiple instruction and also audible data stream. So each processor can fetch its own instruction and each processor can operate on its own data. Yeah, if Tom is task level paradigms him. Yeah. Anatomies. The plug itself, the multiple instruction, multiple data stream architecture is about to add two to realize thought possible and what paradigms him. So in general, multiple data, multiple instruction might have with data.frame architecture is more flexible than the single instruction my data and more generate a plausible that. But, you know, the structure might have what they dance dream is inherently more expensive than the single instruction, multiple data stream. That is why even it against the plot, those instruction level parallelism. And they don't have a paradigms him. And he's not so popular why they are used as that was used by the single instruction multiple data architecture. So for example, you know, multiple and unstructured data stream architecture. All computers can't also explored the data that will paradigms them. Also, the overhead is likely to be higher. And that will be saying the single instruction, right? What data computer? So that's why we will focus on the single instruction multiple data stream architecture and also their variations. Of course they add the title of the OCT teachers might be the best all the, be better than Azar series as shown here. So we know the basic architectures that we can deploy, that data level parallelism on SIMD and also variations, right? So they don't level paradigms. What is that data level? Pyridine, some nightshade, you know, yet they are then we'll power 9. Sum is calmer in scientific computing, right? You break the LFO paradigms and can be used to execute the same code on a large number of the objects. They are parallel lines. The dates exist across multiple processors in the parallel computing environments in EDU courses on the distributing the data across different nodes. And it can't operate on the data in parallel. So the data I have a parallel, parallelism can be applied on the Red Road data structures like the Aires analysis. A matrics is actuated by working on each adamant in parallel. So it can't trust of two task level parallelism as another form of dog, a parallel line XML and we were shown layer. So yeah, this is the MD architectures can, yeah, explored the significant data level parallelism for not only the match it's oriented scientific computing, but also for the, you know, the media oriented image and sound processing. So they are very popular these days above, from the real applications, right? The media oriented image and the sound processing, right? Even for, is better for the applications nowadays. So additionally, SIMD is more efficient than the MIMD or multiple instruction, multiple data stream architecture as promotion in the previous slides, right? So ask me to fetch only one instruction per data operations in the SIMD to explore that data parallelism ONE. Yeah, in order to reduce the cost of the reduced power, right? So mace actually makes the single instruction, multiple data frame architecture Attractive. Yeah, especially for the personal mobile devices. Yet with the applications today. So there are basically three variations of the SIMD. Yeah, as we mentioned before, they are vector processors, SIMD extensions, and also the GPUs, right? They are popularly used. The income per factors are light gray machines, intel MMX, and also NVM respectively, right? So the, the data level parallelism is good for the fall parallel workloads. So as showing this figure here, it is a design of a single instruction mind what they asked during hear me. Yeah. It also aims to improve the throughput. The rather stands that Lindsey, for these kind of architectures here, actually you can fearful or they just stayed the difference between the single instruction, single data here. Because this can of architectures with a single instruction multiple data can execute the data operations parallel with multiple operations here, right? So in the rest of the class or in the rest of this lecture, we have the objective to discuss about how data level parallelism can be exploited in the processors, especially for the SIMD in computing architectures. Yeah, we'll, we'll show all. We will discuss about the vector art teachers, SIMD instruction, instructions, and also the GPU architectures with their realizations are always there. Yeah, that rod vision for the applications and how they can be used that to explored the Beta label paradigms them. As well as ways. We'll be sure. We'll show you some real examples about these channels. Designs I bought the computing architectures. So I'm only talking about the data, about power lines. The main thing going structure might have one data stream on pictures. But they don't ever paralyze them plays an important role in the execution of that single instruction, multiple data stream architectures. But the idea is to reduce the instruction processing overhead and to never rigid the fact that the certain applications, actually some certain applications, they need to do the same operation or a large amount of data. Yeah, you might be familiar with the machine learning applications, all of the AI applications nowadays way so the large amount of data at data operations on the data processing right there, we'll use that during the same operation for a large amount of data. That data intensive applications here. So for instance, there are hands off the representative Operations, batch, and the processing in the graphics processing units, right? For the, for the workload off the audio encoding and decoding. In other operations of that, faith takes simulations and also, you know, the processing of the deep neural networks. Yeah, there might be others with data intensive computing. So processing in these pieces, right? They need the data level parallelism to improve the performance. It's not. There will be plenty of inner loops in the inner loops operations in these kind of operations with the same data. So basing the loop level parallelism can be deployed. Therefore, the iterations can be performed in parallel, right? Yeah, especially by using the data level paradigms them in this operations here. So for the single instruction multiple data as James architectures or the computing machines, the computing architectures. There are basically three types out. Yeah, the first one as the SIMD instruction set extinction, we call it the SIMD ISE. Etch it this can of extensions. And all for the traditional processors. And this center line is the vector processor. They can be, the vector processors can be really just a dedicated for the SIMD processors. And the third law about the GPUs, they have larger lumber off the SIMD processors. So semester we're going to flip the instruction set extensions called the I am the ISAs. They are of the SIMD. So they are primarily be included into Intel and ARM processors for multimedia, right? Simd IS, is actually, they are extensions and they can support as Intel and AMD X86 instructions that actually for the parallel operations on the path to integer or floating point data. This is provided for achieving the data paradigms them. Yeah, I tried to achieve the parallel operations based on the vectors by applying the same operation in parallel on number of data items packed into length of 84 to 128 or even 256 beta vector. So the data will all be packed into the size of the dot, dot the vector. So SIMD also support those scholar operations are integer or floating point values. Though. As we already know that data level parallelism is a way to perform the, you know, the parent. Now it's Qj execution of an application on the multiple processors write. It focuses on distributing that data across different nodes in the parallel execution environment. So it also enables them Antanas, compute accurate though that does stop computations on this distributed data across the different compute nodes. But that is one way to explore the data and willpower nicer. Yeah, that is to use the vectors. As we mention. Actually know the matters as the vector architectures. This is the oldest of the SIMD style of the, yeah, the SIMD are teachers and they are widely used in the shoe recompute of those today, in those days, in that period. So this vector computing architectures were considered too expensive to be implemented in the microprocessors. The course of the number of transistors. And they, I were crowded. And the memory bandwidth was also repaired, right? However, you know, it is no longer so I share the vector architecture, speak basic, operate on the vectors of the data as weighs 64, big 128-bit, all 256 feet, right? They gather the data, that is data across multiple memory locations into one large vector we gesture, Yeah, and also they operate on the data elements independently. And then they can store the data back in the respective memory locations. Yeah, I've seen that you can find that it is really effective, right? For the operations about the beta axis and beta written back to memory. So this lots reaches for files by using the vectors. They are controlled by the complier. And they I used to hide the memory latency and to leverage the memory better ways, even with the stem re, resources provided by the computing architecture. So the base is the one way to use vectors to improve the performance of the memory and tube. Yeah, as we know, as we mentioned before, that is one way to reduce the gap between memory performance yet the computing performance, right? So sometimes the February of vector as a one dimensional array of the numbers. In many scientific and commercial program. Osteo, you would think the vectors as the example shown here about the larger examples here. Actually, you can say in this example that some operations can be applied to multiple data elements. And extend processor with vector data type. Also we call it the vector processor. Yeah, or we call it the vector instructions. Set architecture extensions. Actually a vector processor is one of those whose instruction operate under vectors rather than their scholar. As why we call it the single data value. So the vectors can use multiple data venue so that you're processing. Our teachers are now considered as separate form, the single instruction mind what data computers. It is based on the fact that, that the vector computers per stead, the vectors 1 Ward and two times through the PEC and processors, even though it is still based on the single instruction, right? So whereas the mode is single instruction, multiple data computers can pursue this at all elements of the vector simultaneously. So in the vector processors, as shown in this figure to the ventral, if the arrow with the maximum vector lens of them, you know, does 32 bit floating point numbers. And each vector register file has the eight to 16 vector registers. So all data passes will execute the same instruction here. Among this, each data has, has its own local storage, right? This logo storages is also called the desktop via Zoom here. So the memory access with a vector node's operations and also the store operations can through the wide memory and part. By using this swedish requires provided by this vector based computing architecture, right? So after we load vectors, the operations in the, uh, one example of the vector of features, a true, we need to think about that. Why the computer designers use a bachelor's in R. We can say why we use vectors to exploit the data level paradigm exams. So first, the answer could be that the vector instructions actual can't know that deeper peptides, these are the instructions there. No. Into a vector, into logs or into a vector have that? Yeah. As we mentioned before, the hazard or the interlocks would be bad for the performance when we're doing the planning or the paradigms them, right? So the inner loop control hazards and mandated by using the vector computing architecture or by using the vector instructions. Here. They say there is no need to issue multiple instructions in the vector based computing architecture or the vector instructions. So in this way, it can prevent the execution of resource conflict, right? This is not our hand to solve that. And the performance lose during execution. It could be the significant improvement here by resolving the resource conflict. So in addition, the vectors can also present as a memory access pattern to the hardware. Yeah, actually, it is much efficient, ways lower latency. So on the other hand, we need to know that when we are trying the legs are deeper PR plan. There might be some problems. For instance, you can find that achieve that into knock. The inter love logic. It's hard to be divided into more status. And therefore the Up headland performance had been decrease. Right? Beside that, we can still remember that there might be data had that during the planning, right? And even the numbers are actually, we can see that the number of the Bible's we need to install or to add a, due to data hazards. Actually, it will increase right now off the bubbles well, in craze due to data hazards. So this will result in difficulty in issuing multiple instructions per cycle when we do end up happening executions. Why was that the operation status flags to fetch the instruction, fetch data or issued instruction days peptides days will be the performance bottleneck. Yeah, actually, this is called the flame bottleneck, the same name as the classification of dark pictures. Now it's a flame bottleneck. So one when meeting this. Bottlenecks and the performance or the throughput of the planning or the paradigms of a well be significantly decreased. So let's look into the details of the vector processors after we know why we use vector processors in one example to show the logic of the vector processors, right? So some massively in computing a vector processor, all we caught that we can still put the, the, every processor. It is those thing to a processing unit. We're caught the alpha amino is CPU, right? That can implement instruction set containing the instructions. And these instructions can operate on one dimensional arrays of the data called the vectors. As well. They can arrange the data in one large vector instead off different areas, right? So it is compared with a scholar processors. And a scholar processors, the instruction operate on a single data item and there might be several data items. So the vector technique was first fully exploited around 1960 and 76 bytes famers, K31 and not anomers. So actually the racecar five architecture. Now it has created a vector processor extension to instruction set architecture, which is called the RV six step four bay. So also the Intel processors with their AV x2, they are pretty much vector processors nowadays, why they used in the computing architectures. So in general terms, those CPUs are able to manipulate one or two pieces of data and on time, right? You know. So for instance, most CPUs have an instruction that essentially stays like to, to, to, to, to do the exam will show in this fewer dye is to add the value in R1 and R2 and put the result in the NIH R3, right? So the data for R1, R2, R3 could be actually in theory and needs to encode it directly into the instruction, right? However, an efficient implementation scenes are rarely does symbol. So the data is released and a row form, and it is down pointed. You're parsing in our dress to a memory location that holds the data. So decoding this a dress and getting the data all of the memory. Yeah, and this process takes sometime during this time, the CPU traditionally fit. I don't buy waiting for the requested data, right? To show up and to be ready to do the calculation. So as the CBO speeds have increased, this memory latency has historically become large impediment to performance, and it will definitely decrease performance of the Intel computing performance, right? Yeah, J, we are familiar with this issue and that is called the mammary or to say, the performance is bad, but between memory and also the computing RAM and CPU. So on. In order to reduce the amount of the time consumed by these steps as virtual for that data access from the memory read. Most of modern CPUs use a technique known as the instruction. Happening. In this approach, the instructions pass through several subunits in term. And the first subunits can read the address and decode it. Then the next fetch, the instructions or the values at those dress. And then the next it does the match itself. So with pipelines or trade a two-star decoding the next instruction. If I'm before the first, it has left of the CPU, the value of an accident nine. So the address decoder is consistent in US, right? So this may be the example of the simple vector processors here. Yet to do. They don't have apparent 900 by how to arrange the data in the vector registers. And there may be part of the scholarly gestures here in the computing system. So, so in this class we have briefly introduced at the data level parallelism in how they deliver paradigms them can be used in computing architectures is better. I thought that single instruction, multiple data stream architectures. And there are three main types of variations, including the SIMD IS, is that ADA In excitations of the SIMD and also the vectors in the GPOs, right? We have already a nerve, da, da, da, da. Some examples about the vectors in y, we will use the vectors. And next class we will continue to show more details about the realization of the vectors when they are used or to exploit the data level parallelism in the computing systems. And the way we followed up, the follow up by the introduction about the SIMD ISA's and also the GPUs later. So thank you so much for, for having this class and thank you. This, this is the content of this class. Bye.