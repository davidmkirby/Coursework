 It's time to clasp round. So that's what a warm minute flaws or it's OK. Whoa. Boundaries and everyone's boom. Yeah. I very much appreciate your understanding both the last class. Thank you so much. In any questions before the cars while you're watching the recorded video? Yeah. You can let me know if you have any questions about the previous crust. Okay, So, yeah, it's time to start a class in hub Azara work later in M recording for the class. Yeah. For them if they have been available today. Yeah. Good afternoon, everyone. This is the best ofcourse is a five through eight. The other ones, the computer architecture and by University of New Mexico coming. So yeah, Nasir class, as you watch in the record a video right? With her by starting the discussion of the data level parallelism, a new lecture. So first thing's way. Yeah, when as winner in the previous class, we would like to have for a quick review about this asteroid types of paradigms them approach first this crust. So as way these guys to be for j in last lecture, we'll talk about that instruction level parallelism. That is one of the paradigms or approaches. And the instruction level parallelism I'm actuated can't overlap the processing stages of different instructions, right? So therefore, you can decrease the cycles per instruction. And nowadays we put the CPI, one way to measure the performance of the computing system. So and that is the two in order to increase the instructions per cycle to you. So well, there are some fundamental prognosis away implement, you know, the instruction level parallelism. For instance, as we've shown before. Because you know the instruction level parallelism, actually, I don't know if they'll real progress. It tends to be relatively low for the instructions in the real life cushion because you'll know it add more execution units. I especially sugar scholar or two of the, of the order process or to give them a nation though returns in the exclusions off the line. So yeah, for other limitations about the instruction level parallel lines that I showed there exist another partner, it that the memory is much slower than the processor, as we mentioned. And that is that gap between your memory performance, computing performance. So most of the load operations and also the store operations to the memory, they are conducted two smaller and faster memory we call it the cash, right? However, when the instructions or the data are not available in the cache, when we want to access it, the procedure may store for more clock cycles. All right, So at this time either will retract the information from the main memory. Now, come they caught the academies, right? So that an agenda what as where he's close to incredulous classes. These operations will increase your data hotdogs as we showed before right now, one of the hotspots have on your wind up her planning paradigms and parallel computing. So the instruction level parallel lines, I'm actually also well read result in the Ohio instruction per instruction, instruction per cycle. And it will in turn, slow down. The clocks due to the legs are positive during the operation, right? So you said that when we compare the instruction paradigm some ways that, that, that the other types of parallel lines and legs, the thread level parallelism. Actually, that's your eye level paradigm per mole. We also call it the task level parallelism. So as you know, the thread level parallelism needs to be implemented in the system with multiple processors or on the hardware platform with they can support the monkeys threads. So you may also know that they don't have a parallel lines of as the third law, right. Arise this because there are many data items that can be operated at the same time. So you have a data level parallelism as used for a single operation, repeated multiple data items for this canada of operations, right? So it is the less general than the instruction level parallelism since you know the parallel instructions the same operations while we're doing the exclusions, right? So in the following way we'll explore that they all have a nice them through the types of variations of that. A single instruction, multiple data architectures, as we mentioned in a previous class. So these variations, including the Bachelor of Architecture, multimedia single instruction level. They'd asked during instruction set extension, we call SIMD ISE and also the server about GPUs, graphics processing units. We are more familiar with this one, right? So in the following way, we'll talk about SIMD and also its variations. To show you how to deploy that they don't have a paradise them in this kind of computing systems. So when we're talking about the data level paradigm, that, that first of all, we need to note that basic types of different techniques for exploring the data about paradigms and venture, as we showed in the previous lecture, we have already discussed the different techniques as for the exploit the instruction level parallelism and also thread level parallelism. So we shall now discuss different times of the architectures. Then we can use that to exploit that data level parallelism. For instance, that among based is I am the ETA. The style of the architecture says way we're taught. So actually, before we talk about the SIMD, we need to know the basic classification of the architectures, all of the stream, network computing most. And why we choose the SIMD for the data level parallelism and model, the other three, right? So as you can show, I have nice data here. There are basically three types of three classes. The classes of the computing model is dyes are core to the flames classification. Actually the micro lens diet, parallel computing and make some efforts in the 1960s. And hand nuclei and the paradigms of the instructions and also the Data Streams. So after that, he found a classification to do by the computers basically into four major groups. So these are four major boroughs Peggy by it. So including the single instruction, single data stream, our teacher, the single instruction multiple data stream in hours of instruction, sperm single day aspirin, lasso about that multiple instruction stream, multiple data stream. So from the name, you can have three or four words. Understanding about these four types of the architecture, right? For the first one, single instruction, single data stream. Yet it is the traditional varname, a single CPU come computer, actually, as was shown before, right? Alignments, single CPU computer. Yeah, it is The whereas the conventional sequential processor that it, you say. This era of computing architectures, yeah. They are that, they have that unit processor for the sequential processing. So the programmers single gate as standard a sequential computer, about eight can exploit the instruction level parallel lines or so. But it is not suitable to realize their data level parallelism because they cannot support that less than one, the single instruction, multiple data stream. Yet though, the one we will talk more that if SIMD is the actual vector processor was fine grained data parallelism into computing. So there is an execution. The same instruction should be executed by multiple processors by using different data streams. So single instruction, multiple data stream computers can exploit that data level parallel lines. I'm actually by applying the same operation to multiple items of the data in parallel. So each processor has its own data memory. Yeah, sometimes of them. Simd write about these days the single instruction memory and control processor. In a tent when you fetch, and this is patch the instructions. So for the third alone, the multiple instruction, single data stream at yeah, and maybe the, that the hana computers, it has no commercial programmer systems now, so far the last one about them, much more instruction multiple data from the two multiple processor system. And this time, the computers can issue multiple instructions in the computers. And the Middle East or multiple data stream computing can support the data level parallelism too much sugar powerful than single instruction my work data streaming as well. And it can also support that instruction level parallelism and also the data level parallelism. The MIMD. Each processor can fetch its own instruction and each processor upgrade its own data. And it can tie a task level parallelism to. So in general, has a lot of benefits competitive with the other three, right? However, in general, multiple instruction, multiple data stream, it is more flexible. That is the MD. Therefore, if it is more generally a place of war to the operations. However, you know, the MIMD is inherently a more expensive in their design yet, and also with much more complexity at Wanda, operate during the operation bend SIMD. So for example, you know, MIMD computers can also explode that they don't have apparent einsum. Although you'll know the overhead is likely to be higher, much higher than that will be in the SIMD computer. So that is to say due to the high cost to add them. Yeah, It is much more expressive therapy and also saw complexity of what the MIMD. So in regard to that though they don't have a power lines of this, SIMD would be the better choice to stop all SIMD. That's why we weren't sure SIMD and its variations exploit that data level parallelism instead of the MIMD here. So that is the reason why we will talk about the SIMD here. So theta level paradigms. Again, yeah, well, we know the basic computing system. And how did however heroin Amazon will be deployed, will be exploited and just computing architectures. Yeah, as we know, they don't have a camera. Nice them. It is commonly in the scientific computing and data level. Paradine zone can be used to execute in order. But the sim code on a large number of objects. So they don't have apparent lines are basically exist across multiple processors. In parallel computing environments either focuses on distributing that data across different nodes. Yeah, operate on the data in parallel. So it can be applied on the radar data structure like the arrays at the metrics by working on each element in parallel. So in contrast to task level parallelism as another form of the paradigms them, right? So it is a focus on the data itself. So single instruction, multiple data stream architectures, they can exploit the significant data label paradigms them. Yeah, Actually if will, not only the matrics orientated scientific computing, but also it can support the media iron to image and sound and processing. Yeah, actually at this point 10 applications at the, they have the high demand for the real EPA patients at a very popular these days, especially for the machine learning applications, right? With a lot of a major explanation needed, a voice processing. So data has a data intensive applications. So additionally, as you know, the single instruction, multiple data stream architectures, they more energy efficient as we were mentioning. As we compare with multiple instruction, multiple data architecture. Yeah, As we need to fetch only one instruction per data operation, right? So this or makes the SIMD more attractive. Yeah, for the personal about devices now, for the applications. So when we're talking about SIMD and also how the data level parent ions that can be exploited. We need to know that there are basically three variations of the single instruction, multiple data architecture. So, yeah, as you might know, I've mentioned several times in this lecture, they are the vector processors, way and SIMD extensions. The analysis allows for graphics processing units or GPUs, right? They are popularly used in the accompanying protectors, likes the gray machines, intel MMX, Also EV, right, respectively for the, for their demands in that different types of applications. So we need to say that they have a paradigms that is a bool for the parallel workloads. So as union of pillar, you can have understanding about that. It is designed for the single instruction multiple data architecture and aims to improve the throughput arises that the latency compared with a single instruction, single data stream here. So, and then the rest of this lecture, our objective is to discuss about how data level paradigms and can be exploiting the processors. So we shall discuss about the vectors is m, the exchanges and also the GPUs. Later classes. So yeah, again, show them what you're, to you about the idea of the data level. Parallelism in base can of architectures, right? Yeah, it plays an important role in the execution of single instruction multiple data system. The idea to deploy the data level pyridine them in this federation of textures. That is to reduce the instruction processing overhead and to leverage is affected that the certain application pseudo, they need to do the same operation on a large amount of data. So for instance, there are hands-off representative operations. Yeah, from applications such as processing in the graphics processing units or GPUs right there, our workload of the audio encoding and the audio decoding, and the operations of the physics simulations. And also the legs, the operations to process the Deep Neural Networks today, right? So for this can of operations without the representative operations, with the data potential to be used that day that I will paralyze them. So first essay in this cases or in this applications here, they need that data level parallelism in order to improve the performance. Say so no day. There will be plenty of inner loops operations in this applications. So simply as shown in this example for that, the loops here, the inner loops, paradigms and can be deployed and that's the iteration can be performed in parallel by using the data level parallelism here. Yeah, especially for the representative operational about the inter-group cooperation. So for the single instruction, multiple data stream machines, yeah, for these three types here. The first one is the single instructions, that extension, that is though I as E here for traditional processor and also the vector processors. They are really just a dedicated SIMD processors and Sutherland about the GPUs. Actually, GPUs have a larger number of the SIMD processors compare with the more traditional one about the vector processor sphere. Even they are all the variations of the SIMD machines. So you'll note that the instructions that extension that the SIMD I SEE of the single instruction, multiple data us to machines. They are primarily be included into the legs processors from that Intel and also the ARM processors for the multimedia. And the SIMD IRC is either excretions and they can use that to support the as the Intel and AMD X86 instruction set up for the parallel operations on the pact, an integer or floating point data. So based is the provider for achieving the data level paradigm sun and also for the parallel operations based on the vector. Actually they, they are conducted by applying the same operation in parallel on a lot of data items packed into Lake City for 64 or eight or even to 25056 beta vector. So as time they also supports you load the scholars operations on the integer or floating point values. Again, as we know, data on Evo power nine. So yeah, As we mentioned that it is a way to perform parallel execution of an application on multiple processors right now actually focuses on distributing the data across different nodes in the parallel execution environment. So it also enables simultaneous sub computations on this distributed data across different computing, compute nodes. So the one way to exploit, but the dilemma per night is to use the vectors. Now what do we call it? That? Yeah, when you code them as the vector architecture. So the vector architectures are the oldest of the SIMD style of the architectures. Yeah, they are widely used in the supercomputers of those days. They were considered too expensive to be implemented in the microprocessors are doing, and why. So the main reason that they are considered to be too expensive to be deployed, implemented aids a microprocessor that the number of transistors through crowd and the memory bandwidth repelled for this kind of architecture, it's running high, so high that it is no longer so and the vector architectures at you and me, they can basically operate on the vectors of the data. So they, the best practitioners can. Gases are data. Yeah, That is this battered across multiple memory locations into one larger vector register. And they can operate on that data elements actually independently. And then biggest or bends the data in the respective memory location. So they slides register files coming toward it by the complier. And this larger files used it, you'll hide the memory latency and thus to leverage the memory boundaries. So they, that's the benefits that can be provided by the vector processors by using a larger resistor FIOS way. So compiler based controller. So going to a vector processor as a one dimensional array of the numbers in many scientific and commercial programs use the VAX or processors now. So as a demo showing year some operations. Yeah, this operations can be applied to the much more data at elements here. So extend the processor with vector data type. Also we call it then the ventral courses. You're here 0, we call it the venture instruction set architecture. It's two inches. So a vector processor actually, it is a one. Yeah, as instructional operate on the vector rather than just got spanner actually had the scholar as you've used in the single data operation. So vector processing, our teachers are now consider to be separated from the SIMD computers based on the fact that value note the vector computers process that the vectors one word at a time through the peptide processor scheme. And yet still based on single instruction. Actually in this operations, The more than a single instruction, multiple data stream computers process all elements of the vector of similar Tanner state. So end of that process or SEO, this figure. The vector as an arrow is the maximum effectiveness of the 32 bit floating point numbers. So eight back to the register file has eight to 16 vector registers, or data passes will execute the same instruction. So among this, actually I stayed up, has, has its own local storage here. And this local storage is also called the register file here. So the memory access with the vector for the node operations and also the star operations can be true that wide memory for it by using this vectors. And a vector processors. So, yeah, well, I know the how the design of the vector processors actual with the register files, right? So why the computer designers use the factors? And we can see that the y, we use vectors to, to do the, the high-level paradigms them. So these are some reasons here. And I said that the vector instructions, a little deeper pipelines, say shown up in the vector instructions. There are no into a vector interlocks or into a vector hot dogs. Yeah, pathway shown before. The hazards are not good for the money and the paradigms and right. So the contour hazards and monadic theory, the operation. This is no need to issue multiple structures. Therefore, it can prevent the instruction, prevent the execution resource conflicts, which will raise the hazards, right? So I audition. Vectors can also present the memory access pattern to the hardware. So this will be a much efficient and with lower latency for the operations. Although on the other hand, we need to know that when we are trying to exploit the, you know, the deeper Pepin, there might be some problems as you know, faced us. Sometimes the term knock logic, it's hard to be divided into more status. Therefore, the pet, them performance has been decreased due to the interlope. So besides, we can still remember that during the operation, there might be the data. How about F we measure before? And even the lumber Bibles? Yeah, as thus, that might be the effective way to stop that data has that by inserting the bubbles, right? However, even the number of the Bible's increase due to data hazards increase, right? This will result in difficulty in issuing multiple instructions per cycle during the pipelining. So one ready was it that the operation stays like that? Fetch the instruction or to issue the instruction will be the performance bottleneck during the operation, April in MIT, the hotdog during the operation, right? Though, accurate, this is known as the flame bottleneck. So nasa main bottlenecks all the problems during the planning. So actually vectors have the better performance to deal with this operations. So after we've done the basic design of the processor and also how and why we chose vectors. Here. Let's then look into the details of Brexit processors for the operations here. Yeah. Specifically in computing the vector procedure, always call it that. Or we can call it the area processor is central processing units that commonly call as the CPU. And it can implement instruction set containing the instructions that can operate on one dimensional arrays of the data card. We'll call it the vectors. So compared with Scott processors to access the data in that one dimensional Eris as the vector. So you always this design, the instructions can operate under a single data item instead of the motherboards, right? So the vector technique was first fully exploited in the, around they 76 by the famers create one at two and most animals National Lab stop. In communist states, the legs, the risk of five-hour teacher. It has created a vector processor, a changing of the instruction and the instruction set architecture, we'll call our base-64 rate. That is the volume of that processor was the vector design. As well as the, as they used to in the Intel processors with x2. And they pray much better processors. So in general terms, those CPUs are able to, in order to manipulate a one or two pieces of the data at the same time, right? For instance, most are CPU's instruction that is dangerous. This too. Add R1 to R2 and then put the result would come back to Australia. So the data for the R1, R2, and R3, they can be inserted that I needed to encode it directly into the instruction. However, Ada, an efficient implementation. This is saying really that simple. So the data is released and in the real role flow. And it is instead of point 2, by passing in the address to a memory location that holds the data. So decoding this dress and then getting that data out of the memory. Yeah, process takes some time. Now we'll call it the delay. So during this delay, all during this process, the CPU traditionally will sit idle. And the CBO, we'll wait for the requested data to be ray at, to show up. It'll continue to perform the calculation, right? So as the stapedius bids have been accurate, as caving in crazy. And this memory latency has become a large impediment of the performers, as we mentioned, the memory. All right, so how we can access the data ate the most effective way to reduce the gap between the computing, nice to use the factor and to install the day. I love that talking yields a batch of processors here as well way here. So in order to reduce the amount of the time consumed by these steps, yeah, As we're measuring CPU, wait for the data to be accessed, to be a fine, right? So most of the modern CPUs use the techniques known as the instruction Pac-Man and the instruction pipeline. The instructions pass through several subunits in term. So the first to stop unions can read, address and decoded. And the next the faster the venue and don't suggest. And then the next does the mass itself bottom is the max upon the computation for the same time. So with the peppering but as two-star decoding the next instruction even. And it can be before the first has mapped to the CPU. Yeah, in the fascia of the assembly and NIH or further for NLP FOB points here, right? Yeah. So that happening, yeah, to start decoding the next instruction, if I'm before the first one has laughed and said, Yeah, imperative, right? So as that the example way show me about the car assembly of NIH, right? So that the address decoder is consistent. Use and adequate instruction takes the same amount of time to complete ten known as the latency. But the, you know, the six-year current process or Intel badge of the operations are actually much faster and more efficient in that. It gives so as at a time. So the vector processors can take this concept or one, actually, one step further. Yeah, can you go to, you know, the vector processor has a traditionalist got a processor with yeah, I was actually a larger vector reducer file. And this register file can support a varied number of the resistors and different sizes. So it has multiple functional units that can arrange that. Nice. So a validation of that processor have the dot, the vector, the load operations for the load instructions and also the store instructions, including the scanner instruction gathering instruction that can guess as valid data from the memory into a single vector. And then they can gather the roots are fed into the memory. So instead of happening just at the instructions, actually the vector processors can also append the data itself. So add. In the previous slides, right? The processor is fetching the instruction, say, not just to add the R1 to R2, to R2, but you add all the numbers from here to there, or two, or the numbers from here if you bear right? So a steno that consistent having to decode the instructions are done. Fats that they don't need it. You're confident in the processor now, reads are single instruction for the memory. And it is the same thing, right? The definition of the instruction itself that the instruction will operate a gain and nasa items of the data. So and a dress one increment moderate than their last days. The operations actually can't know for both of you needed saving in the decoding time. For might've operate multiple instruction that axis different data. I've seen so far the vector processors that you're a typical one. A typical vector processor has the following nice components me. So first available processor contains traditionalist, got a processor with a large vector register far as were sharp, sharp at the figure. And this was just a flat support, a very normal of the region starts at different sizes. So the vector register file is organized to ink or to the reeds yester, every edge of the region or the area of the elements. And each of this region experts who holds a vector yeah, for the predators 32 registers. Yeah, One of the design. And Asian reducer can't holds the 64 elements and 64 based elements vector. So in order to supply the data to the functional units and provide enough overlap for the operations. I'm sure there were just a fire have up to 16 rate falls and aid reports. These reports and the right ports are connected to the inputs or outputs of the functional units as to the cross boss wages in their design. So they can support that compound data operations for their load and for the ride. For the stroke. So seven, hello back the author, the actual functional units. So the span of the bachelor functional units actually includes the operating functional units for the lab, for the data types, including the integer point in those stores. So sometimes the bachelors and also gather units are combined. Design, such as the ALU, components in the computer system. So this function, uh, units, Sunni, a Pac-Man it during the operation and they are handled starting operation, average clock cycle. During the operation. There is the control unit. This control unit can detach the structure, a handout in all sorts of data heard that before. The stop controller or SAR components can be responsible for that. Honda the detaching. So there's octets are the control unit where detect that the structure of the data and data Harvard. In those areas that the typical vector processor will, her vector still are known in store units. This you need to handle the vector data to be innovative firm the memory, and then to restore it back to the memory. So they absolutely Parchman unions to new ways like your battery or that, that, that the bandwidth or Rondo one word per cycle. So after a mission they earns it that you need to also handle scholar nodes in our circuits stores tomb. So unless at the backed up forces are also has less water. Gestures do this with gestures can pass on the inputs to the vector function we use. And this mood, yes, first, awesomes, compute the addresses for the vector u sub mode, the mode operations at the store operations. And there are 32 general purpose registers in 32 floating point through gestures in both design and one put out there better. Functional units can nudge the scanner venues as they are with all of this better we just or flight. So these are the basic components of a vector processor and what they are responsible for during the operation in the vector processor. So basically the nuns out the vector. Yeah, and is it divided to be fix that tipping on narrow single instruction library database architecture will record the exam. And if so, where are the fixed on month if we're not efficient for both whites as IMB. So they know better known as rejector varies of fakes or less bad, that it can be used to cure a single vector. And A1 has, I'm used to rate in her womb, but ports gigabyte now will be 232 back the reducers and each of this backbone JSON response code 3648 associates for pace. So that the Iraqi buy-in for the vector registers. So the instruction set architecture as radius class or before their vector dimensions can't extend instruction Sarah sorts of ways the instructions and to operate on multiple data packed into the vectors in parallel. So these functions then that architecture vector, its teachings or the vector instructions are our two it is I am the Arctic instructions. So they define multiple operations in the classroom for the instruction fetch, the instruction decode in the ischial can be reduced to write. The operations are executed in parallel. Csun are various measure and low dependency among these data elements. Therefore, it needs are much simpler hardware design. So the instruction set architecture vector, what should the memory access them? No, patents for their memory axis are predictable. Therefore, the axis performance can be improved by preventing duty or the simple memory scheduling policy you may have beside that, for patenting of the mighty banking are for sure in memory hierarchy. They may be used for improving their plan was to for the operations. So according to officials, often instruction set absolutes are vector. Yeah, it has multiple advantages. So first, it has a single instruction that fine. Lexer and operations, that is a law of operations here. They avoid plant almost height. There are cross out the instruction, fetch, instruction decode an issue. Also the frequency of the branches have been reduced. So fun though the mesa and operations data 100 amp. So this is, there's no one there it is. We can use this data's the Maronites and Lloyd without the homepage hierarchy, Deepak with apparent nicer and a plant also obscured the operations at data in parallel. Yeah. We'll assume that based off the unparalleled data passes for this operation sphere. So also, you know, there's no need to designate the code notes on roadshow people. Therefore, this can reduce the branch instruction sequence. There are lots of advantages to the processors. And also there. Yet at the same time, also these other vomit out the bachelors, right? Well, that they can only work if the paradigm, some irregular, or for instance, the data or the single instruction, multiple data parallelism. It'll be very efficient for the paradigm that is not regular. There is also limitation for plane searching said architecture, vector bodies or memory batteries. It can easily become a bottleneck during this, in this oxygen, especially if the magnesium and the memory operation balance is not maintained. All the data is mapped. A problem with it. Cheaper lottery bat for that, I will raise the higher latency in this cases. Yeah, guys that Craig know that the vector execution time, right? It depends on three vectors for that. To mayo the operation on the performance of the vector processors here actually include the nuns up the upper end of vectors, the structure of HIV that in that they got right for the word determine the execution time. So if a given month of the vector and the initial, nice to read that either way the airway stuff vector units you're going to consume or new upgrades and produce new results, right? So we can plant with it that pan this town was taken to execute or a vector instruction. So was just a one name. And the initiation rid off or Edmund per clock cycle, actually exclusion term of one vector instruction is approximated the vector nuns that herself. So nice, the way to hold your vector. Time. Yeah, the, the summary that are approximate what the batter nose itself. So why design the vector processor? The length of the vector will be, yeah, One of the most essential parts that will affect the performance of the vector processors too. So, yeah, Do you know the vector processors can be always the easiest, right? Yeah, especially for the control hazard induced by the operations. Legs are branches after this class or people, right? So now we need to think about how to deal with the branches in vector for the exclusions legs are conditional execution tray. So the solution that is two years, the prediction here. So actually the computer science or prediction error architectural feature bad provides an alternative to conditional transfer of the Council during the operation for the instructions, right? Right is implemented by their mercy instruction such as the conditional branch, conditional core and all conditional written and also some branch tables. So prediction and whoops, bytes, curing the instructions by both parties are on the branch and only permitting those instructions farm, but take them pass to motive. Why the axe head to date, as you respected. So that instruction from the past are permitted to modify the actual tool. Stayed. The course. They happy are associated. Or they how being predicted with a great big picture. Actually, they are predicted and the compile time, right? So during this process, a Boolean biannual used by the instruction will be US that you're comfortable whether the instruction is a load, your modifies outfit your state or not. So these are the prediction to change the execution stage so bad, it's basically going to England, vets a procession, Damascus and will be used in the same time. Yeah, lobby of NADH. And this is used to keep the factor with the single pit elements. And the flat menu where we determine based on the vector comparing agenda comparision of the vector. Besides that, I also miss flag registers where I'll be used at the chromosome mass before the next vector operations. So this is how the conditional exclusions are used by prediction. When additional ways, the flag value at the flat rigid spurs in the vector processor. To deal with the conditional executions. We have these faster the basis of the vector PR sensors. We will now look weren't some massive that can be used. Actually they nominate you or adopted by the vector of Sigma to improve the performance. So in terms of the optimization of the Outbound processing, yeah, The first an optimization technique is to use the Channing. Yeah, in computing a Channing as yeah, techniques that can be used in computer architecture. Yet by using the training here, the scanner and the vector Ri gestures January, the interim, a result. And this result can be used the immediate kid, they always out additional memory references in order to reduce their computer Nishnawbe, right? So let's shiny timing was first used by the Seymour Cray supercomputer around 1976. As soon as an example here. Let's suppose that the color here with a Bachelor's of the next 30 to 32, 64 bits we gestures team. So there will be a very long read after write, Harvard during operation. So the chaining technique can be used here to software data. Since the big one as not a single chip on a groove of the individual elements. Therefore, the planning plan we implied and also forwarding can work on the element faces. Furthermore, the flexible it's showing is that improve autonomy to you. Actually it can't or low effect of Chan to any other active vector operations. So A's cases are more raid operations, more red parts, and the red parts are needed. To support the Channing. They can't enable. It can be enabled. There I not hardware resource and you know, yeah, let you raise more read and write ports here. So in order to increase their planning size field by using chain here. So another optimization techniques for direct perception is to use a multiple names. For them. I will name this optimization. Yeah, looks and having hand on my knees. So that more than one element can be generated per clock cycle, right? As shown here. So you can use the hidden names in the vector unit. It's thought that Beta level per night, so its inventor noun contains a slice off the bat. So we'll just her bio. And our data has for each factor functional image and one or more falls into the memory hierarchy. So the elements so easy back to regions are actually into our knee or cross lines. So each cycle, the lance was identical, comatose seem most. And then they can execute multiple elements operation for each back to instruction. Multiple elements operations are executed per cycle. We need to know that there is no need for the inter-domain communication for most of the instruction because most of the data are that have no dependence between each other. So for instance, this year, it shows a home phone lens. They are phoned in the vector processor. So the Vector3 gesture storage, Earth actually divided or murder foreign lands. Yeah, thou with H9 holding every force anomers for the event. So read yesterday that the adder units and the Modify unions and also the dog or non-union store yearnings. They can work together on the force parallel line. Right? Now are responding. You need half-full extrusion or papayas and adding multiple news. Okay? It is a popular way of increasing good performers of the vector processors. Yeah. It does not increase their control and their design complexity too much in their design. Yeah, in the next designing the vector processors, there will be the eventual nuns we gestures that you handle about 64 y two vectors are actually mentioned that the 64, I wanted to add a fixed size for the factors you will. So the main effects are NAS, depends on the length of the factory gestures. And the vector noise may not even be known at compile time. Therefore, the processors Nirvana and the usage of the very nice, actually the vector months when it's yester. Yeah, so the vector can be loaded into the register them. This will work as long as the real LAS is less than the maximum non vector. That's for example, assertive. All right? So that's a resistor. It has to be some cases where, you know, there are less than is greater than the max is maximum eminence. Actually, there is a technique called the strict, meaning that were being used. There flow several iterations, data multiples of their maximum nurse. And the remaining iterations are done separately. In these cases. It awesome. Well done. The vector Moscowitz yester, yeah, as shown before, the vector mask that we used in the vector processors, right? So the vector Matsui gestures, you'll instruct your handler the instruction fetch stage, and we're back to code. So sometimes we may not want to execute the operation or all the elements of the vector. Yeah. So depending on some conditions, we may want to operate on, certain elements are known and others are not. So this is done with the help of the vector Moscow a jester. And it has one page per element. So depending on where there's a Maschine paid, a fed or not for each element, the operation is pirate out for that adamant that we really need. So also the myoglobin is a procedure, has mammary backs to the memory system. Optimization can be used to support the vector processors. So as the memory banner is, demands are very high for the vector processors. So the memory interleaving and dependent memory banks are used in the breadth of processors to use it you support for their controlling the bank or dress independently. And the loading or storing the non-sequential awards in multiple vector processors when they are sharing the same mammal or so, they can support their advisor, memory banks. And ongoing the vector processors. Are there some programming better our teachers? So program structures are affecting the farmers are linear. So at this architecture it that we can provide our symbol exclusion mode. And the complier that can tell the programmer that which portion happening vector realized and which I'm not. The compilers can provide that feedback to the programmers and also the programmers can provide hints to the compilers. So they sat, there are architectures are Barbara programming vector of features between the vectors and also the verbal numbers. Do it to organize their data according to the arguments. So all of this choose a nice, tune it on yes or no of vector units to reach high computation us recruit or is dark and using their complicated logic that are necessary for the highly structured ischial rates, right? So by the time the vector processors day you re use objects 16 maize in their design. And there's some great work showed that the future designs are expected to just even more acute than the 16 days in the vector processor shampoo. So This example to use the both of the chin and a much more news optimization severe was the instructions here. Actually, I mean daisy example of the veteran news is that scheme was for names and two functional units here. Yeah, there are 12 admin operations that can be chained per cycle here. So there is then sir optimization technique that cord there are conditional execution. So as weighty before the solution for the vector from these no excusing, that is to do the prediction way. So specifically the vector, yeah, the mask is will be used right in the Moscow region. Like so. Thus, it is to add a vector that will gestures with single beat elements. So now we can use a vector there to set up. Then we gesture, find NA, use the flag register as a Moscow control for the vector sub families to add the executed only for the vector elements with the corresponding to the snag animals that fit. That women should be thought that the mask where the author never want the real data we need to, Jews and others in the sim vector may not. So that is why reduce or musket to flower that data in the register here. So yes, we have known the next instruction, The SIMD instruction set extensions, right? So we also call it the multimedia, a single instruction, multiple data instructions set extension. Yeah, As a similar way, in addition to the events or processes or to explore the data level parallelism, right? As the standard one. Yeah, there may be some comparision became the vector processor, right? So as we talked before. But among the four types of computing models, according to the classification of the single instruction might have or data stream. Simd can't effectively utilize that data level. Einsum and time the media, it's Tunisian started ways those single observation that many media operation applications can operate on data types. What kind of data hard-sphere actually listed as hives, you have narrow ways than the native word size. For example, an adder can be partitioned into those smaller size the errors. However, unlike about the instructional which I've added, right, the IMD exclusion will have the following limits machines compared to the vector instructions isolation before the mouth, the data afterwards I encoded into the op code itself. And so about the number of the instructions for improved manner, there is no usage RPA back to progesterone in this cases. So there's no support, blah, blah. Sophisticated addressing mode, for example, for the stria XES, AES ones, why don't gathered axises. And also there is no support for the conditional excuses as there is no support of water masking registers in a vector processors, right? So on this record, but these are the monthly or the bottlenecks. Or make it, make it harder for the compiler to generate the single instruction multiple data code and make it harder to increase their actions, increase the GitHub corner of the programming in the SIMD architecture. And that is I am the assembly language. So now SIMD extensions here, they have implemented a man and pronouncing to one of the examples of those single instruction, multiple data instruction set its tuition here as for entail. And also their Sarah's cord that into our acts on faith that Canaanites corners processor. So actually what the design from now by using the SIMD extensions here from the news about the exam, the family co-processor, our teachers. Yeah. Yeah. Renews the button in 2012. Yeah. No. Thank rid of core architectures. They combine many Intel CPU cores on a single tube. So Intel make architecture, etching. It is targeted for the high parallel and a high-performance computing workloads in a garden or fields that computational physics, chemistry about already and also financial services, right? So today is that woke nodes are wrong as classical parallel program on the last compute clusters. So for that design, for Intel bought the SIMD as actual Intel MIC architecture. That is, they end up achieving high-throughput performance in class, the embarrassment and the rate of flow, Fanny Palmer constant. So that's just going to ask you on the left there figure here. This is the first generation of the Intel axon products. Yeah, Canaanites corner here. Now this architecture, so I'm guessing that time, the ISE year. So missing. Now, architecture is a multi-core chip with the X86 based a vector processors first. And then. Now co-processors are connected. Exam processor? Yes, through the the connections, the etch it as a tablet. It is shown in the right here that Intel Xeon processor, it primarily composed of the processing course caches, memory controllers, he's I elements logic in a very high bandwidth here. And, and also the bell directional ringing connection. So H core content with the private caches. Yeah, KP coherent by the global distributor tab directory, right? So they said that basically these thighs firm that in terrible at the time, the year. In further step. Each call in that into our axon co-processor, that is, define it to be Pareto efficient or web providing your high or Alexa booth or the high pilot parallel workloads here. And a closer look reveals that the core uses are short in order and that it's carnivores, the putting the full res, spread. How real and that you are executing in a short per pen. And eight is the estimated that the cost to support the infant teacher magazine is more eyes to Person of the M, pass out their core and even ness of food tube or the product level. So according to the crowd, the thigh or the cost of bringing the entire architecture of permeability actually is marginal. So that other designs out there about the SIMD for the propene also as you're in this fever. Yeah. I also have the products are here. So after we know the basic design and also some real doubts about the Vetter and MD, right? There will be yeah. Yes. These were already know that approach is Fonda vectors and that is I am needs fear yet to deploy their data level per night. Is that right? Yeah, What's the difference between a vector processor and the basic SIMD here? So actually, you know, the vector processing oxygen are now considered separate the formula, Assign the computers. Yet based on the factor we know about the computer per set of vectors, one water at a time. So they're Pokemon and processors. Even though das nur based on the bus and noise function by chewing well as moments, SIMD computers process all elements of the vector simultaneously. And during the implementation, the vector processor are typically more flexible with non-fixed of fractured lens. And the flexible and less on those programmers to have larger or few vector road yester or even smaller. Move that through gestures here. Yeah, that kind of nanotubes are complex. You, for the design of our teachers. Well enough time and you have radiative victory gestures. Well, you know the NAS as fakes to him. There'll be said that, but also the vector processors and half the scatter gather instruction that Campbell's the data between memory and the dance. We gestures to make a balance between them. So a sub I have the, these are the managers at your SIM needs tuition. We've shown they have certain advantages over the vector our teachers to etch at the time the tensions. Yeah, comparator with a vector processor, right? They cost nato to add to the standard arrow and it is easy to implement. It also, the time it's tensions were nato its first-aid. Yeah, that means it is also easy to complicate this wage or repairs made it extra manually advance during operation. So we said that also the SIMD extensions. Yeah, now they need no, no virtual memory problems. Here in this panel of our teachers lead the cross major cause analysis of the, yeah, the related operations that need the virtual memory of partners in their architecture. So when we compare with the conventional better architectures, so they'll probably know SIMD and also to variations including the recto processors and also the Center one of our staff. Single instruction did ask during the extension here when no heartbeat Barragan sign and what kind of vanishes edges. The other one is to one compare with different designs, just support the primary naive them here. So fuzzy, Sarah, about the GPUs and how they argue is that you explore the data level per nine CRM. We'll talk more about the GPU next class. Show you some example. Assume their performance in some real-world examples about their GPA or stretchers are nowadays we are still working on. So these are all the contents today. Thank you everyone to return to the past and hope we'll have a birthday. Thank you.