 Good morning. Good night. Actually, I'm a Hong Kong is laid in the knee. Welcome me. Yes. Thank you. Thank you very much. Thank you. Okay. Now, anatomy surmise first. So can anybody see my screen now? Yes. Yes. Yes, we can see I can see other thank you. Morning everyone. This is the class, of course, easy. 538, wants to computer architecture and the University of New Mexico. Today, we also learnt you have run, be sure to give a talk about the research. Dummy specific FPGA overlay, an architectural company relation, co-design method. Name roaming. I appreciate it sees it is late and lies in Hongkong now. So before that, let me give a short introduction of rubbing and his research as Yoga has homepage run Major at the PhD students of parallel computing at the University of Hong Kong. Pays the research interest is domain specific FPGA overlay, which delivers FPGA accelerators with an architecture computation design flaw. Particular domains including inmates, sugar, resolution, machine learning inference, rough mining and this Boston contraction. He also has a research experience on the hardware accelerators for machine learning. So today we are glad to have him to give a talk about his research. So what I'm rubbing UK, share yours gray. Oh, well, yeah, let me share my screen. Okay. And maybe I'll share in first. So you can. Yeah, thank you for introduction, professor. Yeah, it's a great honor to be here. My name is around being shipped from the University of Hong Kong. Actually, I'm a final year PhD students. And I hope you're staying safe and aware in the US, I guess most people are unit. Hello, I'm sorry. I still like a happy face. Yes. Yeah. Who is sharing the happy face? Now? Just yell your slides. Actually. It's not shared by me. I give you the right to share the screen. Maybe. Ok. Green. Here I said, you can now start share screen shot while it's the ADA participate is Sherrie inside a lot of people sharing. Sharing. Okay. You can child they know. A great high cache. Yes, that's perfect. Okay. I guess you can see my topic. Yes, we can do is to walk. Yeah. Perfect. Okay. So that's that's not so okay. Am I talking to the ASE CTE domain-specific FPGA overlay. And it's actually an architectural compilation co-design methods. So cos, I'm as a student as well. I hope this can be discussion and iOS that Q and a section in the middle of a discussion, or feel free to stop me and ask your questions. And my talk will cover four parts. Firstly, some motivation and why we use domain-specific computation with FPGA and then proposed a systematic design pattern. We call domain-specific FPGA overlay. And then I will introduce two examples in my works. I'll follow this design pattern and the design, the part a demand that is very popular that they've learned. So first, let see why we use that. Domain specific computing is a very famous chants from the Open AI and color points where we present the AI applications in different ages. And the horizontal axis is the year from 1916 to 2020. And this is a radical x. This represents a computation demand for corresponding applications. We can see it's an AI application, or recent ten years have a much faster increase. So whenever we use a Matric card, doubling time, that means how long it'll take to increase the number to two times. In the recent ten years. Doubling time of computation demand is 4.43 months. Okay? Let's use a consumer producer model. The demand is a consumer. So where is a producer? To Moore's law, is the user before? But the doubling time is two years. That means we can, we shall wait two years before the computation power babble. And importantly, it has been ended. So we guys have something to do how to meet this increasing demand. First before we do even something we need to think about where the demand come from. Is that reasonable? My answer is yes. I summarize the reasons to three at spect. First, the scale up since we study is getting larger. For example, we study a social network graph including billion nodes. And the second, the granularity we study is getting tinier. For example, we use computation model to modelling it every sale. In the war are two guides. The precision cancer treatment and assert their relationship are getting more complex. Previously, we just did modally basics with director mathematic, but currently we needed to investigate the relationship between scenes by complex neural network. So we use, we use a technique called parallel computing to address this issue. Before I doing something, let's see some previous success stories in the success stories in the hallow computing. So I'd like to, and I something, use a time domain. So the first-generation I chord is distributed, uh, system. Okay, and organize hung traits to sudden CPU cores with networks and a key technical and named and message parsing. The Bennett Centre is looks like this, looks like this, and it's incredible, but very expensive, an energy cost. The second generation is GPGPU. Instead of in the visual service, GPU packs thousands, of course, on the cheap without controlling mass or the card. Simd single instruction, multiple thread. And it contained. But it also makes a networking to on-chip interconnections. Gpgpu is a big movement. It will reduce a Coster and Power BI nearly 100 times in some applications. Yeah, there's a story if you frequently bought some stocks from Nvidia, you may become very rich now. But if you didn't, don't worry and maybe after Professor Yang cost, you may phyla and some other ideas to build the next the company as greater as NVIDIA solid. See Watson next-generation or what is a following direction to improve its a parallel computing GPU is a perfect example. Yes, TV succesful story of domain-specific computing. It designed for the matrix multiplication in the planning and weeds out to redundant logics. That achieves I'm much better efficiency on silicon cost and energy. That's ever more. For example, to item two from the show, it's a molecular dynamic cheap that shows a important role in the backend design, especially for the current stage and the graph K4. It's a cutting age processor for AI. So let's have an apple to apple comparison between the general-purpose computing and domain-specific computing. So from the processing, it's asked several perspective, perspectives in the comparison. Okay, that burst on the processing element. It's a general purpose. Say always say general-purpose processors, usually a set, an arithmetic logic unit, ALU on cheap, that supports all kinds of computations. Addition, subtraction, multiplication, division, sign, explanation, and so, and so beta type is a standard from the I triple E, for example, the int float double. However, for domain specific. Because we so processor only near two feet or domain. So you can have different abstraction. Instead ofs are element-wise operation. For example, the TPA you use. The operation is basic operation is matrix multiplication. And it can even redefine. It's a data format, the data type, you can use arbitrary bit wise operation. For example, that developed brand like floating 0.8, which is very suitable for the, the planning models. So the advantage of domain-specific computing is make every gate on silicon useful. Then to the memory, general-purpose memory always contents R-one to R-N caches you may find is, it is very difficult to write programming with a high performance while tackling with these caches. But for domain-specific, because we know that access, data access pattern of this domain. So we can dedicate memory. I'm cheap for this domain. For example, we can now say that X pattern is continuous or interleave. We can design specific preloaded unit on the cheap to maximize the memory bandwidth. Okay, if it is a processor, it's a compiler. To compile the compilation part, the general-purpose, say you, It's a compiler can usually do basic optimization because the design space is too large. So they can do some basic optimization like removing redundancy, redundant codes or something. If you want to achieve a high performance, you probably need a scattering. It, scatterings are war cloud manually. So the users don't like that. But fundaments, but Cpk compact, we can do domain-specific optima as Asians in the workload of scattering to achieve, to have the users achieve higher performance. So it is much friendly to users. But once a problem of domain-specific computing is expensive, we cannot make a cheap for every domain, right? But we have FPGA. So that's a motivation why we use FPGA bought domain-specific computation. Fpga is an movable type system for the integrated circuit. Is like this, is an array of fine-grained logic components. It look likes this finger. The underline layout is composed of program ball logics column by column, and there are routing resource and the switches to interconnect them. Multiple block types. For example, the laughter bought a bigger, shows a lookup table, LUT. With M-bit input, signal says there's a configurable RAM that stars or output values for each input case. It's a developing into where help to calculate these values according to the logical function we want to realize and load into the RAM, the array FPGA configuration. With thousands of interconnected entities. We can realized arbitrary logics. And there are also DSPs for common arithmetic like multiplication. And of course, the right bottom block, B RAM, block Rang Bie Ren, is for massive storage on cheap. Different to the generic cash. K5 is caches. We can organize the B rents for specific data access pattern and provide enough on-chip bandwidth. So therefore, advantages of FPGA, its beta, why it has been wise reconfigurability as cost efficient compared to the isaac tape out. And importantly, it's a prototype of Isaac. So if you want to finally tagged Isaac, you can make some prototype on FPGA to make it work. And importantly, it's a good time to use FPGA because if it is accessible on the cloud, so you can provide the end users some surveys on the FPGA acceleration based on your domain specific architecture. We have a quick recap. So now we have, now we are clear about two points. The first is domain-specific computing is highly demanded. The second, these FPGA is a perfect platform to realize it is actually. So if you have some questions you just ask, I'll type in the chart. Are to ask any questions you have. Yeah, of course you can ask at the end of my talk. So okay. Absolutely clear about that high availability of FPGA, for example, compound was a snake thoughts. Yeah, that's, that's a very good question because FPGA is a reconfigurable, uh, so a lot of logic on the cheap should just support their reconfigure our ability. So we cannot make every gate on the FPGA asic Computation Resource. Yes, so maybe it depends on your logic. So maybe, wow, 20% on the cheap results that ONE cheaper is off the on-chip results can be used as comput computation. Yeah, but even these, it is very competitive if you are running some bit wise computation and yes, some, some, some application with irregular that Hayek's pattern. And of course you can interconnect them multiple, multiple FPGA together, say async, come they, you'd probably do this using multiple FPGA to stimulate one big Thick, likes Intel and AMD. Yeah. I think there's some question in the honorary. Can we expect to replace high-performance? Yeah. Yeah, that question from direc. Can we ever expect FPGA to replace high-performance A6? Yes, very good question. And in my graduate thesis, I did some analyses that why we use FPGA and in way in which scenario we use Isaac. And finally, I get two answers for it. The first is where we use FPGA is when the early market up your application of your domain cannot support the manufactory of Isaac. And the second scenario is when your application should be eatery, actively developed, the algorithm changed a lot. So if you want to manufacture it and Isaac, you need to wait for 6.5 a year out, one year, something like this. But if you want to deploy, it's an FPGA acceleration for this app, your algorithm very quickly, maybe into mountains so that the FPGA is bachelor choice. You can develop very quickly. Yes, it will go along the ham for a state, Catalans. Even though SAP have maybe better performance for specific applications, right? Yeah, exactly. Exactly. Montages. So yes. Exactly. Okay. Yeah. So perhaps we can move on. Yeah. Okay. So the next week, Hume's problem, how to design. Yeah, and how that's means how to organize audiences. But the silence of logic blocks on the FPGA to realize it's a domain-specific computation. Okay, if you lend across named detailed design, a computer architecture in your under graduate, you may find it very complex, like play a puzzle. And during my PhD study, I abstract and I narrowed it down and unstructured method to a high level and the proposed a unified design pattern with which we can easily analyzes the workload in particular domain and propose the hardware and the user interface. Domain-specific FPGA overlay. So once a domain-specific FPGA overlay is a method or it's a design pattern of customization. So long is like this, the right figure. It's a medium between the FPGA device and the user, say vanadium. So it should be friendly to the two ends. So it should be friendly to FPGA is at means that designs a hardware should be resource efficient. It has a good timing and the scalability in some hardware metric our present later. And it should be so designed, it should also be friendly to any user. Let me add a user is who use this application, not who use a hardware or something. Yeah, and I should add that in to parametrized applications in a command, for example, plot the planning. You have different layer numbers, different layers, eyes, feature, size or something. So basically the overlay contains two components, the pattern Architecture and our compiler. So power can be architectures that should, can be competed to FPGA in seconds, maybe five to 20 seconds depends on the size of FPGA. And our club compiler and translate the user's application to the low-level controlling instruction with optimization that can be loaded to overlay hideaway at runtime. We microseconds for faster context switching. For example, you, while you are computing different layers of the narrow network, you can only change the software and instructions on overlay hardware. You can not read pro grandma's reprogram, say FPGA because it's too long, it's in the second scare. Yet. So there is a systematic design pattern of the overlay. For example, on the right side, it's the FPGA device and the user, and on the left side, this FPGA device. So when we do is we first work abstracts a workload or working with abstraction for the end-user. Here, as I mentioned before, because it's domain-specific. So the operation here, we have different granularities. For example, the fine-grained operation like multiplication, addition. This is element-wise that operates on one number. But for example, the convolution or the matrix multiplication. It who are all pray on a lot of number, right? With a specific data access pattern. So, and of course the arithmetic is also we need to abstract because we were, because of many, many applications are coming out. We had different arithmetics. Okay? This either we need to think numerically, clearly, workload abstraction face because this is a basic for the hardware and compilation design. So the hardware organization. In this step, we just to consider how to organize the FPGA underlying components to the processing elements and the memories according to the abstraction of or cloud. And then also we need to set happy instructions for the instructions and corresponding to the workload. The instructions sometimes we need to consider are reusability, albeit because we need to start a instructions on cheap. So it should be reusable. Otherwise, there would be too many instructions at exceeds the instruction memory. Okay? And Compile. The last step is set, is designed compilation strategy. This is very important. And yeah, basically is a compilation strategy is how to map, so UCAS application to the architecture in a very efficient way. For example, how, how to make the wrong faster, how to makes on chip, how to make more reused for the on-chip data. So they can, has less access to the off-chip memory, which is very power consuming. So basically the things we did in the estab, our modelling of the constraints and the objectives there in the mapping. And importantly, these are not three separate steps. This is thus set three stabs, weeds, feedback her feet. Btob act to each other so we can iteratively design. For example, we change a little bit in the workload abstraction. So we need to change the hardware organization and also the compilation. And then sometimes it is very popular. We need to codesign Zaha and the hardware, the overlay framework with the algorithm. I will show an example later on. Because say, extend the algorithms algorithm can be changed that make it more a little, maybe a little bit, well not affects the accuracy at where be more friendly to the hardware that is very popular because it's more power, all cost efficient. So this is a very abstract design pattern. I will show some detailed examples. The first example is deep learning overlay. It's ten out for FPGA. And the motivation of this work is people always say FPGA is too slow. Because if the FPGA is too slow, ADA advantages where vanish because the GPU can run at maybe one gigahertz. But sometimes FPGA can only wrong 200 megahertz. As it says, there's a problem in chat. Are you talking about how good yours is that? There's a question from either wrists so high. So are you talking about partial reconfiguration? Iu risks synthesized based on the workload abstraction. Good question. You know, lotto on FPGA, I say neither ox act as a partial reconfiguration is just to reconfigure a part of the FPGA. And it also takes maybe 100 millisecond. It cannot realize a read time awesome scene. And I'm also not racing size. So hideaway R4. Two workload. I'm not sure you mean so what cloud abstraction? Yeah, I see what cloud abstraction changes occupied or changes. I need to resynthesize. But if the workflow change, the parameter change, we only need to change the software and instructions. We don't only eight, you'll receive size, architecture. Okay, I help. Hi, I'm uncertain you okay. This care Elm reliefs. Sorry. C. Okay. You got it. Okay. Sorry. Okay. So let's come back and let's say what when the peephole. Yeah. We just talk about FPGA is too slow. I just want to say it's not a problem of that PGA and it's also not have problem of the design. But it's a problem when people want to map an improper design to the FPGA, that's a real problem. And this work, we address this problem and we call it architecture or design FPGA layout mismatch. It's a mismatch. For example, here on the right side or I showed design like systolic array for the CPU, we can see the optimum memory is connected to an input buffer and then transmit data to the boundary piece under the Peak, Data can be bypassed and on the p and the result will be propagated amount of PPE array and the Xander areas out. We go to the output buffer. It is something like this. And when people want to map this architecture to the FPGA, let's see one way. What will happen? The downside paper is the downside figure is layout or the FPGA. So the results are arranged column by column. So you can see is a massive be rents the block ramps or building blocks for the buffer on FPGA are connected to only a few particular PSPs, which is four p. Since things, because in this design, the input buffer, it's a Latvian per buffer. So a lot of b rams are only connected to the boundary piece. So that's a problem. So that's leads to an imbalanced routing distance. I'm in, in the FPGA implementation and it's safe, safe for our results to a very bad timing. For example, you can achieve only 100 megahertz, but it's a theoretical mix. Mm at operating frequency of FPGA is not like this part of the SP. It can achieve over 700 mega megahertz. And if you want to scan design, it's a problem. If you desire more, that's maybe it's okay by the frog to makes it design bigger. And so you can image, say imbalanced the reality is, it's inspire bad. Okay. We use an overlay design flow or overlay because I measured. So we first abstract, so workload, okay, we have a very simple analyses, absurd or machine learning performance benchmark. So we'd select the five application, do plenty models. Okay, we can seeds and most men would close on the man will operation our convolution and the matrix multiplication. So it's accounts for 99% house. I'm seeing on the ADA mission that knee applications, okay, so our hardware actually supports this is enough. I think. While we analyze it. Pseudocode of matrix multiplication and the conclusion, we can find it had it, they have very similar pattern. It's Mac operation, multiply and accumulate operation in the last tape loops, unless a level of loop ceased at different levels or different. Okay, so wish should our hardware should support this computation pattern. It is general for the, both the convolution and the matrix multiplication. So we began our design is hideaway organization. Step two. First, we design is a basic, basic processing element. Here we quite tired processing element, the cross is use our resource in a time like this. On the right side, it's local area in the FPGA layout. So we designed a PEs at only use resource in a local area of FPGA axis. So for each tp contents are whale buffer, intaglio bath activation buffer. And the DSP is a way to buffer is use a dram here. And so activation bar phrase consumed, consumed, CRB, Configurable Logic Block and some LUTS, hectare as distributed the RAM and the DSP for the multiplication. And you can see the DSP contents addition at the address where the interconnection between the neighboring TSPs. So we leverage it as accumulation pass. Okay? And importantly, we sent two clocks in the TPS o'clock air and the clock edge. And thus why? Because and it's a result CARB and the DSP for activation buffer and the multiplication of these two kinds of resource has higher theory, has theoretical higher operating frequency, and see send the b ram in the FPGA. So we set the clock edge for activation buffer that DSP, that two times of the clock LMS block random w buffer. So with this setting, we hopes at fist tp can achieve near to theoretical operating frequency of the FPGA. Okay, so now we have TP is a basic element. We consider how to scab hit. The scab is also very important. We scabs designed considering the FPGA resources where like this, okay, so TB cascaded and chen, Just like a column of DSPs in the FPGA layout. And we also said have partial sum buffer and set can start accumulation temporarily if the results should be accumulate with a new input activation. So it can fetch, so from the partial sum buffer and accumulate again after finished agents to output two off trip pass. And of course, on the TPS, one super block, okay, we cascades several TPS. We call this design hierarchy the super block, okay? The other TPS in the sewer block Use the same control instructions. So there will be control pass and the beta pass bypass these signals among the TPS. The silver block is something like this. Ok? So this super blog, okay, well, we then consider how to extend more of the design. Here we use a two dimensional layout aware super block stack. So in horizontal and vertical. So any horizontal super block, also you, the CellBlock we'd rather than TO share the same instruction like the single instruction, multiple data in method and the bird calls. These are individual super blocks, zap article stacked. So it is very flexible. It's a hardware design is like this. It's a very flexible designs. And each row of super blocks can be controlled by the instructions. So next is how we scheduled. So we, now we have a scale after scad app. Design of many, contain many TPS, men apiece. So next problem is worked on. Dilation is like this figure, how to map the Mac cooperation in the follow-ups to the TPS. Problem is contest usually contains two parts. The first is on wished HTTP. We part spatial mapping, right? It say it's an audit TPS compute can produce operations concurrently. So I'll wish TPS as in spatial and in which clock cycle is intertemporal, right? So HTP should come, which operator should be computed on the certain tp first and the second, that, yeah, this mapping where I've bet. So performance importantly. So we have objectives. The O-rings compilation. For example, we hope to architectural computes a workload as fast as possible, like some minimal time on, minimal off-chip dram access. This modeling process, it's very interesting, but yes, we can model and the solving mathematical twos. But I will not elaborate too much on this because this talk I want to introduce a big picture of overlay design. So if you are interested just to look at our paper or maybe just send me a mail. We can have further discussion or we can introduce some specific topic on this, how to modelling the mapping problem, which mass 2t. Yeah, we come to a simple evaluation when we evaluate FPGA overlay. So we first event, we, when we want to, you have to score a hardware design for that FPGA. We have two metric. The first is the achievable maximum operation frequency, f max. Yeah, it's Fmax is high, okay? You, you get a payback in proportional. And the second is hardware utilization in running real-world applications. For example, if you buy a GPU card with one solid enough, Kodak corresponds where you're running deeply on the training of some other computation. You can only use ten course, so utilization will be only 1%. It is very, it, it's not a good scene, right? Okay. So in the evaluation and Hallowell aspect, we evaluated Atkins is designed on two different devices to chance. And for each device, we scab design from Paris law, too large and too. So DSPs here, ISPs that DSP consume other resources on FPGA. And the blue line. The blue line is F max in after the implementation, after placing the routing. So you can see the F max is, it is very stable. And the actually achieved so far, the DSP can achieve over 650 megahertz and on different achieves and with different design scare. So we can say it has a very good scalability. Okay, so, and another aspect is that compilations. So we want to evaluate how much of the hardware resource we can utilize in the real world benchmark layers. So we use a performance week. We explore the performance with a roof. My modal, I guess you have lenses, roof line model, but if not, I can quickly briefly it. Horizontal axis is the operation off-chip memory axis and the vertical, yeah. Yeah. And for example, if If the point is a solution, if the point is on the right side. So it can to more computation for each dram access. So it is obviously better and it's a horizontal or vertical axis is the attainable performance. So the hires a by a bad Her. And four here we also add another x, another dimension, it's the color of each point to. Here is a. In our case it's a weight buffer efficiency because of weight buffer in our design are separated. So different Ps cannot share the weight in different buffers. So if you, if the, if, if the scattering I locate two operation use a same waiter with the same width value to different piece so that the weights should be duplicated in different TPS. So it's a way for efficiency web behalf. So the darker the better here. So we can see It's a compilation can search for the mapping solution that achieves near to say array called here is we have assumed, assumed in y, that's the vertical axis. So these points are actually very close to the theoretical throughput. So in analysis we can achieve over 80%. How do I use ionization in most to benchmark layers? Okay, so achieve, achieve, Christ, x_max and high hideaway utilization. We can say it is good overlay design like this, analyze this method does not, can also be applied to all kinds of defines FPGA. Okay. So do you have some problem to have some question? Yeah. Go ahead. Yeah. Okay. Sure. So okay, let's come come to the second example. In the previous example, focus more on the hardware design or something. Maybe it's a little bit boring. So maybe you ask, OK, as an architecture engineer or can we do something with Michelle and training of the new best case I'm seeing in the Michelin model? Yes, of course, for example, this we show an example for the overlay for the fine grain structure, the spouse neural network model. To be more specific, hates codesign triangle. In this work is a kind of, it's a new method to implement machine learning model. That's a triangle three vertex, the model compression and parallel architecture and the workflow, the compilation. It's a co-design so we consider the hardware in the model compression axes. Okay, so first brief the background or the target or the domain of this. It's the recurrent neural network. Southern recurrent neural network works like this, the right figured. So it processes. I'm serious, likes a speech regulation or any artist scenes. And so Sam, Okay, so the unearned sales that computation is Carter said the sale process inputs, the input is they imbedded factors that in serial comes he served as history. So my poll, in particular as our multiple cell types with different computations, I have demonstrated to upside. Okay, it's GUI areas, TM, I'll say it just means they are composed of different computations. So each line the equation is named as a gate in the modal. That's how they have bits coming. Zap process information of current time point or memory rise of previous ones. Okay, in designing, in designing an overlay for, we first abstracts a workload. So here we can see it has different data flow and say it's a man workload is the same. It's matrix-vector multiplication as highlighted, as highlighted by the red box. Okay, if you want to Accelerator a simple, sorry. I hope you can see the previous design should work. Very good. Actually walk, walk, farewell. And it, because it's foreign based model for the matrix multiplication. However, we want to play some tricks on this. Because a whale metrics are usually large, that hands a realtime inference. We always have a problem. How to make an inference faster? In general, we have two directions. The first is reduce workload, workload. That is a model pruning of other weight matrix sparse buying that's just a, removes some not important weight values in the weight matrix. Another direction is increase the parallelism weeds para Hathaway. Okay. As a berry, Grady peephole, I I want both. So that would be a problem because there will be a trade off between the spouse computation's await. The width metrics we're sparsity and the panel architecture. For example, trade-off like this on the right side. We are computing a large, dense model. It's very easy to parallel when you don't need to design a domain-specific architectures because GPU can perform well. On the right, on the left side, it's completely random spouse, it's very flexible, but it's hard in parallelization. If you run some random sparse matrix computation on GPU, you find, so utilization will be very low. Fortunately, it's a neural network model is trainable, so we can explore in the middle. So struck it sparsity with different granularities. Yes, this is our walk. I'll be more specific. So previously we have the element-wise prolly. It's very flexible and to achieve, it can achieve a high priority rate. However, it's a random sparse matrix, makes a parallel acceleration very hard. This is because you need to scatterers and non-zero workloads to parallel processing elements. Workload balance the manner, but that's too fine-grained to Hallowell inefficiency. Yeah. Where encroaches upon. So again, from a workload reduction thou so, so people proposed disease, also cost grant pruning. So as a, as a right figure, as the entire row and column are prone as a whole, such that the remainder non-zeros can compose a dense matrix, then it guarantees a good hideaway efficiency. However, it hands of flexibility. It's a compression rate. Cfr we present. The CSP is carves the compress distracted blogs that men tend the fine grain, the sparsity pattern in the model, and it makes a efficient parallel hardware possible. Let's see how we do that. For example, we have a weight matrix and we partitioned it to small blocks. And each block, we perform a role column prolly that zeros out the noun significant row columns and there's a remainder. Now zeros can post advance kernel matrix like this. Prompt sparse model. Compressed. Yeah, we developed a comparative method is a compressed sparse model block by block to the specific compression format. For each block. Your car to the CSV. Csv format for each block, the CSV format includes a XYZ metrics, the nonzero row, column indices, and the nonzero values. Actually, this structure makes a CSB more efficient on storage, sends a general sparse format with a CSB patent with his pattern, that dense kernel matrix in each block makes it efficient parallelization possible. And the block size is at adjustable, so it help maintain the fine grand pattern in the model. And in practice, we'd developed a pruning flow to obtain this catalog sparsity, the CSP format it away metrics while maintaining a good accuracy. Basically the algorithm say alternatively, approaching the two targets. The first is the CSB sparsity pattern we want. And the second is a high accuracy with a remainder weights. Okay? So previous lays out some totally machine learning developing, If you are interested in, investigates the pattern, what kind of sparsity pattern in the model? You can, it's just a, Yeah, maybe an introduction work. And importantly, while we prone intermodal, Wow, ambassador model training the model we, we deliberately make it friendly to the hardware. That is a very practical idea because you can make a hardware cheaper x_min and it makes a taught the final efficient at her. Okay, so before we step into the overlay designs, a hardware compilation design, let's take a look at the big picture. We have events on the model. And then we apply the pruning, the CSB pruning to obtain the CSV format, either weight matrix exists, okay? So we still have challenges in accelerator design. So first is hardware should the adaptive to different island types? The second is because of blogs operand independently and may have imbalanced non-zeros. I'll workloads that where rays out an under utilization issue of parallel processing element. Okay, to address is we developed a program PBL architecture and basically a compilation for workflow scattering that facilitates a high p you'd efficiency or utilization. I will introduce later. So for the first challenge, programmable architecture, fall virus and stereotypes. So we have abstracts that arithmetic primitive to his memoir upload CSV format it, sparse matrix-vector multiplication and the element-wise operation. Then we design the data flow architecture for the corresponding arithmetic primitives. So you can see it included to enclose factor buffers, the element-wise operation units. And importantly, CSV ending, we use that Florida CSP based matrix-vector multiplication MB. And importantly, the program ball data pass so that so it can be suitable fought a different data flow of different cell types. We also set, say instructions to control this data flow architecture, like the normal instructions into content for which it saying instructions contain different dissections. Vr WL like instruction. So for different components are primitive. Unit it, the sauce operation 123 and the desktop nation address. Or the index of the unit cell. We said have a compiler. Compilers can trends different cell types to the targeted instructions and the Zen loads these instructions to the architecture before the computation. So we solve the first challenge here. For the second challenge is a little bit complex. So let's see how we organize the CSP engine. Inside the CSP engine. It's a parallel piece processing element for multiplication. Between the P Amal appease. The therapies are organized into level hierarchy. The p times q p is compose our T group that process a dense kernel in the kernel matrix, a dense kernel matrix inside a block, and perform a row-wise accumulation in the local buffer. Okay? In the high level of hierarchy, you can see on the right figure that k times l p groups, okay, is a P Q carryout parameters that can be allergists according to our design scare, okay, we have k times Rp groups process multiple blocks individually. We can find xhat subvert copy groups share the same slice of input vector. And the results. And the results in Horizon enthalpy groups should be accumulated. Therefore, the mb computation CFO, both the input and accumulation buffer where be updated only after or the P groups. Other blogs in this round, accomplishment accomplished. As a result. There will be a workload imbalance issue like this. This is a demonstration of one block round for the 4P groups, but in practical, many blogs for a weight matrix. Okay? We have a time of CSP with metrics is two, the two by two blocks. And that counts of non-zero, I embed I imbalanced amounts blocks. While the four blocks are located to the 4P groups, it's a clock cycle of computation will be different. And the slowest P group were bound the throughput in this round. For example. Here we can see the block for contends and most workload, okay, it's is very slow. So under p, groups should wait. So the overall utilization is only 41%. Okay? It's not good. So we design something. This is what we can do. Okay? Usually it's a GPU cannot tackle this scenario where because it needs some some hardware support if you want to share the workload. Yeah, intuitively, just share the workload among P groups, right? Yes, we did do it. We did it. It's a Hallowell Support follow workflow sharings. To address this issue, we developed some hardware support for this. Okay, so workflow can be shared among key groups. Support this feature. In the hardware aspect, we added extra connections between neighboring P groups as a red lines, you can see such set 1P group can transfer some computation workload to its neighbors. If the originally located block contains more non-zeros, say adder. Okay? Apparently we're the presented a sharing scheme or the P groups can achieve 100 utilization. And the overall processing time, fuzzy shrunk is significantly reduced. So there's one question how to control the workload sharing. The problem. Said, scheme we proposed to use micro instruction that is different to the previous Dataflow, be car that macro Instruction, micro or insights the CSB ended, use age. Okay? So because the CSP weight matrix can be obtained before deployment, okay, we can lies is as sparse workload one time and find optimal sharing scheme. For kernel matrix of each block. Settab, two portions that are shared with neighboring tea brew groups in horizontal and the vertical axis, it is highlighted by the small red blogs box and the blue box. Okay? These two bonds, okay? So the size of these two boxes are the pyramids. We want to figure out how, how, how much workload can be shared to the neighboring keys. We want to figure out the pyramid. Okay? We developed some algorithm to find the pyramid and achieve the workload balance, our share that in the next slides. Okay, after we find these pyramids, these pyramids where B translate to the micro structures alone with the indices of nonzero row, column. And there will be translated into micro instructions and the control, the hardware behavior for the wildflowers Sherry. We build, compile HE algorithm for searching the optimal Sherry scheme. The search target is to minimize the time for each computation run for this block round. First we analyzed as a workload of this round and the opt-in theoretical average time. Assuming the workloads of RPE groups are perfectly balanced. Then from the initial time margin 0, we build up a constraint logic programming therapy that represents a feasible searching domain for the sharing pyramids, mentioned it in the last slides. Knows that it's a permitted execution time is a sum of the average time. And the time Margie. Okay, subsequently, we adopt a mass to carve the SMT solver. Yeah. And the inputs are constrained. And to do a second stability test, because we have a time margin, right? The average time is a theoretical minimal time and that we have a time merging, we compile this too is a, uh, CRP is asked to describe a language to describe so constraint and send to a sovereign to check. It is Steph is satisfiable. So if yes, okay is good, it can't bouncing instruction. And if no, some margin can be increased. So we increase it a little time, a little bit on the time managing and repeats the search again. This compilation flow is performed on our blog computation runs. And actually, because the high margin is increased by only one Huang's clock cycle. So the satisfiability based searching flow guarantees a global optimum in the search. And so we can obtain some minimum time. We can obtain, we can obtain the optimal sharing scheme to achieve them minimal execution time. Yeah, It's a little bit about mes and the parents. So come to the high level just to do some experiments and the evaluation. Yet this is the same thing you can do. You can develop some model pruning algorithm. For example, we develop our CSB pruning algorithm, always pied Hodge. And we can implement the data flow architecture, including the sharing scheme with ATI error, Verilog, and it's FPGA, okay? Fpga is extensible. We have our bond, but you can also access it on the cloud. Aws and compilation tube. For macro and micro instruction, we implement with surpass paths with some existing libraries. In the evaluation, we first attack apes CSB pruning. Fine-grained sparsity pattern can guarantee a high point in rate. Or that's the same meaning to the if is flexible enough to keep the fine grained patterns in the model. Okay? We evaluates, it says with different benchmark layers and with different benchmarks. For example, here we have ten and they each benchmark we have cluster of five bars. Left. Most bar is far to non-structured products that's element-wise or the random pruning. And so right for bars are representing two. So CSB pruning with different block size, okay? So we can see if the block size is small with 16 and our senate to achieve all, most the same pruning era to the random product. So we can say that. Csb method, it achieved very good flexibility. Okay? So then it's, it's, a workload is reduced. So we can check if the VC's reduce the workload a computation, it's a p utilization is good enough. If the utilization is bad, if there will be a trade-off. Yeah, for example, if you use a GPU, you can. If it is a dense model, it's 100% utilization, but it's sparse motto with half of the model's eyes, half of a computation. But so utilization is only 20%. So finally, you can get laughs. Performance Computing advanced model. Okay, at here, we evaluated the impact of a workload sharing. For example, we have say, we we said we have shots before that if you results workload, sharing, it is back in is what pairwise maybe less than half utilization. So with sharing, for example, for here is each class of biases represent a benchmark, a layer with different block sizes. Okay? We can see as outfall bars each. For each benchmark layer, we found the average efficiency was improved from 42 to 94%. Okay? I forgot to say the four bands represent low sharing, vertical Sherry, horizontal sharing and with both the 2D sharing. So the 2D sharing can improve the utilization from 40 to that without sharing to 94 and is very incredible improvement. Meanwhile, we find the technical work slightly better, owns a large blocks. This because a larger CSB block, it may contain a larger dense kernel matrix that is easier for partition in the workload of scattering. So we said the vantage of both a high peroneal rate and an efficient hardware parallelization, we achieve a much lower latency and high of high efficiency. Saying existing works, they exists and works locked content. The random spouse or in some other sparsity pattern, this design for the hardware, yeah, by the way, achieve much better. Okay, summary as a motivation for this work. So improvement attributes to a good in the granularity of sparsity. Okay, so this is a second example. And finally we come to the takeaways. So in my talk, I first introduce why we use domain specific computation. We FPGA and the Zen I introduced a systematic design pattern required overlay. And then we introduced the two examples for our L1 is for the commercial metric multiplication and especially his towel, FPGA. Consider and with consideration on FPGA layout. And an example. For the particular sparsity pattern, including some training tweaks. But I hope I provide enough ideas for you. And finally, I can prepare a gift. A question for you. Yeah, alright, is this the Nvidia fuzzy neural network? And what where you find? Ok, thank you. Thank you. I'm being awesome presentation. Thank you. Thank you. I hope you like it. And if you have some question or you have some this dash and feel free to send me a mail. Thank you. And see if there's some problem yet. No problem. Thanks for you. I'll say the outcome exam was off. The FPGA overlay design is one for the machine learning models, samples from the hardware organisation, and thus techniques for the workload, compilation, and thus games for their call exploration workflow. That's very interesting and helpful for us. And since it is already midnight in Hongkong now, in a way, why is one passionate? Yeah, I'll come, I'm going to find the My next to narrow network. Lets you build the next MBTI, LOS greatness and media. I think we have the opportunity if we just to play with a software, maybe. Okay, we can just start follower. But if you play the software and hardware together, you can build an Yeah, that's a perfect at Harvard and challenges ends up future. Awesome, yeah, self-aware co-design. So this class, we are focusing on the list of the code you done where students are mostly interested in. So thank you so much and it is very late in Hong Kong. So icicle, you have questions, you can send that email address as done in the chat. Or you can go to the full page of my being to no more, less than glib. Appreciate it. Thank you and have a great life. Thank you. Remember area to have a great day as they care to save. And I bent by, by, by a variable to think up for students. When me, for one minute. If you ask Delia EI, you have got the scores of the homework too. So if you have any questions, you can send me e-mail and i less than the answers for you, that email right away after this class. And also about the talk. If you have any feedback, welcome to send me e-mail. Thank you so much, everybody. Go back. You have YouTube everyday. We can take you right there.