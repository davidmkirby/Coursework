 Okay. So in class today? Yes, I recorded for them. Okay. So good afternoon, everyone. This is the class, of course is a faster it wants to computer on teacher and the University of New Mexico. So therefore, the hello for the class must awake here by studying the, yeah, achieve some discussion about that data level parallelism. And also for the, yeah, you can remember the specific computing matures or about the yes, you can remember what the SIMD architecture we have discuss are two types of variation. The Bachelor processor and also the SIMD extension. So it for the class this this this Monday. Yeah. Is it was shown that for the costless Monday, I share with you with a seminar about God, dummy, specific PGA overlay. So and stem that the bigger actually talk about the architecture of the student in, I'm going to talk about actually about their oxygen nation co-design master and their research about the customer might the FPGA, that is a word or whether you used, okay. Why did they use the hardware, all the other bonds to a computer oxidation or decide today. So they're customize the FPGA overlay research for the deep learning systems, right? So they have investigate the structure about that application domain. You're actually at a software level right? From the arithmatic to the Data Flow and also the design scalable architecture. And they're customize the instruction said As we mentioned about the, yeah, from the self-aware to as hardware and also the interface between them. So I audition in his talk, hey, they have, yeah, At that point, the architecture to FPGA in efficient ways to customize the ADA. Actually that is approach or the design of compilation. We'll call it data strategy for the optimal workload scheduling. Yeah, that is, the main product, does not aware. And they showed the hardware and software co-design, yeah, for the other wants to computer architecture. So in this class we are going to continue the discussion about that. So the vibration of the SIMD for the data level parallelism. And some of us, some of them will mention before, but got like the general purpose computing a GPU that is abandoned at, that keeps your programming right. And it's used off the GPU together with the state you. Yet you accelerate the computation in the application traditionally handled only by the CPU, right? So inherent you dying combined with the CPU and also the GPU, they can be assigned the two dedicated computing elements, you or the GPU, right? Jenna provost computing on the GPUs, or the combination of Ben, we call that GPGPU, right? And that is the use of the GPU. And it is typically a jaded, typically can handle only for the computer graphics or to perform the computation in the applications that are traditionally handled by the CPU, right? So why we use the GPU? Because they are more efficient ADA for the data processing, right? Even though you know that GPU programming has been practically available only for the lecture plus the two or three decades, right? So whoever is evocations now, including the virtually every industry, right? So for example, like the GPU programming, it has been used to accelerate that Baidu, the digital image and also the audio signal processing like stress can of evocations. Other smacked up a statistical physics, scientific computing. It also lacks a medical imaging, right? And computer vision and deep learning era nowadays. Yeah, all such kind of applications among many, many other areas. So a resume years, as you know, GPOs are designed to accelerate the creation of images for output to display device, right? It has often been happening, the traditional CPU to solve the problems in the Aristotle work, whoever's in the handout. Only by the CPU, Yeah, actually is with a lower performance as you pan or some graphics card manufacturer, including the most famous author, we know as the EBRI. They provided a simple way how to understand the fundamental difference between CPO and traditional CPU or GPU at a traditional CPU. So they said that a CPU will consist of a field course. This course are optimized. What does sequential CLRS processing? Well, you know, a GPO has the massively parallel architecture. Data architecture consists 0, 0, 0, no similar and more efficient course. This course in GPUs are decided for handling multiple tests, actually doing them simultaneously. So the ability to handle them much more tasked with adverse impact. Actually, it makes the GPUs highly suitable for some tasks that we measure, right? Is better for the machine learning task was odd, I guarantee. So. Yes, such as Japan, the Fibonacci Sequence studies, the very simple example of that GPU suitable for some. Even they, they, they don't benefit from the parallel processing error. However, among the past, they do stick and even benefit from that parallel processing. That is, deep learning as we're mention. So one of the most highly sought after skills today, actually, deep learning is better for the average. Something that G-protein, the lexer activity in the layers offer neurons in the Moodle, right? There are no The mercies to know how to understand the non-wage recognize patterns or diets image with automation or that composed of the music. As a result of growing importance in the artificial intelligence. Actually, the demands for, for the developer to who need to understand the general purpose computing on the GPU. But it hasn't been, sorry, right? So because you'll know that GPUs understand the computation of partners, exit terms of the graphics primitives or early efforts to use the GPUs. On your legs that CPUs that are required to formulating the computation O'Connor's, yeah, in the language of the legs graphics cards. So yeah, it is a fortunate that I've now it is much easier to do the GPU accelerated computing. Yeah, we need to send to the parallel computing platforms like the Nvidia, CUDA as we showed before, right? And also shown here. Yeah, this is the crew that processing flow yeah. Actually be processed or conducted away with their opens. They are owed the open ASC say that combine the hardware and the software approaches to realize the parallel computing these platforms. So these platforms are low, the developers to ignore the language barriers that exist between the CPU and the GPUs on this platform. And there are bad, they confer course the higher level computing concepts, right? So initially agree needs to buy a bigger in a raunchy. Saw that Stephen, yeah, the coup, their system or the coup that FAB moments you here. Is that the omitted? Yeah, framework. Also use it today. Though with Kodak developer able to dramatically speed up the computation, to compute the data applications by harnessing the power of the GPUs they provide. So that as a benefit, the crudest system can provide that for programming sounds like GPUs. So yeah, as a waste for the legs, the programming GPUs, they are from the AMD and others have though also have the open style programming. Yeah, the oven computing that what you were just to say that open, say I'll write this language is that open? The royalty free parallel computing API is designed to enable the GPUs and Azara co-processors. They can work together or in tandem waste the CPU and to provide the additional row, the computing power. So as a standard, actually, the first award at the OpenCL at 1, they wrote and was raised around 20088 is that independent Spanner consortium with this programming now which developers have non sought to divide a computing problem into a mist of legs are concurrent substance that actually make that visible for a GPU to be used as a mask called processor working with a GPU to better handle the general problems together. So the potential of this feature genius computing model with CPU and also teach you right, that was in Cuba by the fact that the working with the CPU to better handle the general problems. And yeah, it is potential for this heterogeneous system. Yeah. For the programmers that they could only, yeah, actress properties, programming language, right? Action, the traditional one, we're limited their ability to write the legs. Applications are cross-platform applications, right? So yeah. Well I talking about that the primary benefit of the open computing language, yet a substantial acceleration in the parallel processing. So the OpenCL takes or computing resources that are available on the platform, such as, you know, the multi-core CPUs and also some multiple GPUs on the path. So as pure computational units, when they thought they store associates, legs and molecules, one CPU and also the GPOs, right? And they are CRISPR on it, but okay, a different level of the memory. You ought to take advantage of the resources available in the system. So the open computing language, the Open SEO, also complements the existing OpenGL, That is a nasa API, by sharing that data structures and memory locations without any copy Okta, or the conversation overhead between them. So is there another benefit of the open computing language? So the benefit as the cross burner self-aware portability. So that means this low-level layer, crohn's sure, you know the explicit nine between the hardware and the ocher software layers. This design or the hardware implementation specifies, such as the drivers and the runtime right? Invisible to the upper Nemo software programmers through the use of the high level abstractions, right? So low, the developer, a kid, they can take or the bondage of the legs. That's ahead of where we started having two. We shuffled the upper south the world structures. So that Changi from the prover during programming to open standard also, it contributes to the acceleration of the general computations. Yeah, in cross when they're fashioning that are provided by the open computing language, the Open SEO. So to tell them about the open cell model, the PECC for modal. Yeah, it is defined as the base coding a host that, that is connected to one or more OpenCL devices. So as shown in the figure here, the other instead of modal showed the pen from modal comprising one host. Yeah, plus the medical control devices. And h of that device has multiple compute units. Each of this unit, we'll have multiple processing elements. And the host in the open sale. That is, could be any computer with a CPU that the CPU can run a standard operating system. So yet to be as one of the OpenCL device, actually this opens their devices, can be, you know, the GPU, DSP, or just a multi-core CPU. So all of this. Vice king consist of a collection of one or more, compute the units. Now with this computing units, we can call it the course. So computing units, it Mercer. Yeah, it is composed of one or more processing elements. For the execution, right? The processing elements will execute the real instructions, the legs, the patterns or the classifications for the processing, including the signal instruction multiple data or the SVM, the SIMD, all the laws that were mentioned before. Well, among this, the single instruction, multiple data stream processing, this panel of instruction requires. Yet the vector processor assessments that GPU, or the back two units in the GPU riots were mention. For example, the ADI, Radio HD, that is the GPO made up of lags, the 20th SIMD units. So it can translate that to compute units in the open sale. So that is to say the different, these dial up the OpenCL platforms like Chrome, GPU, DSP, or the multi-core CPU. They have slight difference between this pattern forms with there a kid instruction set architecture, ISA, MD we mention here. So again, into other programming model, their global memory, this memory, memory units, they are connected to the local memories in the kernel functions, a unit, the OpenCL execution model comprise two components, actually, including the corners, programs and the host of programs. I love about that is that the permanent programs, the basic units that are used to execute the code, the code, or they can run, or one or more open sale devices. And the kernels are similar to us. They function that can be like the data or the task a parallel. So that was our hand, the host program. Yet the host program can execute on the host system in a device, that device context. The crease Colonel, executing instruments using the command queries, right? Also the kernels cubed in order, but they can be executed in order or out of order based on your demand. So the others, they explodes. Actually the parallel computation on computer devices. By, you know, it you find the problem into like an dimensional or more, more dimensional index space. So in the OpenCL kernel is CUDA for the execution by the host program. Actually, an index space will be defined during the operation. So details about how the open cell can be executed or conducting da, da, da da da, da programming model can be conducted GPU or the GPU or CPU based architectures. So in addition to that, it also opens, it also allows grouping of work items together into the work programs. Now, another benefit here, so that some futures are by the Open say and how they can deal with Luxe, paralyze them to, to benefit from the GPU and the CPU or the heterogeneous system with GPU or CPU to deal with computations in the architecture. So, yeah. A quick review here about why we discussed before. Yeah, about that. Like, you know, you can still remember the instruction level parallelism, right? To overlap the processing stage of different instructions and the data level paradise, the estimation about them. Yeah. Enables metadata. Items can be upgraded at the same time. Yeah, they used forcing the operation repeated on multiple data items. Actually, you know, wha introduced the SIMD architecture to exploit the data that they don't have a paradigms them and also does re times of the variations including vector processors. I said I am, It's tensions as was the last one we just mentioned about the TPUs, right? Also note that our teachers with this pen off different processor does die. And what's the other one agent, these other managing the exploration of the data level parallelism. So, so far we have introduced are two types of parallelism, including the instruction level parallelism as signal one, data level parallelism. So when it comes to the CR one, Yes. Where level parallelism always caught the task level parallelism? Yeah. Compared with the center one day they'll have a parent like some agenda swear level patronize them. Well, be yeah. Implemented a multiple processor system. Yeah. All the hardware with multiple threads. It can use that to address the functional problems. Let me know the power or memory or the instruction level parallelism or and also there are one pin off or we caught a break. Oh, yeah. In the later, I will show you this problems that can't be addressed or left by the thread level parallelism. So fine. We learn or how their motivations to, yeah, for the task level parallel lines them approaches in the increasing development of multiple processor. And also you are familiar with the many core systems. And we need to know how does thread level parallelism can address these problems. Yeah, As we mentioned, the power or memorable IPO and also brig, hours during the computations in the computer systems. So let's begin by showing down motivations about the multicore here. Actually today when you, actually, when we are talking about the computer architecture or the computing system, with the fact that the train is the Manning court system design for the computer architectures. Yet, even for our mobile devices, our telephone, there will be four or six or eight course today to boosting the performance or to have a better performance. So the most common use actually afford the medical system design for the computer architectures. Actually, the women's, you about the heterogeneous system. Yeah, it is a combine TPU with the Savior in the same system as we discussed before, right? So basically for the not equal system or the medical system, actually it is the computer processes are integrated, so cute with two or more separate processing units. Now we caught the course of this course, can re and execute a program instructions. So the computer has several processors. So manufacturers typically integrate the course onto a single integrated circuit would die. Yeah, i is also known as the chip multiprocessor or that Justo courts the AMP. Or they can be integrated onto multiple dyes into a single chip, Pankaj. So the microprocessors currently a US being, the idea is that in almost all personal computers, they are multicore systems. We are familiar with them and so on. So many camera system that control the AP teachers. We need to think about that. Why we need Monaco or not? For instance, I want to show you about done by me and thought that all away. Know that motivation for the course here. Today, most microprocessors are not depicted by their speeds, but also by the number of cores. For decades, CPU performance grew exponentially, which was due to increase in chip densities and faster clock speeds. The prediction for this type of growth can be accredited to Gordon eat more. But by the turn of the century, chip designers were confronted with a new dilemma as frequency is increased. So did the chips power consumption and heat dissipation if performance growth was to be maintained in new approach had to be taken. Early microprocessors consists our single CPU and features by multitask and made it possible to run multiple programs concurrently, given the illusion of parallel processing. But at the core, these chips executed their tasks in a serial manner. Designer started toying with the idea of work and multiple CPUs in parallel, like a job broken down into smaller tasks and then distributed to a group of people. Workloads can be divided and executed across multiple CPUs, also known as course. This is considered through parallel processing, which greatly improved performance and all without a knee. Higher and higher clock speeds. By the mid 2000s, multi-core CPUs hit the market. Today, the number of cores within a single chip can range from two to four to as much as a team. And as a consumer Greg, chips server great, expands even higher. Well, multiple cores aren't enough. The applications must also be suited for parallel processing. So many developers had to rework their code to take advantage of this new hardware feature. But when working together, performance benefits are undeniable, which is one of the reasons why we have multi-core CPUs today. So this is the grouping or zone for the monocle, yeah, that ITA bought the performance ie, plasma, right? So actually, you know, multiple microprocessors than happy around for a long time. As you know, Nexo makes a state that has the roots going back to the usage was when used around the 90, 78. And there's, our teacher is more ration with the processors to be available later. So over the marginal.com on the performance gains have come from the increase in the lecture video about the force as a clock speed and the ox da da, da da, da actor. The axons which are in improvement. Yeah, that HIV or recent pool sensors, they have increased the number of cores on the cheap. Yeah, instead off and for signing. But beings in the performance of that single thread running on the computer, all running on the processor, right? So the core of the processor is up for that. It's cute. The instructions in occupation. So I have multiple bars now available. A single processor to simultaneously Excuse data model ever patient's actual, it is common to say about that, well, we might have bifurcations to be excluded on the system. So multiple floors or foster system with was more comfort than any. This come up in any, achieve the performance enhancement for the increasing transition speed and also the communication bandwidth teaching this course. And also together with the increasing use of the image and the radio data and kind of emerging ampere patients, they have to apply the masks for the processing 0, the performance. So the reason for that change to a monopole processor that is easy to understand, right? So it has become increasingly part. Yeah, It's pleasure to improve the Sarah performance nominees to improve the performance in Sarah excusing y. So back take too large amount of the area on the silicon chip in order to enable the processor to execute the instruction Foster. And also do so increase the amount of power consumed at the have's generated on the cheap, right? So that means if you, for what you're encouraged, the exclusions with a power all advocates now generated by virtue, well be much fire a single processor system. So the performance gains option, and so that's the approach I know sometimes impressive, but more often they are relatively modest OR gate lengths that 10 percent to 20 percent. So in contrast there, rather than using the APA silicon to increase the single threaded pro-feminist single-threaded operation. Actually use that to add an additional four chip on the system. Can curb use the processor that has the potential to do twice or mona of the world your path. So actually crosses or that has like for example, full course in meiosis you for towns foster or the fourth times performance improvement comparator with a single one said, most effective way of actually improve the overall performance. Yeah, one of the ways is to embrace a number of the threads that actually the processor can be always 0, the process that I support. So obviously, I feel euthanize, uh, you know, multiple pores actually becomes a software farmers I rather than the hardware program, if the hardware as this year for you to support, right to about need will be disposed to a layer to know how to divide the software. Apologies, all this stuff that we're tracking can be used by some to be a hardware besides worth my horse. Tricky part. It's quite dark. Data level quarantines, threat level quarantines and spam. So. Yeah, traditionally, as we mentioned before, the instruction level paradigms or approaches, they are proposed for the performance in a classroom or in the medical systems. So until 2 thousand at your end of the instruction level parallelism. Yeah, instructions can be executed in iPad. So multiple instructions per cycle, like that actually were spawner or the very long instruction word processors were mentioned before. So the technique of branch prediction can be used words, us, Navy exclusions and also some panel of approaches like the olive order exclusions to dedicated for the instruction level parallelism. So on treadmill that actually there is two way or multiple ways Schubert SCADA system. They are design here to enable it to the more instructions executed in the same clock cycle. So that perform as o. The system can be significantly improved based on I introduced before when we're talking about the instruction level parallelism. So actually after that, I was nearly 40 years. Yeah, writer in the Senate, the randomness, searching for Alexa, signal performance and also the power computing daddy that actually the researcher from, yeah, exactly that, that, that the author of our book about the farm, David Paterson. They mention about there are some farmers who cause for, by the one we're exploiting instruction level parallelism as with data level parallelism here. A look at that superscalar processors here. Yeah, these are the haze of Famers. Three bars. They were not yeah. The real or restate, adding enough. And they may as well have been right, this three IMO, I'm unlovable in pediments define the times of the encouraged computing performance that stand out. Yeah, justice than a frog, that performance enhancement. So they prevent the computer users from ever reaching the None of them, you Yeah. All the all milky and Avante, right? So there may be a four ends up ors. Yeah, That's thumb approaches we can deploy or we can use to overcome this, all right, ear. But they are, they are known as the power or the memory. The instruction level paradigms them all. Oh, this might be the force more about that break or so. For that first one, you might be familiar with Daddy's the power 0. So the plural, nice. Faster computers. We'll get really hot. When do some occasions all the workloads in the computing system. So based on the Palmer equation I've shown here, actually, the power consumption increases more rapidly with the increasing operating frequency, right? According to this equation. So actually, you know, around that beginning of 2 suddenly string. The ever increasing processor speed was chakra. Very high power consumption. So yeah, for that PRO at your nose, tail, yes. Hates the power. Whoops. Yeah. You know, Intel engineers a wound, right? Or to the measles three inches apart. And bank are being the gas and one Harding to the mammary or daddies or figurative or later edges to each other to stock the par or here. Yeah. Researchers, all the fund or others from industry. There were there was June the interclass or not one but two printer process or decides inmate around to sell them for. So the aim is to just take you yeah. It can be paid. This kid up. Next up are the high power. And they have some techniques to deal with the par or there. But that is not enough, but with the development of that system and also the high demand for the performance improvement. So they can never do when the microprocessor scared of two hearts and they will create working during execution. So entailed crudely change the direction. Yeah, they choose to slow down the processors. And after that, it allows them to produce, to do WHO or the medical term. As we know that that is about the Vatican went to the muscular system due to the power 0. So then it. Memorable? Yeah. Accurate memory or we have mentioned several times before. Yeah. Again with the power or during the same decade with the powerful. Inca also acknowledge the problem as a critical concern. And they recognize it that the credibility with the legs. The engineers senior lab. Yeah, actually in Moscow, New Mexico. So they have simulated data fusion or high performance computers. Actually, computer contains like eight cores. In the 16 course. Changes to your course. Consider monocle processor and the chip makers say that they are the future of the industry and what is the direction into her warranty do with the medical and even the Manning course. So however, even though, you know the model is learning about and they can't enable the future rather than accelerating the parcel with a single call to meet the power or a Gmail of the really interesting and enter the commercially valuable future computing opportunities, either known as the embarrassing, the parallel ray. Even with their money go eight cores, 16, 32 course, they might use parallel computing on this course. Fiber. Know that with their minds from the engineers came from Intel. Intel has seminary. Describe their future view of the parent partners S. Let you go nation a mining and necessities of the data. And this data, this vector actually they collected, afford their research, all of their innovations. They found that there is a memory. Even they can enhance the performance of the computing by using Monte course. But there is the, or between the performers me confirm the computer is computing on the course and also the between the performance on the memory. So that is the memory exists between the signal and also the memory, right? As we mentioned, because it is hard to increase the performance of the memory axis. And the spirit cannot be improved as not waste the CPU with modern horse here. So that is a statement about the mammary or and the sterilize about the instruction level paradigms them all. Yeah, the instruction level parallelism or means actually a deeper instruction Pep Band. Yeah, it really means to digging deeper, powerful. So as we know from the predators, size or the Progress Classes are bought the instruction level parallelism. And we discussed that the instruction level parallelism means we can't execute multiple instructions or we can execute the pieces of the instruction at the same time in order to make computer run faster. All right, so the computers have the parallel. Here. The height is dy, and the verification time, as well as when there might be the high cost in the computer apps which are way. This will result in the animation which hurting more instruction level parallelism like. So filler showing here you can see that from those who are in order processor, the moderator have nice superscalar on-off order processors. Yeah, to the very deep rest of superscalar processors achieve, the performance cannot be improved. Or where you can see that nato performance gain can be achieved that even with a B efforts here. So therefore, with this existing cases, with the development of the processors right, instruction level parallelism is almost neared. Its limit, means that you cannot achieve the continuous performance improvement by youth in the instruction never parent Langsam sphere. So the force more, it might be the breed or here, yeah, actually taken the predator three about the power or the memorial and also the instruction level parallelism all together. Yeah, they mean that the computers will stop getting faster into the break or so. It could be possible to overcome the verb or action by a multi-core processor. Yeah. It contains two or more processors in the cheek. Yeah, it can provide faster performance by not efficient and simultaneous processing of the multiple tasks. And also that it will consume less power due to the lower cutoff frequency in the course, I'll just save them the processing elements to you. However, you know, there will not continue benefit that system performance. And we're measuring the power efficiency of the medical system with multiple processors. So furthermore, furthermore, if you, sometimes I'm an engineer, I optimize it won't. Or among this yeah. The the the aggrieved bits will be having author of The two hours next, the power memory, the instruction level parallelism. As emotional when we're talking about the approach is still diploid. The lexer instruction level parallelism, it will quote the high power or the, yeah, the access to the memory will weigh during the instruction level paradigm. So why you are trying to locate a womb or there might be some effect on the others. So they are some problems we are facing today when we're doing the three level paradigms. And here, the money flow processor or the many systems might be the way to stop this conference. And I had no real change today for the design of the computer system then also the other boats and computer architecture side. So that at one aspect for the need for these eye out that multicore systems. And yet you get the final performance. Yeah, only these guys a multi-core system. We also need to think about how many posts or we need in the system. If we think about this question, that means not more processors will be better, right? Because the high cost, the high power to support this course in the table and also the costs are to design this computing system. So specifically, for the applications, we need to think about how many close our knee. Yeah, for some real cases. Awesome. Real exam we're seeing, I think in life, Let's first you can see the comparison of the lecture, but the loading time by like two cores of base folklore, it also six courses. Yeah, Based a window systems so that we are show the different number of course can provide the performance. And the performance will be rewire as the tan. Here with the system. You can say to compare with the performance they can provided by different number of the course here. Okay? Wow. All right. At an upward the issues was that the performance of the pan. So that will have the operating system. So I thought actually that might be a huge difference when you are waiting for the app is shown, right? So when we know the, there are indeed some difference between the performance or the person since day one execute the same app. Computers are the computing system. So how many core down you only needed for the, yeah, applications like Samoan games. Actually there as another short video that you can say. Actually I guess it that computer game or I, as one of the representative applications that need high processing speed at the high performers due to the large amount of computations, right? So this short video here about the different performance provide a different novel, of course, hormone and games. And I said this one, you can watch it later. It is somehow similar to the previous one to say the speed and the processing performance on different computing system. Whereas for cause six cores, eight and 10 and act tough courses. That's why you would like to PRE, purchased on computers, raise more cores, always better performance. It will have the better user experience than the UK experience, right? So you can watch that video later to state the difference when we're using the different number of the course for the modern games to show at the performance. So, yeah, well, we know there's a performance difference on different types of ecosystems. And many times, or we can say that there is a further question that one, we have many cars are available in the computing system. Yeah, What can we do to explode the performance that can provide by this course is that we have all available to use. So for this question, we actually need to sing about those span level power lines or yeah, I'm good this way at all. We call it the path bits can be mapped on this provided a specific course for our execution, yet to boost the performance. That is about the resource utilization as we're motion. So this is the bug, the square level parent magnesium, and accommodate any need for the modern computing system, as we're showing here are waste Modena course, all the computing resources, alcohol use, and how we can use this thread level parallelism to allocate the excuses, all the computations on days market course. So, yeah, Even nice way. Imagine that we are facing about the power or memory or instruction-level parallelism law. Yeah, thread level parallelism as yet, also need that is needed for the mobile computing system. That is an effective way to explore the software level paradigms to boost the performance needs. So that might ecosystem. So multi-core processor is a computer processor that injury that occurred was more extra two or more, right? February the processing units and we're motion this processing units actual code, the course. Yeah. So each of this course can read it. It's cured programming structures. Yeah, as if the computer has several processors you can use. So basically, the processor can implement multiple processing, a single physical package. And that designers of these architectures may pop up, causing the medical device tightly. Or new state depends on the area or the limitations of the fiscal design. So, for example, you'll note, of course, may or may not share the caches if they have enough space to store the data. So they may implement the message parsing of the shared memory. Yeah, also, you know, the course can communicate with each other by the Interco communication methods like so to the collisional boss or the shear communication lexer network on chip to share your communicator with this cortex. So the common metal of the topologists to connect with this model of course, that is to interconnect the course include as a measure, Azar slacks or bars, ring, or the two-dimensional image or a crossbar. Besides, there are some Next up, we are familiar with the network on chip connection, or we call it the topology. So besides that, no multiprocessor system widely used the entire database servers as well, the web servers. Also about some of the telecommunications markets, multimedia applications and scientific applications. So that is the way about the topology for this multiple computation elements. So in general, the applications always the thread level parallelism as opposed to the instruction level parallelism, a way shown before. The applications with this random paradigms. And actually it can be applied in a multi-core system to boot, boost performance. So yeah, the medical systems, a task parallelism approach. Yeah, they can be the developer and several functions can be upgraded on the same data. The average, the minimum battery or the authors, there is no dependency between tasks, or we call it the thread level parallelism for what this meant was red or the multiple tasks. There will be no dependency between these tasks. So they can be executed in parallel, in different or separate compute computation unit. So what's more, we need to note that the par or in multiple system can be resolved, right? So actually by integrating multiple slow courses. So that means the course or running and lower clock frequency and low voltage non-test new, deliver the desired performance using NOR, or we can put less power. Yeah, That is an effective way to reduce the power. And we're not a hit the power-law. Top of that, we can still, we can also scale of the number of posts rather than the frequency to keep the lower power. So also the memory or can also be resolved by the attack of the memory level parallelism. So specifically, the memory level paradigms them as a term in computer architecture that refers to the ability to have legs are appending might have a memory operations, right? So in particular, cache misuse, these are common operations during the execution, or some operations to translate that looks sad above our message at the same time. So conventionally, it is performance news in the memory and increase our gap between a memory performance and compute the conventional CPU performance though. So that's where lemma paradigm, or the paradigm has them. For the memory level parallelism here actually the memory or CPU and memory could be resolved. In addition to that as well of a paradigm, but also derive the parallelism. Multi-core systems do. So. There are basically two can of molecule systems, including homogeneous system. It also heterogeneous particle system is that, is that you understand homogeneous and heterogeneous. That was way before. It's right. For that first of all, homogeneous system, medical, medical system. It a colony consists of the identical course. They are more than one core and the core shared the same architecture and microarchitecture. The example of the homogeneous medical system. That is the core. The core, yeah, that means wait for Coursera cortex 50, true system. So a score on this system is identical in the system. So heterogeneous medical system you already have unless he's dying in a very verification passed because they have the same teacher and a shared this thin micro architecture. So E is the much symbol. On the other hand, that heterogeneous medical system. It usually contains different types of the course. And this course differ in the architecture and microarchitecture. This course is optimized for a different row, dedicated for different workloads. There are some Ag, nonetheless, about the heterogeneous multi-core system. Yeah, it is the combination of the microprocessor core with a microcontroller class called fobs them what the, the big later architecture with a course that was higher frequency or the bodies. Moral course for the non-common come computation intensive tasks. So they have different types of the force integrated on the same system. Where a pod, a heterogeneous system, yeah, with different types of law course lives according to a codex or the DSP cores. Yeah, the complexity for the architecture design will be much higher and also higher cost or two. But the desire of the heterogeneous system. How to better performance because they have specific course for that applications with different demands. And also they will decrease the power cost for the application skew. So during the developing of the course, actually there are few challenges, even they can benefit a lot, right? So even one new para, parallel models it also solution that form into medical systems. Most of future computing system performance, we are limited by the power. So it is the way we can quote that is the new firewall, right? It might be different from the previous one. This new car or a medical system. Yeah, it is induced by the technology scaling. One continuous according to his emotional or so as we know from the most, nor before the number of transistor woke up every two years, right? So the increasing number of the transistors and also the power density. Yeah, the power density is the power requirement per unit on the tube. And it will result in more heat dissipation per unit area, empowered or casualty side. So this will cause the new power-law we measure here. It is going to constrain the development of the medical system. He had the most popular example for the result is, yeah, I remember, I mention one time about the tax Telekom phenomenon. So ducks in an actual phenomenon that unknown showed that among of the Tsukiji of egregious occur that can not be part. And the nominal operating voted for givings. Know that giving some more design power. Actually, somebody's NPO is a constraint for the power for the chip. So, yeah, unfortunately, where the number of transistors that can be packaged on the cheap, yeah, actually, that number has increased, right? Had the power consumption per transistor has not taught at a corresponding rate. So the amount of the power four per transistor has dog more slowly than the size of the transistor has shrunk, right? So even truly, based on this, the reason that you will hit some real physical limits of the compute computate computing at patriarchy side. So this Prevnar isn't a steroidal happen in the real operations. So the first and least dream microprocessor is to encounter these zeros. Primer was a lexer entailed course that several years ago. Actually, I did a lot of research how to deal with the tax problems to make a trade-off between the performance. I see, Yeah, that is a way to use some software design ways, the hardware design for the topology design with this course, in a way willpower part of the force with some long distance. Well, we took part, we can design a topology to minimize the communication design communication delay between this course, we use topologies leg stuff that the ink instead of the original mesh we use the further the torus. And also like to do the topology design actually directly to, to deal with the new par or here to solve that problem. So, what's the increase in latency? Memory has become the new memory or two, the medical system. So yes, animals that data memory or is ending at microscale it, right? As the novel cause increased amines, the performance and husband is degraded due to the lack of the memory bandwidth every major me for animals or the memory access partition between the course it over the memory bus. So a multiple systems, these men, of course, we're comparing to operate and might have the access to the memory simultaneously, right? The existing memory latency and bandwidth. Actually, they become insufficient to provide enough axis. And yeah, actually we measure the axis about the access ports, right? For the simultaneous operations to the memory. And also the, they can not provide the resources for the course. This will be the bottleneck and we are heavily impact the performance of the process in benefit of fog computing model, of course. So that is the real change about the new memory or when we're physio about the medical systems today. Even we have fights. They stopped by the soil level parallelism or the memory level parallelism as shown before. It is kneel on, changes should be solved. So the third one, the third challenge, It's hear about the interconnection problem in the multi-core system. Yeah, That is some can about the topology to connect multiple computing course. So as you're in the previous slides, as you would say, the mother, the course I'm connected why? For the data communication, right? Agile multi-core processor implements multiprocessor in your single physical package. So the designers may couple the course in your medical device tightly or loosely. As Malaysia depends on the ARE you can use right. Now these demo, the course may or may not share their cash. And they may implement massive parsing or the shared memory data into core communication mastered between this processors. So the common network topology is used to interconnect the course, including, yeah, I mentioned that boss, the ring or the two-dimensional mesh or the Krusbar. Our idea for the, for the Torah study that another network topology to connect the harmonic force. Yeah, different ways for the interconnection will restart given performance, especially for the data movement or the data communication. Or they have given level of design complexity to your data will be out of here. Yeah, according to the demand from an application, whether it is computationally intensive or the data intensive. With the increasing number of the course, the interconnect lens increased since that data moves across between this course on the cheap right? So the interconnection delay actually will increase with the topology. With the complexity of the topology or the long distance between this course, they need to communicate that data. As a result, also, the entrepreneurship power density has been raised with the size or the number of the force and crazed, right? So this has some challenges in here about the, yeah, the new power or the new memory or a duct design of the multicore systems, even they have better performance. So beside that, you can also think about more and to know the some others in the design of the medical assistance. Yeah. My my search on I or you can say that there are some other problem. Next lesson, globalization multiple. It's cute, it's caching. And also the context switching. If we have more resources, should be managed on the same system. So that awesome challenges, together with the benefits we can chip from the development of the multicore systems here. So yeah, J, to take us a little bit long to finish this lecture. Yes is yeah. This is one of the most important part in the design of the computers. Especially richer the software to, together with the hardware side. Our next lecture, we have learned the pepperoni execution, right? So we have introduced at the data level parallelism after the instruction level parallelism in previous lectures, right? So, yeah, As we mentioned in the very beginning of this class there yet, okay, remember advanced topics of that happening? So why do we mention when nerd in this class is about the advanced photonics? One way during the prevailing order, explored the parallelism during the operation. The instruction level parallelism, data level parallel lines. And the last one about the spread of a paradigm is just a nerve in this class. After we know miss. So how we can design language, for instance, the common language we know to exploit this parallelism or to design the software based approaches by negative two, to do the out of order execution, to deploy the a certain level parallelism to boost the performance. So one can use the software based approaches to first start the performance based on this basic operations or basic approaches we learned in this class. So, yeah, yeah, I mean the end of this lecture. Yeah, just to add that addition to the concluding here, there is a video from the I triple E Computer Society, Silicon Valley. This finger comprehensively talk about the lexer, the memory or at the numerical compression here. Yeah, this is the layer will be a long video. You can watch it after class if you pursue that will be additional knowing. In addition to our predators, several classes in instruction level parallelism, data level. It also has a thread level parallelism together with the computing oxygen or design was medical to solve this problem or the challenges in this video. So you can watch it after the mouse Y. It is convenient for you. I say you'll then take something away. Barber shop at your shot. So that miss class today. Thank you, everyone. And take care everyone. Thank you.