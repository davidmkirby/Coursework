 Hi. I hope you have a lot of jokes today because this is the first of April. I thought that you were talking what you are seeing so many of you I'm sure know Professor My name, monkey, That's it. I'm on one of our colleagues in the department. He holds so that you may not know very prestigious title of being the endowed chair funded by King Philip II of Spain, one of the few remaining kingdoms in Europe. Yes. And if I define, it used to be also co-founded by accompanied guide us along the way. Not only is there finally, larger dollar? Yeah. Okay. Which is like an energy company. It's a utility. Yeah. So garnish. So the professor Muslim as a mom, got his master's degree in telecommunications engineering with a major in electronics. Throng the dotplot detected toluene, yes, I presumably are Catalan. I am someone on a beautiful place you should visit whenever you have a chance. That was in 1994. He left Catalonia, wants to spend proper and studied at the University of Callisto said of the Madrid. We're, he got a PhD in 1999. Also telecommunications technologies. In 2013, he came to Vietnam as a full professor on that endowed chair position. And he is our main explained in the department on machine learning, which is what you will hear today about. And then the various applications of machine learning to smarter than us smart grid. First of all, diets, particle accelerators on there did a number of applications, keeps growing. So On top of that, he has many publications over a 102. He also was the costs are of books on that field. So one is digital, digital signal processing. What's kernels? Which is WHO published by two booty press and widely. That was in 2018, last year, was machine learning applications in electromagnetics and antenna array processing. You'll probably hear about some of it today. Thank you very much. Oh, yes. So Let's talk first of all, thank you everyone for being here. And so let's talk a little bit about what is machine learning basics about by the introduction. In when I was in Spain, I, I knew without machine learning I had to teach electronics, electromagnetics. And then as this kind of thing find by, when I came here, they told me pitch whatever you feel like teaching this to do whatever you want in your area of expertise and methods. Okay, Let's do machine learning and motivation as teaching it is that it's actually any research that is very easy to final stamp. And there is a bunch of very nice classes online, right? But most of them, they are not nice, they are just very complicated. And I've seen many, have seen many of them. And I got bore and disappointed because they don't get a big picture of what an x. And you end up months really understanding what is machine learning when you enter by just knowing a bunch of techniques to those things. By, I compare being not an expert, but being a person that knows about machine learning. To go to a mechanic, Car guy right side. If you're a user of machine learning, this is comparable to drive a car. You can drive it from point a to point B. But this to draw a box, I'm just being a fiber. What happens if you want to improve your car? You cannot. At what happens when the car stops? It stops. If you are really not Machine Learning, you can take all this, these packages, software that are on the Internet and use them. But your ray with you also know what is inside and you can do more things, you can modify, you can, thanks. All right. And engineers were basically hackers, might that have repurpose things all the time. And this is machine. It's about repurposing the sectors and the algorithms that we have. So they basically other way to use this. Okay, yeah, Genius, not so machine-learning has many definitions. And why is that? Well, it's because machine learning. Machine learning has its origins in psychology. The first, among the first people to, to theorize about how can we put together machines that are able to deposition by themselves to learn from data. The first psychologists and people in social sciences too, they were the first to start. All right, fit and furious about that because what they wanted to do is to produce machines that are able to mimic the behavior of the brain that is so far as possible. So we haven't done such thing yet. But many of the structures that we have, they are in spite in the brain, in tiny islands of the brain, even the most sophisticated machine learning techniques and such that we have, they are inspiring and very simplify and phagocytosis of the visual cortex of months. But there is a lot, a bunch of other areas of knowledge. Mathematics, my probability, algebra, optimization, and computer engineering. Computer science. There are many areas of knowledge that are mixed in Michelle. And this is part of its beauty by, but what is, what is machine learning? What is my idea? How can we define machine learning? Well, I'm learning machine is some structure that can learn from data, that can take decisions from observations. So they, they, they flow. In. A learning machine will be something like that. We have some data, some observation right side with that, because it's what we have on the internet, right? Perhaps. And then from the data, the machine, what it does is to extract information. And this is Jen, right? So we have a picture and we want to identify, to detect a cat. What does the machine has to do? Well, extract the information of this data, which is useful for the classification of force before the task at hand. And so this information can be whatever. In the case of the god, I don't know. Whatever eyes, nose, ears, or any feature that the machine finds useful for declassification. So there is a difference between the data, which is what we observe when the machine observes and the information. Usually we do not call this information, we call it features, right? We will see in a town, we get the data and we extract a set of features. And this set of features, they are then processed. And what the machine does is to extract knowledge, right? So this is a flow from data to knowledge through information. And then with that, that information is the decision that the machine picks, right? So we have a picture or we have some, some data, some observation, and we want to extract some variable which is latent that we cannot see. And if the machine tells me something that I dumps novel, this is the information. Of course, I can tell this is right, but in a way, obligations, what do we know? For example, what we have is a an MRI from The brain of a person and we want to detect a given disease, pathology, tumour, schizophrenia, OCD, these kind of things. Depression. Pathologists that cannot be seen directly, I bet about who cannot see. But the machines, they can infer this knowledge for us. So first of all, well, this is kind of an accessory, accessory slide, but I want to. Tries about two kinds of machines. Some, some machines, they, some learning machines. They are supervise and some of them they are not. What is supervised learning and unsupervised learning. What we do is to present data to the machine for which we know the knowledge that the machine is supposed to extract. Right? So this is pi of an experiment that idea to two students on, that we publish. And so what we do is we have a bunch of faces of people. And when we learn and we teach our machine through an algorithm to detect the moon when a person is happy or sad or scared or whatever. And so what we have is a set of data which is labeled, right? We take pictures of people happy, but it is C. And then we let the machine process the information, take the error of the classification, the decision. This error is fed back in order to improve the results of connection. This is supervised learning. But in other cases, and of course, when we put a difference face of a person that was not included in the training sets, then the machine is supposed to tell me whether the person is sad or happy. And that will be the nodes, right? But what if I have a bunch of pictures? This is a real set of pictures of people with different emotions. And actually what happens is that we took this from the internet. So this is basically people faces and we don't know whether they are sad or happy or whatever. And this Very, this distinction is very important because most of the data that we have available is not labeled. In this case, where phases, right, you can go to the Internet and, and use some Google up that detects facial phases. And then you have a million faces. And this is potentially knowledge. But what happens is that we don't have the labels and we cannot just go and labeling all these data one at a time because this one we got cases. Whether we do, we put the machine to extract a different kind of knowledge, which is in this case an algorithm that tells me about the structure of this data. And the structure of this data is related to the mood or the emotions of the people. So that these machines, learning machines, they are also able to group or cluster the phases depending on their emotions. Of course, I don't know whether these people are sad or happy or whatever. But then I can use this data with this machine. So the machine can label them. Right? And so It's very important to know about that because most of the data that we have is just data. And we are and we don't have the possibility of label and name them. So that will be very easy to do. This is very challenging. So what is an example of learning machine which is supervised? I put this example in other, in other parks and, and, but I started by an example in which we can, then some people find it. I find it's a fancy than I changed it to. I, Republicans versus Democrats. Some other people found it profoundly offensive. So finally, here's a Coulomb force and if they don't have the money, right? So let's put together a machine that is able to distinguish between cows and horses. So you have a barn door and then you have an animal at a time, right? And you want to know how many horses and cows go through what the wheel, while we put the camera on the end in, in, in front of the Bandura. And every time a horse or an animal, an animal passes through, some detector, reacts. That's it. And we take a picture of the animal. And so then we have our data, a picture. What do we do with it? Well, we have to extract features. What would be the easiest thing to do? Well, we take the picture, we segment it using some standard image processing, and then we would wish to extract the why of the animal. And so my hypothesis as that is that they, they, they, they forces, they are skinnier and polar and I'm the cows. They are thicker and shorter. So we done, we should be able to distinguish. How do I put a learning machine able to do this? Well, first, I have to be there and let some animals pass and take data and label them by myself. So I did this. The machine takes a height and a y such that this is something that machine does by itself. And then I put that this two in a row, in a space of two dimensions, two features. And this is a space of two dimensions. And so that would be a horse because is tall and thin, right? So you have these two features. Then. I do it with more animals. That is how I can, I know it because it's labeled is this figure and shorter. And I do it as much, as many times as I can. So these are the horses and the cows made my than one. So what we can see here is I've given this a structure of the state, two clusters. What this is telling me that they, the ratio between high and y and width of a cow. The cow is different from the ratio of the horse. And this is what I want the machine to detect. Here is very easily, but it's two-dimensions we can say, but what if we have a picture in which we extracted 20 features or himself a bigger, we have something more elaborate, like an MRI or some biometrics of people or whatever other things. We have 20 dimensions. We cannot see anything. We cannot represent the space we need to trust on the machines to find this kind of. So what is the next thing, that thing that you would do? Well to trace a line, right? Which we call a separating hyperplane that is able to separate between horses and cows. And of course, I will have an error, right? Some horses, they will be here, song cows will be here. It might be the case. What they want to do is to minimize this. But before we try and let's see how this, this separating hyperplane, in this case the hyperplane is just a line. How does it work? Well, I'm sorry. How does this work? This is a linear function that is represented by a set of three parameters. If and then this points here. They are the points for which the output of the function is if this is the way in which we represent a hyperplane spaced points, for waste, a given linear function is equal to 0. And so then this points here applied to the function will have a sign for it. For example, positive and then this will be negative. Right? This can be represented, what do I am? I can represent it like that, that just don't mind. I must have missed out on something. So this is what we do in a linear classifier. And it simply extract from the data a set of features. And then we have a set of parameters. With this features and this parameters, we construct this function which is the dot product between this these pictures and these parameters. And this is nothing but a linear function that represents a hyperplane in the space when the outputs corresponding to x and the y equal to 0. Right? So if we ignore the error and we put a 0 here, we have w transpose x equal to 0. Dy is the equation of hydrogen. If the result of this function is positive, we have formulas. If the result is negative, the other glass, and this is what I don't want to spend for this. Here we have our classification. This is it. So. We can represent, let me say we can represent them. They dance for this. This is something I'm sorry about that, but it's something that's out of place and I don't know why. So so just, just a note. So this is my linear estimator that tells me what is the classification. So if the peoples of this, of this function, I only care about the sign. That's basically then the classification of the sample. This is the set of features corresponding to my observation is equal to the sine of this focus. In order to make it more general, we can add a bias, right? So an affine transform, an affine transformation is equal to w transpose x plus a balance. And so that adds more generality to my estimator. So then here I have, in my case I have two features. Then here I have two parameters. If I add a bias, I will have another, a third party. In general, in any classifier, the number of parameters and a half is equal to the number of features. That will be the easiest learning machine pathway. That is a classification, introgression. Introgression what we do is to estimate a number which is, which can be, which we'll not binary brioche from minus infinity to plus infinity from my observation for, for, for. So for example, imagine that with the observation of the temperatures that we had today, an oligarchy, I want to infer what is going to be the average temperature tomorrow. Right? So again, construct a linear function like that. We're here, I put all the temperatures that we saw that we observe today, from the morning to the, to the night. And then I combine them linearly, and this would be the average temperature tomorrow. Of course, if I do the observation of all the temperatures of one day and the average nevertheless, the next day for a one, I will have a training set for which I can compute the output of the estimator and the error. And then I can construct an algorithm to minimize this sum. So far. When we saw here is a learning machine. From this learning machine, we saw the structure of, the structure of the machine is just this, something that we can express in a mathematical equation at first? And the second thing that we, that we saw is a criterion. We haven't seen academia. But what they mean is that I have never, in my estimation given this set of parameters. And this error will be higher or lower depending on the cell, will be higher or lower depending on the hypotheticals here. So my retailer, my gradient can be based on simply they minimization of the error or the square foot, or the absolute value of yet. So in machine learning, these are the two, two fundamental concepts. First, the structure that we use. This is a start there, which is very symbol. And the criterion. This third thing, which is the algorithm. Once I choose a great deal, I said, Well, I have, for every sample I have an error. Then I can compute the average squared error or the mean is whatever you want to minimize. This is my retain. How would you like to do a similar reaction? This military people were thinking. So I choose, I choose to compute the mean square error, and then I want to minimize it. And the algorithm is the way in which I implement my criteria. Right? So three things. My instructor, you're asking one, we have many and make my video. And then beyond the environment, somebody asked something. Okay. So let's see. Most cases, classification and regression. We want to minimize that. This can be done in several ways, but one would be just obtain a set of samples, right? Cows and horses and then I can label them. Or the temperatures are today and the main cameras off the day after. And then computes the header of the classification this way. And what I want to do is to minimize this error with respect to w, with respect to my set of slides. So I'm getting the error is equal to compute the expectation they are between. The mean error is equal to computing to compute the expectation of this difference between my label or might decide output. And they obtained up. And seems the expectation cannot be obtained. Because I don't have the probability distribution of the squared error. I change this equation by this one, which is the sample mean, right? So the expectation of the error is approximately the mean of the errors that I have. Developing this where I have an equation. This equation can be, and this equation can be processed further in order to compute the minimum with respect to w, What I do is to compute the gradient. You know how this works. We compute the gradient of this expectation with respect, respect to the validators. We do 0. And then we have, after a few algebraic operations, we have a solution. And the solution. You can obtain it by yourself. You don't have to mind about that. I want you to understand the algorithm and the criterion. The criterion is minimize the square of the expectation of yet, what is the average? While I constructed by computing the gradient or the derivative of the squared error, the expectation of the error with respect to the parameters equal to 0. And then I have an equation that they can solve and solve. So this is my solution. And the solution depends only on the observations and they decide op-amps. And this is my bandwidth. In this case, this is my admin, something Bering Sea, right? So and these are examples, real examples, right? In weeks, I put some, some data, artificial data read is one class, blog is another class, and I apply this algorithm and I obtain this, this solution here. This is a solution, w transpose x plus b plus a bias equal to 0. So whatever is here, it produces a positive output, whatever is here, a negative outcome. And for regression, what I obtain is some line that, that adjust to the set of data with the minimum possible squared. And these are the two basic operations that we have in supervised machine learning. Now, what is another important thing in machine learning? Well, if we talk about supervised learning, we have 22 lines of research, too big, too big, lines of knowledge of thinking, machine learning. And one is, I don't know. This. One is what they call kernel, that is called kernel learning. What is this? Is an alternative formulation for what we saw. What do we need this alternative formulation? Well, there's composite results so far. They have linear. And probably you were thinking, yes, well, okay, maybe a default, but this is very restricted because the data might be nonlinear. The data might be structured in a way in which a linear classifier, a linear regression doesn't work. This is what happens in many of, in many cases. The data is not structured in a way in which I can use a linear function to represent. Right? So there is an alternative formulation for linear classifiers are forgetting ethnic restaurants. That gives me a Advantages. This formulation is the following. This theorem says that the function that I constructed can be expressed as a linear combination of dot product between the test and the train page. What does this mean? Remember that we multiply the test sample X to produce at a sign output y my a set of parameters w. This theorem says that w is a linear combination of the training data. So my machine can be expressed like that. And then by applying the same trick as before, minimizing the squared error, I can obtain an expression of this parameters that has this form where K is a matrix of the approach between samples. This equation is very interesting because it depends on a matrix dot-product between samples. And so the next step is that this holes. This is true no matter what done, what is the product that they use here? This is a little product between samples in a Euclidean space. But they can use any, any expression of fire of a function that has properties of dot products. It doesn't need to be this one. Any function that has the properties of that products can be used. And this still works. So what this is going on here. So what is a given dot-product that is suitable? Well, the square exponential, this, this has the shape of a Gaussian function by x, the exponential of minus something squared, right? And this something is the difference between one sample and another sample. Well, this has the properties of the dot-product. So it is a dot product by product in a different space. So we can think of this as, of this was a dot product of a transformation of the data into a higher dimensional space. And in this, in this particular case, the space has infinite dimensions. So we pass the data into a space of infinite dimensions. I don't know what is the transformation. It's unknown. It's not in general, but it has injury dimensions and their reconstruct a linear classifier. So that is the, that is the beauty of this transformation. Now, what happens if we put here instead of this dot product as good exponential? What do you think? Our function is not, is non-linear. And in particular, using I dot-product with it, which is like this dot-product. We can express any, any function we can, we can, we can approximate it. We can explain any set of bits. We have infinite expressive capability just for this. So how do we construct a machine that has the same properties as the linear machine that classified my, my, pacified my forces and golfs by that is non-linear. Well, so first I express my machine using this trick, right? I express my machine like that. So this is any symbol and this is the set of all training samples. Now, I change this dot-product by this exponential. And remember that I have to compute k, which is the matrix of the products between data. In order to find alpha, I do this. I compute the dot products between data by using this expression. I extract alpha, and here is my non-linear estimate. And it works. These are two examples, right? This is a typical problem, cannot be solved using linear machine, right? But we use a linear machine that's simply uses a different product. So it's a linear machine in another space, in a given infinite-dimensional space. And it works. So this are the points for which the function is 0. Using that product. And this is the double spiral problem, right? We have two files like our strong your overview. So we have red points and blue points. And so again, this is a problem that you cannot solve efficiently with a linear machine by just applying this new product, it works. And this is what we call kernel learning or kernel machines because these dot products, these functions of two vectors that are non-linear and they have the properties of the products. They are called Kirchoff's. And this is one layer machine learning. It's called kernel or some people call it shallow working. Why? Because I got this my that was missing. That's out of place. What? We have, my flutter, again, it's a startup like that. I take my test sample. I multiply, I compute the dot product of my test sample with all the training samples. And then I multiply this dot-product by this coefficients alpha, add them together, and this is my, this is my output. And if instead of using this dot product, I use a different product, for example, a square exponential. I have a machine that has nonlinear problems. And it's interesting to see that here's, this machine has only one layer of nervous, right? Some people call them neurons, become, I call them nodes. And this is what is called shallow learning. But there is more This machines, this non-linear machines, they can be very powerful in particularly can be too powerful. If I use a dot product in an infinite-dimensional Hilbert space, I basically, I can classify anything. You give me a set of data labels at random, and I can classify it with 0 errors. There is a theorem, say so on. It's very easy to prove experimentally. So you apply this easy algorithm and you will have all the points classy font, no matter what I believe. And this is not good because actually the machine is learning particular positions of the points in the labels, but it's not learning, understood, right? So what we have to do is to force these machines to be seeing silver damage. So we have to reduce the expressive capability of these machines. And this is what we call February session. Our way to regularize is simply to light a small Lyapunov to the kernel matrix. They're more this, there's a whole bunch of techniques to reduce the computational challenge. They've expressive capability of these machines. So a real application of this is in, for example, in electrical grid. So here we have a real obligation that I presented a while ago. In weights. I take I observe the power used in a city for about three days. I don't remember. I think it was three days. And then we estimate or we predict the power that is going to be used for the load that is going to be used during the next day. And so this line here is the actual power that we observe the next day. I'm sorry, not this line is not the actual power that the predictive power. And this points. They are the actual values that we observed. And these machines, they also allow me to compute confidence intervals and prediction. So, and this is very important because they, utility companies. They want to know not just my prediction, but also how good my prediction is. So evaporation is bad. They want to use it if the duration is good. So the confidence interval is tight. They were using like so. This is something that we can do easily with Carmen shins. And they bear a lot of other applications, right? So for example, here we, what we do is to segment this sellout or a set of clouds in, in infrared. And so we extract features of the clouds, the temperature at the high. We. And then we compute what is the probability that a pixel is in layer of clouds or the other? So here we have one clouds, and here we have the other cloud. And then there's this guy, the background. Right, so it's, it's using kernel machines. We can do things like that. Computing the probability of a point in the sky belongs so about or another about or to the sky. Here. There is another obligation in which we first compute the velocities of the points on the Cloud. And then we are able to, we are able to estimate the velocity of the fluid of the air in the sky with a higher Cura seeing here. So we can see the flow of two layers of clouds mode. This is the flow of this dark layer of clouds, and this is the flow of this hotter and more bright layer of mumps. And some can see here how the things work. It's not easy to see, but anyways, it's very accurate. And so with these things, just saying, just kernel learning is linear machines in infinite dimensional spaces. But what about deep learning? We are talking, we talk about shallow learning. Learning with one layer of nodes and we have very powerful machines. But there is another line of research, which is the classical one. It's called bit more. And then it's based in the concept of new neuron. In machine learning is a structure like that where we put the set of features and the input. We multiply each one of the set of features by a set of parameters. So we're here to apply a dot product just as before. But after that we apply a nonlinear function to the output of this linear. So this is what we call a neuron. And this neuron, it has this nonlinear function which tests are usually it's a sigmoidal function as functioning in the shape of an S shape which is limited. We use this only because if we don't mind, several layers of these neurons will obtain a non-linear function. That is, that's it. Alright? And so we can construct layer with several nodes. And each one of these nodes contains a non-linear function of a different dot product of the input data with a set of Panther. This lines represent different parameters and these neurons or these nodes, they contain a non-linear function and monotonic function. We can start starters like that. And so if we start several layers of this dot products and non-linear functions at the output. The output is a nonlinear function of thing. Yes. Yes. Exactly. And this non-linear function is just this sigmoidal function. It can be simpler than that, right? But it must be nonlinear. And here we have combinations of this non-linear functions, right? So from the second layer, we have a non-linear function of this. We need at least one hidden layer, right? In order get nonlinear functions, because in order to get a glyphicon, because we need this non-linear nodes. And so by stacking different layers, what n, What do we do here? Well, this nodes, they are representations of the input data. And these representations, they have their representations in difference in different spaces. In this case, we must from space or three-dimensions to a space of higher the image. And this itself is a features such as the feature section that we cannot understand from a human point of view. So but a feature suction that is convenient for the machine to do the classification. So these neural networks, they can be very complex and they can have a very high capacity Inspire. We have to train them. We have to train them using, for example, the same criteria, reducing the quadratic error or similar approaches. But instead of having just a few parameters, we have in many cases, many parameters. All right, So here, from here to here, here we have 3, 6, 9 neurons, nine nodes, and three inputs. So here we have three times nine parameters, because we have 3 times 9 connections. And so in this neural network, we have a bunch, a whole bunch of parameters. And what happens is that in order to train this properly, we need usually many date, right? So this is what we have to pay. In this case, we have to pay the price of meaning more data, but we can do things like that. So this is a neural network with two hidden layers. And this is the problem that we wanted to classify. This is a problem while there has an explanation in terms of communication channels, but I'm open to what the British side just, let's assume that this is a given product which is clearly non-linear. We cannot separate this lady. Okay, well, with a neural network with two hidden layers, we can do things like that. And there's, there's very poor median. So what we have is the input to a vector of two-dimensions. And the output has this structure, but simplified to only two layers, my ads. So, and this would be a neural network that is classifying data into the dimensions. But we can use, we can use the images. And then the neural networks. They have a more complex representation. Here. We have a matrix which is make it my image. And here we have a set of nodes that also represent them, be much but in a higher-dimensional space. And so the connection from this layer, this layer is now an array of three dimensions. In our shallow learning, we have an array of, when I mentioned about dimensional vector in my previous neural network, we have connections and we can explain or we can express using matrices. Here we have tensors are arrays with more than one, more than two dimensions. So at any rate, they way in which we use these neural networks is exactly the same. We have the input. In this case, the input will be pixels and then connections to the next layer. And these connections, they are represented by numbers. And these numbers or these parameters, we have to optimize them using any clarity. This guy Gideon, is implemented using what we call the backpropagation out. I'm not going to go explaining the backpropagation algorithm by. So since we can compute the output error, we can transform this error back to the input using them, using the parameters and computing the gradient of this error with respect to these parameters, we can optimize this learning machines, this neuron. And finally, we can come, we can complicate things a little bit more by using what we call common convolutional neural networks. That are networks that actually, instead of doing this, instead of mapping the images to a higher dimensional space using a whole bunch of parameters that map. It's all, all the pixels to each one of the nodes and the output. What we do is to just use a convolution. And so in this case, if we take this image here and we take this matrix, small matrix, and we compute the convolution of this small matrix. With this larger matrix, we will get an image. But this image, the entire image depends only on the parameters of this convolution matrix, in this case is just nine parameters. So we have a transformation from one image to hire, to the next layer, just using a few parameters. Then after that, we can apply a non-linear activation function to each one of these pixels. And we can even sub-sample. We can do it many times with different convolutional matrices of convolution kernels. And then obtain instead of one layer of with, with new image, with an original one, we can obtain several. What, why do we want statistics? While these kernels, this convolution matrices, when we pass them through these images, what they do is to look for features. For example, this kernel here. It might be looking for edges. So it might be computing the derivative of this image in one direction. And so convolutional neural networks, what they do is exactly extracting features of the images. And these teachers, they are the ones that we will use later to produce a classification, right? So in order to detect objects, we use this kind of network for two reasons. First, this convolutions. They are able to detect features, circles for shapes, edges whenever. I'm first, second, their representation of this feature structure, it's a lot more combat because instead of having a bunch of, a bunch of parameters that pass from this image to the next layer. We only have a few that work for the entire image. So this convolution neural networks, they look a lot more complex, but usually reality, they are not, they are a lot cheaper and they work nicely. Let's see some example. This is the original convolutional neural network that was presented in 1995. I think it was presented before that. So for my young and it was used to to classify handwritten digits. And it was used for the USPS or the United States Postal Service. And so this is what they do. So they compute a bunch of convolutions of the image with different convolutional matrices to obtain a bunch of different combo images than this image as they are subsamples bus through nonlinear activations. Then we do it again and again. And then we obtain a set of mini images that are features of this one. Then this is passed through a conventional neural network to obtain an output. This output it has is, is, it's a vector of ten, of ten outputs. It's a factor of 10 scalars in order to detect 10 numbers from 0. And it worked nicely. Dad was he was the first machine ever able to detect handwritten digits with a very high at USC Ubuntu to the Yann LeCun page. And you have a live demo office. Many other things. So that's, that's it. So Godzilla Janae, boom.com. And then you have this work. So the input is this. These are the first convolutions. The second set of convolutions, the third set of convolutions and are just these spots here. It turns out that for different zeros, this set of features, they are very similar. If you, instead of putting a 0, you put a four, then this set of features, they are different, but for two different force, the set of features, they are very similar, right? So this machine is able to detect the 0 or any number from 0 to nine with high attitudes. Now we mark things that are more powerful than this was the beginning of good morning. And in order to finish, this, might want to present some applications in which I work that are in, in smart grid. So here what we do is to input images of the sky and at the same time, a time series of the radiation of the sun. And we predict the radiation of the sun in, during the next minutes from 15 seconds to 10 minutes, something like that. And so this is interesting, etag, it's either sting operation is done in order to be able to have the optimal dispatch. So we can store energy. If we are predicting that the sun is bind to the solar, how husbands to decrease during the next minutes. And so what we do is the fused images with time series. And for the images, of course, we have a bunch of convolutions first and then a standard neural network. Then there is another time, either type of neural network for the time series. And then we combine the outputs of these two layers using another standard neural network. And it works nights. We partnered for the best possible results. This is another version in which instead of wanting much will your three images and you can construct very powerful neural networks. These are our, this is the performance in error of our machine or with respect to their forecasts, rice out from 15 to 19 and 50 seconds. And this is compared to the state of the art, not using this kind of machines. And in particular, we can see that deep learning, which is what we use, is much more powerful than corner and they get this application. We have obligations in cyber human systems that detect, for example, using infrared images of the cameras that the firefighters carry weekend, we were able to tract, try the bodies of the people that are in the sea. We can also compute or estimate the obstacle of interest. Interest, doors, ladders, Windows, fires, and other objects of interest in in real fire scene right here. And we have detection and tracking of of objects and people. And here we have an obligation in weeks. What we do is to segment the fire, right? And in an automatic way from the cameras of the firefighters. And there are other approaches that pass from the original image to our reconstructed image. And this is an application that is, for example, in particular, useful to vacant spot fake images of images that had been tampered with or images that have been that have bulk. In this case, what we do is to, to cut, to cut me much and let the machine to construct it. So, and, and it's, I mean, if you take a close look, you will see that these fake, that this is fake but it's convincing. So well that there is a lot more. I just don't have time to, to talk more about this. But as an emotion, machine learning is a well-established, powerful technique. All those machines to learn from data. And we have obligations in many fields. And they, fields of obligations are growing. And research is not both. Many people's head. It's caused that we have everything, but it's not true. We have a lot to do in research in machine learning and the future. A machine, and it will depend on the availability of computational resources. Resources, I have to say that machine learning is publicly available or publicly available if the scripts done for the humankind to use them. So anyone can do machine learning now from the resources that are available for everyone. And this is something you follow me children. It is not proprietary anymore, except a couple of things that mucus. That's it. Thank you. Good questions and listen to the child. Now. Some people saying that there's things about yes. All right. I don't know. Uh, so first to go see again. Sorry. I am. So whenever they say Oh, yeah, now, yeah. Right? So if you ever have any problem to solve our working machine learning in general. In general, you want to contact me for anything I will be more than happy to, to help to calibrate because this is my life and what they do is when I thank you very much. Thank you. Hi. I'm David. I just had for our terribly machinery.